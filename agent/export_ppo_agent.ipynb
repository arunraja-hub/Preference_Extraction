{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export PPOAgent - ActorCriticNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.agents.ppo.ppo_kl_penalty_agent import PPOKLPenaltyAgent\n",
    "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
    "from tf_agents.networks.value_network import ValueNetwork\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from rl_env.DoomEnviroment import DoomEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPT_DIR = '/home/jupyter/train_data/agentV2.9/train/policy'\n",
    "CHECKPOINT_NO = 19000\n",
    "NO_OF_DATA_POINTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DoomEnvironment(\n",
    "    config_name='rl_env/custom.cfg', \n",
    "    frame_skip=4, \n",
    "    episode_timeout=2000, \n",
    "    obs_shape=(60, 100),\n",
    "    start_ammo=6, \n",
    "    living_reward=3, \n",
    "    kill_imp_reward=100, \n",
    "    kill_demon_reward=10, \n",
    "    ammo_reward=5, \n",
    "    health_reward=.5\n",
    ")\n",
    "tfpy_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = ActorDistributionNetwork(\n",
    "    input_tensor_spec=tfpy_env.observation_spec(),\n",
    "    output_tensor_spec=tfpy_env.action_spec(),\n",
    "    fc_layer_params = (200, 100)\n",
    ")\n",
    "\n",
    "value_net = ValueNetwork(\n",
    "    input_tensor_spec=tfpy_env.observation_spec(),\n",
    "    fc_layer_params = (200, 100)\n",
    ")\n",
    "\n",
    "agent = PPOKLPenaltyAgent(\n",
    "    time_step_spec=tfpy_env.time_step_spec(),\n",
    "    action_spec=tfpy_env.action_spec(),\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    entropy_regularization = 0.0,\n",
    "    kl_cutoff_factor = 2.0,\n",
    "    kl_cutoff_coef = 100,\n",
    "    initial_adaptive_kl_beta = 1.0,\n",
    "    adaptive_kl_target = 0.01,\n",
    "    adaptive_kl_tolerance = 0.3,\n",
    "    normalize_observations = True,\n",
    "    normalize_rewards = False,\n",
    "    use_gae = True,\n",
    "    num_epochs = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/train_data/agentV2.9/train/policy/ckpt-19000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_state = tf.train.get_checkpoint_state(CPT_DIR)\n",
    "checkpoint_paths = list(checkpoint_state.all_model_checkpoint_paths)\n",
    "checkpoint = [x for x in checkpoint_paths if str(CHECKPOINT_NO) in x][0]\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_checkpoint = tf.train.Checkpoint(policy=agent.policy)\n",
    "load_status = policy_checkpoint.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = tfpy_env.reset()\n",
    "observations = []\n",
    "actions = []\n",
    "for _ in range(NO_OF_DATA_POINTS):\n",
    "    action_obj = agent.actor_net.call(time_step.observation, time_step.step_type, network_state=())[0]\n",
    "    action = action_obj.sample().numpy()[0]\n",
    "    actions.append(action)\n",
    "    time_step = tfpy_env.step(action=action)\n",
    "    observations.append(time_step.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  7220300   \n",
      "_________________________________________________________________\n",
      "CategoricalProjectionNetwork multiple                  404       \n",
      "=================================================================\n",
      "Total params: 7,220,704\n",
      "Trainable params: 7,220,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.actor_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EncodingNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7200200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  20100     \n",
      "=================================================================\n",
      "Total params: 7,220,300\n",
      "Trainable params: 7,220,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoding_net = agent.actor_net.layers[0]\n",
    "encoding_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = encoding_net.get_layer('flatten')\n",
    "dense_0 = encoding_net.get_layer('dense')\n",
    "dense_1 = encoding_net.get_layer('dense_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_acts = []\n",
    "dense_0_acts = []\n",
    "dense_1_acts = []\n",
    "for ob in observations:\n",
    "    flat_act = flat(ob)\n",
    "    dense_0_act = dense_0(flat_act)\n",
    "    dense_1_act = dense_1(dense_0_act)\n",
    "    flat_acts.append(flat_act.numpy()[0])\n",
    "    dense_0_acts.append(dense_0_act.numpy()[0])\n",
    "    dense_1_acts.append(dense_1_act.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_acts = np.array(flat_acts)\n",
    "dense_0_acts = np.array(dense_0_acts)\n",
    "dense_1_acts = np.array(dense_1_acts)\n",
    "observations = np.array([ob.numpy()[0] for ob in observations])\n",
    "actions = np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 36000), (100, 200), (100, 100), (100, 60, 100, 6), (100,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_acts.shape, dense_0_acts.shape, dense_1_acts.shape, observations.shape, actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceData(\n",
    "    collections.namedtuple('Trajectory', [\n",
    "        'observations',\n",
    "        'actions',\n",
    "        'activations_lyr0',\n",
    "        'activations_lyr1',\n",
    "        'activations_lyr2',\n",
    "    ])):\n",
    "  __slots__ = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ExperienceData(observations=observations, \n",
    "                      actions=actions, \n",
    "                      activations_lyr0=flat_acts, \n",
    "                      activations_lyr1=dense_0_acts, \n",
    "                      activations_lyr2=dense_1_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/train_data/agentV2.9/AgentExperienceData.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(exps, '/home/jupyter/train_data/agentV2.9/AgentExperienceData.pkl')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
