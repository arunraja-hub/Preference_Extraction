{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export PPOAgent - ActorCriticNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.agents.ppo.ppo_kl_penalty_agent import PPOKLPenaltyAgent\n",
    "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
    "from tf_agents.networks.value_network import ValueNetwork\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from rl_env.DoomEnviroment import DoomEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent, Load Checkpoints, Flatten Model and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPT_DIR = '/home/jupyter/train_data/agentV2.9/train/policy'\n",
    "CHECKPOINT_NO = 19000\n",
    "NO_OF_DATA_POINTS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DoomEnvironment(\n",
    "    config_name='rl_env/custom.cfg', \n",
    "    frame_skip=4, \n",
    "    episode_timeout=2000, \n",
    "    obs_shape=(60, 100),\n",
    "    start_ammo=6, \n",
    "    living_reward=3, \n",
    "    kill_imp_reward=100, \n",
    "    kill_demon_reward=10, \n",
    "    ammo_reward=5, \n",
    "    health_reward=.5\n",
    ")\n",
    "tfpy_env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = ActorDistributionNetwork(\n",
    "    input_tensor_spec=tfpy_env.observation_spec(),\n",
    "    output_tensor_spec=tfpy_env.action_spec(),\n",
    "    fc_layer_params = (200, 100)\n",
    ")\n",
    "\n",
    "value_net = ValueNetwork(\n",
    "    input_tensor_spec=tfpy_env.observation_spec(),\n",
    "    fc_layer_params = (200, 100)\n",
    ")\n",
    "\n",
    "agent = PPOKLPenaltyAgent(\n",
    "    time_step_spec=tfpy_env.time_step_spec(),\n",
    "    action_spec=tfpy_env.action_spec(),\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    entropy_regularization = 0.0,\n",
    "    kl_cutoff_factor = 2.0,\n",
    "    kl_cutoff_coef = 100,\n",
    "    initial_adaptive_kl_beta = 1.0,\n",
    "    adaptive_kl_target = 0.01,\n",
    "    adaptive_kl_tolerance = 0.3,\n",
    "    normalize_observations = True,\n",
    "    normalize_rewards = False,\n",
    "    use_gae = True,\n",
    "    num_epochs = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/train_data/agentV2.9/train/policy/ckpt-19000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_state = tf.train.get_checkpoint_state(CPT_DIR)\n",
    "checkpoint_paths = list(checkpoint_state.all_model_checkpoint_paths)\n",
    "checkpoint = [x for x in checkpoint_paths if str(CHECKPOINT_NO) in x][0]\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_checkpoint = tf.train.Checkpoint(policy=agent.policy)\n",
    "load_status = policy_checkpoint.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ActorDistributionNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  7220300   \n",
      "_________________________________________________________________\n",
      "CategoricalProjectionNetwork multiple                  404       \n",
      "=================================================================\n",
      "Total params: 7,220,704\n",
      "Trainable params: 7,220,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.actor_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorDistributionNetwork/EncodingNetwork/dense/kernel:0 (36000, 200)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense/bias:0 (200,)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense_1/kernel:0 (200, 100)\n",
      "ActorDistributionNetwork/EncodingNetwork/dense_1/bias:0 (100,)\n",
      "ActorDistributionNetwork/CategoricalProjectionNetwork/logits/kernel:0 (100, 4)\n",
      "ActorDistributionNetwork/CategoricalProjectionNetwork/logits/bias:0 (4,)\n"
     ]
    }
   ],
   "source": [
    "for v in agent.actor_net.variables:\n",
    "    print(v.name, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_model(model_nested):\n",
    "    def get_layers(layers):\n",
    "        layers_flat = []\n",
    "        for layer in layers:\n",
    "            try:\n",
    "                layers_flat.extend(get_layers(layer.layers))\n",
    "            except AttributeError:\n",
    "                layers_flat.append(layer)\n",
    "        return layers_flat\n",
    "\n",
    "    model_flat = tf.keras.models.Sequential(\n",
    "        get_layers(model_nested.layers)\n",
    "    )\n",
    "    return model_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7200200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  20100     \n",
      "_________________________________________________________________\n",
      "logits (Dense)               multiple                  404       \n",
      "=================================================================\n",
      "Total params: 7,220,704\n",
      "Trainable params: 7,220,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "flat_actorNet = flatten_model(agent.actor_net)\n",
    "flat_actorNet.build(input_shape=(1, 60, 100, 6))\n",
    "flat_actorNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = flat_actorNet.get_layer('logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_actorNet.save('/home/jupyter/train_data/agentV2.9/actorNet.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Enviroment and Save Observations, Actions, Activations and Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:01<00:00, 82.24it/s]\n"
     ]
    }
   ],
   "source": [
    "time_step = tfpy_env.reset()\n",
    "observations = []\n",
    "actions = []\n",
    "preferences = []\n",
    "for _ in tqdm(range(NO_OF_DATA_POINTS)):\n",
    "    action_obj = agent.actor_net.call(time_step.observation, time_step.step_type, network_state=())[0]\n",
    "    action = action_obj.sample().numpy()[0]\n",
    "    time_step = tfpy_env.step(action=action)\n",
    "    state = tfpy_env.envs[0]._game.get_state()\n",
    "    tfpy_env.envs[0]._game.advance_action()\n",
    "    preference = None\n",
    "    if state is not None:\n",
    "        label = [lbl for lbl in state.labels if lbl.object_name == 'Demon']\n",
    "        if len(label) > 0:\n",
    "            label = label[0]\n",
    "            if label.object_angle < 90 or label.object_angle > 270:\n",
    "                preference = 'Attacking Monster'\n",
    "            else:\n",
    "                preference = 'Attacking Agent'\n",
    "        else:\n",
    "            preference = 'Dead'\n",
    "    preferences.append(preference)\n",
    "    observations.append(time_step.observation)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_net = agent.actor_net.layers[0]\n",
    "flat = encoding_net.get_layer('flatten')\n",
    "dense_0 = encoding_net.get_layer('dense')\n",
    "dense_1 = encoding_net.get_layer('dense_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 594.74it/s]\n"
     ]
    }
   ],
   "source": [
    "flat_acts = []\n",
    "dense_0_acts = []\n",
    "dense_1_acts = []\n",
    "for ob in tqdm(observations):\n",
    "    flat_act = flat(ob)\n",
    "    dense_0_act = dense_0(flat_act)\n",
    "    dense_1_act = dense_1(dense_0_act)\n",
    "    flat_acts.append(flat_act.numpy()[0])\n",
    "    dense_0_acts.append(dense_0_act.numpy()[0])\n",
    "    dense_1_acts.append(dense_1_act.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_acts = np.array(flat_acts)\n",
    "dense_0_acts = np.array(dense_0_acts)\n",
    "dense_1_acts = np.array(dense_1_acts)\n",
    "observations = np.array([ob.numpy()[0] for ob in observations])\n",
    "actions = np.array(actions)\n",
    "preferences = np.array(preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 36000),\n",
       " (10000, 200),\n",
       " (10000, 100),\n",
       " (10000, 60, 100, 6),\n",
       " (10000,),\n",
       " (10000,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_acts.shape, dense_0_acts.shape, dense_1_acts.shape, observations.shape, actions.shape, preferences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceData(\n",
    "    collections.namedtuple('Trajectory', [\n",
    "        'observation',\n",
    "        'action',\n",
    "        'activation_lyr0',\n",
    "        'activation_lyr1',\n",
    "        'activation_lyr2',\n",
    "        'preference'\n",
    "    ])):\n",
    "  __slots__ = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ExperienceData(\n",
    "    observation=observations,\n",
    "    action=actions,\n",
    "    activation_lyr0=flat_acts,\n",
    "    activation_lyr1=dense_0_acts,\n",
    "    activation_lyr2=dense_1_acts,\n",
    "    preference=preferences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/train_data/agentV2.9/AgentExperienceData.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump([exps], '/home/jupyter/train_data/agentV2.9/AgentExperienceData.pkl')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
