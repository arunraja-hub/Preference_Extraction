# Lint as: python2, python3
r"""General TF-Agents evaluation function.

Runs evaluation on policies generated by a TFAgent. It is recommended that
the agent be configured using Gin-config and run using eval (see eval.py in
this directory).
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import math
import os
import time

from absl import logging

import gin
from six.moves import range
import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import

from tf_agents.environments import parallel_py_environment
from tf_agents.environments import py_environment
from tf_agents.environments import suite_gym
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.metrics import tf_py_metric
from tf_agents.policies import greedy_policy
from tf_agents.specs import tensor_spec
from tf_agents.trajectories import time_step as ts
from tf_agents.utils import common
from tensorboard.backend.event_processing import event_file_loader
import environment_specs


def _latest_eval(eval_dir, eval_metrics):
  """Get the latest global step for which an evaluation result was written."""
  if not tf.io.gfile.exists(eval_dir):
    return None

  expected_tags = set(
      [common.join_scope('Metrics', metric.name) for metric in eval_metrics])

  # Record which summaries were written for each global step.
  events_by_step = collections.defaultdict(set)
  for events_file in tf.io.gfile.listdir(eval_dir):
    loader = event_file_loader.EventFileLoader(
        os.path.join(eval_dir, events_file))
    for event in loader.Load():
      if event.summary.value:
        events_by_step[event.step].add(event.summary.value[0].tag)

  # Find the greatest step for which all expected summaries are present.
  for step in sorted(list(events_by_step.keys()), key=lambda step: -step):
    if events_by_step[step].issuperset(expected_tags):
      return step

  return None


def _evaluate_checkpoint_repeatedly(checkpoint_dir,
                                    evaluate_checkpoint_fn,
                                    latest_eval_step=None,
                                    checkpoint_timeout=None):
  """Evaluates a checkpointed model at a set interval.

  Args:
    checkpoint_dir: The directory where an existing model is loaded from.
    evaluate_checkpoint_fn: A function evaluate_checkpoint_fn(checkpoint_path)
      that is called for each checkpoint, e.g. a function that computes rewards,
      generates videos etc. The function can optionally return a should_stop
      boolean to stop the repeated checkpoint evaluation process (e.g. for
      vizier studies).
    latest_eval_step: The global_step of the latest checkpoint evaluated before
      calling this function.
    checkpoint_timeout: The maximum amount of time to wait between checkpoints.
      If left as `None`, then the process will wait indefinitely.
  """
  latest_checkpoint = None
  if latest_eval_step is not None:
    latest_checkpoint = os.path.join(checkpoint_dir,
                                     'ckpt-{}'.format(latest_eval_step))
  latest_eval_time = time.time()
  while (checkpoint_timeout is None or
         time.time() - latest_eval_time < checkpoint_timeout):
    checkpoint_state = tf.train.get_checkpoint_state(checkpoint_dir)
    if checkpoint_state is None:
      time.sleep(60)
      continue

    checkpoint_paths = list(checkpoint_state.all_model_checkpoint_paths)
    if not checkpoint_paths:
      time.sleep(60)
      continue

    if latest_checkpoint is None:
      # Begin at first checkpoint.
      next_idx = 0
    else:
      # Go to the next checkpoint after the last evaluated one.
      try:
        next_idx = checkpoint_paths.index(latest_checkpoint) + 1
      except ValueError:
        # If latest_checkpoint is not in the list, it must have been TTL'd, so
        # just start from the beginning.
        logging.warning('Cannot find latest checkpoint: %s', latest_checkpoint)
        logging.warning('Taking first available checkpoint.')
        next_idx = 0
    if next_idx >= len(checkpoint_paths):
      time.sleep(60)
      continue

    latest_checkpoint = checkpoint_paths[next_idx]

    retries = 3
    for _ in range(retries):
      try:
        should_stop = evaluate_checkpoint_fn(latest_checkpoint)
        break
      except tf.errors.DataLossError as e:
        logging.warning(
            'Encountered a DataLossError while evaluating a checkpoint. This '
            'can happen when reading a checkpoint before it is fully written. '
            'Retrying...')
        time.sleep(2)
    else:
      logging.warning('Failed to evaluate checkpoint after retrying.')
      raise e

    latest_eval_time = time.time()
    if should_stop:
      logging.info('Told to stop. Exiting...')
      return

  logging.info('Timed out. Exiting...')


@gin.configurable
class Evaluator(object):
  """Evaluates policy checkpoints, either once or repeatedly."""

  def __init__(self,
               root_dir,
               env_load_fn=suite_gym.load,
               env_name='CartPole-v0',
               num_parallel_environments=1,
               agent_class=None,
               num_eval_episodes=30,
               write_summaries=True,
               summaries_flush_secs=10,
               eval_metrics_callback=None,
               env_metric_factories=None):
    """Evaluate policy checkpoints as they are produced.

    Args:
      root_dir: Main directory for experiment files.
      env_load_fn: Function to load the environment specified by env_name.
      env_name: Name of environment to evaluate in.
      num_parallel_environments: Number of environments to evaluate on in
        parallel.
      agent_class: TFAgent class to instantiate for evaluation.
      num_eval_episodes: Number of episodes to average evaluation over.
      write_summaries: Whether to write summaries to the file system.
      summaries_flush_secs: How frequently to flush summaries (in seconds).
      eval_metrics_callback: A function that will be called with evaluation
        results for every checkpoint.
      env_metric_factories: An iterable of metric factories. Use this for eval
        metrics that needs access to the evaluated environment. A metric
        factory is a function that takes an eviornment and buffer_size as
        keyword arguments and returns an instance of py_metric.

    Raises:
      ValueError: when num_parallel_environments > num_eval_episodes or
        agent_class is not set
    """
    if not agent_class:
      raise ValueError('The `agent_class` parameter of Evaluator must be set.')
    if num_parallel_environments > num_eval_episodes:
      raise ValueError('num_parallel_environments should not be greater than '
                       'num_eval_episodes')

    self._num_eval_episodes = num_eval_episodes
    self._eval_metrics_callback = eval_metrics_callback
    # Flag that controls eval cycle. If set, evaluation will exit eval loop
    # before the max checkpoint number is reached.
    self._terminate_early = False

    # Save root dir to self so derived classes have access to it.
    self._root_dir = os.path.expanduser(root_dir)
    train_dir = os.path.join(self._root_dir, 'train')
    self._eval_dir = os.path.join(self._root_dir, 'eval')

    self._global_step = tf.compat.v1.train.get_or_create_global_step()

    self._env_name = env_name
    if num_parallel_environments == 1:
      eval_env = env_load_fn(env_name)
    else:
      eval_env = parallel_py_environment.ParallelPyEnvironment(
          [lambda: env_load_fn(env_name)] * num_parallel_environments)

    if isinstance(eval_env, py_environment.PyEnvironment):
      self._eval_tf_env = tf_py_environment.TFPyEnvironment(eval_env)
      self._eval_py_env = eval_env
    else:
      self._eval_tf_env = eval_env
      self._eval_py_env = None  # Can't generically convert to PyEnvironment.

    self._eval_metrics = [
        tf_metrics.AverageReturnMetric(
            buffer_size=self._num_eval_episodes,
            batch_size=self._eval_tf_env.batch_size),
        tf_metrics.AverageEpisodeLengthMetric(
            buffer_size=self._num_eval_episodes,
            batch_size=self._eval_tf_env.batch_size),
    ]
    if env_metric_factories:
      if not self._eval_py_env:
        raise ValueError('The `env_metric_factories` parameter of Evaluator '
                         'can only be used with a PyEnvironment environment.')
      for metric_factory in env_metric_factories:
        py_metric = metric_factory(environment=self._eval_py_env,
                                   buffer_size=self._num_eval_episodes)
        self._eval_metrics.append(tf_py_metric.TFPyMetric(py_metric))

    if write_summaries:
      self._eval_summary_writer = tf.compat.v2.summary.create_file_writer(
          self._eval_dir, flush_millis=summaries_flush_secs * 1000)
      self._eval_summary_writer.set_as_default()
    else:
      self._eval_summary_writer = None

    environment_specs.set_observation_spec(self._eval_tf_env.observation_spec())
    environment_specs.set_action_spec(self._eval_tf_env.action_spec())

    # Agent params configured with gin.
    self._agent = agent_class(self._eval_tf_env.time_step_spec(),
                              self._eval_tf_env.action_spec())

    self._eval_policy = greedy_policy.GreedyPolicy(self._agent.policy)
    self._eval_policy.action = common.function(self._eval_policy.action)

    # Run the agent on dummy data to force instantiation of the network. Keras
    # doesn't create variables until you first use the layer. This is needed
    # for checkpoint restoration to work.
    dummy_obs = tensor_spec.sample_spec_nest(
        self._eval_tf_env.observation_spec(),
        outer_dims=(self._eval_tf_env.batch_size,))
    self._eval_policy.action(
        ts.restart(dummy_obs, batch_size=self._eval_tf_env.batch_size),
        self._eval_policy.get_initial_state(self._eval_tf_env.batch_size))

    self._policy_checkpoint = tf.train.Checkpoint(
        policy=self._agent.policy, global_step=self._global_step)
    self._policy_checkpoint_dir = os.path.join(train_dir, 'policy')

  def _reload_agent(self, checkpoint_path):
    """Initializes the agent, reloads the checkpoint, returns global_step."""
    logging.info('Loading checkpoint: %s', checkpoint_path)
    load_status = self._policy_checkpoint.restore(checkpoint_path)

    # Initialize the agent.
    load_status.initialize_or_restore()

    logging.info('Loaded checkpoint at global_step %d',
                 self._global_step.numpy())
    return self._global_step.numpy()

  def eval_single(self, checkpoint_path):
    """Evaluate a single checkpoint."""
    global_step_val = self._reload_agent(checkpoint_path)
    results = metric_utils.eager_compute(
        self._eval_metrics,
        self._eval_tf_env,
        self._eval_policy,
        num_episodes=self._num_eval_episodes,
        train_step=global_step_val,
        summary_writer=self._eval_summary_writer,
        summary_prefix='Metrics',
    )
    if self._eval_metrics_callback is not None:
      self._eval_metrics_callback(results, global_step_val)
    metric_utils.log_metrics(self._eval_metrics)

    if self._eval_summary_writer:
      self._eval_summary_writer.flush()
    return global_step_val

  @gin.configurable('evaluator.Evaluator.watch_until')
  def watch_until(self, max_num_global_steps=None,
                  checkpoint_timeout_secs=None):
    """Watch policy checkpoint directory and evaluate all checkpoints.

    Args:
      max_num_global_steps: If set, terminate the evaluation after evaluating
        the checkpoint at this global step number.
      checkpoint_timeout_secs: How long to wait for a new checkpoint to appear
        in root_dir before exiting. By default, wait indefinitely.
    """

    def do_eval(current_checkpoint):
      # If max_num_global_steps was set, return True if we passed the max.
      # Otherwise, just return False. (True => stop checking for checkpoints.)
      global_step_val = self.eval_single(current_checkpoint)
      return self._terminate_early or (max_num_global_steps and
                                       global_step_val >= max_num_global_steps)

    latest_eval_step = _latest_eval(self._eval_dir, self._eval_metrics)
    if latest_eval_step is None or latest_eval_step < max_num_global_steps:
      _evaluate_checkpoint_repeatedly(
          self._policy_checkpoint_dir,
          do_eval,
          latest_eval_step=latest_eval_step,
          checkpoint_timeout=checkpoint_timeout_secs)

  def terminate_early(self):
    self._terminate_early = True


class WindowedScoreAccumulator(object):
  """Keeps a sliding window of scores, and reports avg score over the window."""

  def __init__(self, window_size=20):
    self._window_size = window_size
    self._score_window = []

  def put(self, metric_value):
    """Store the objective value in window, and report avg score over window."""
    self._score_window.insert(0, metric_value)
    while len(self._score_window) > self._window_size:
      self._score_window.pop()

  def mean(self):
    return sum(self._score_window) / len(self._score_window)

  def stddev(self):
    mean = self.mean()
    sum_squares = sum([(score - mean)**2 for score in self._score_window])
    return math.sqrt(sum_squares / len(self._score_window))