#-*-Python-*-

import tf_agents.agents.ppo.ppo_kl_penalty_agent
import tf_agents.drivers.dynamic_episode_driver
import tf_agents.replay_buffers.tf_uniform_replay_buffer
import gin
import trainer
import rl_env.PySC2Environment

include 'configs/base.gin'

AGENT_CLASS = @ppo_kl_penalty_agent.PPOKLPenaltyAgent
ENV_LOAD_FN = @pysc2_tf_agents_env
ENV_NAME = '' # Change the environment used here.
NUM_TRAIN_ENVIRONMENTS = 1
COLLECT_EPISODES_PER_ITERATION = 10
NUM_GLOBAL_STEPS = 15000
NUM_EVAL_EPISODES = 10
CONV_LAYER_PARAMS = [[16, 3, 2]]
FC_LAYER_PARAMS = [64]

REPLAY_BUFFER_CAPACITY = 500
trainer.train.train_sequence_length = 500

trainer.train.train_checkpoint_interval = 1000
trainer.train.policy_checkpoint_interval = 1000
trainer.train.rb_checkpoint_interval = 2000
trainer.train.log_interval = 500
trainer.train.summary_interval = 500
trainer.train.summaries_flush_secs = 10

# No initial collection.
trainer.train.initial_collect_random = False
trainer.train.initial_collect_driver_class = @initial_collect/dynamic_episode_driver.DynamicEpisodeDriver
initial_collect/dynamic_episode_driver.DynamicEpisodeDriver.num_episodes = 0

trainer.train.collect_driver_class = @collect/dynamic_episode_driver.DynamicEpisodeDriver
collect/dynamic_episode_driver.DynamicEpisodeDriver.num_episodes = %COLLECT_EPISODES_PER_ITERATION

tf_uniform_replay_buffer.TFUniformReplayBuffer.batch_size = %NUM_TRAIN_ENVIRONMENTS
tf_uniform_replay_buffer.TFUniformReplayBuffer.max_length = %REPLAY_BUFFER_CAPACITY

ppo_kl_penalty_agent.PPOKLPenaltyAgent.optimizer = @critic/tf.train.AdamOptimizer()
critic/tf.train.AdamOptimizer.learning_rate = 1e-4
actor_distribution_network.ActorDistributionNetwork.input_tensor_spec = (@environment_specs.observation_spec())
actor_distribution_network.ActorDistributionNetwork.output_tensor_spec = (@environment_specs.action_spec())
actor_distribution_network.ActorDistributionNetwork.conv_layer_params = %CONV_LAYER_PARAMS
actor_distribution_network.ActorDistributionNetwork.fc_layer_params = %FC_LAYER_PARAMS
value_network.ValueNetwork.input_tensor_spec = (@environment_specs.observation_spec())
value_network.ValueNetwork.conv_layer_params = %CONV_LAYER_PARAMS
value_network.ValueNetwork.fc_layer_params = %FC_LAYER_PARAMS
ppo_kl_penalty_agent.PPOKLPenaltyAgent.actor_net = (@actor_distribution_network.ActorDistributionNetwork())
ppo_kl_penalty_agent.PPOKLPenaltyAgent.value_net = @value_network.ValueNetwork()
ppo_kl_penalty_agent.PPOKLPenaltyAgent.entropy_regularization = 0.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.kl_cutoff_factor = 2.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.kl_cutoff_coef = 100
ppo_kl_penalty_agent.PPOKLPenaltyAgent.initial_adaptive_kl_beta = 1.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.adaptive_kl_target = 0.01
ppo_kl_penalty_agent.PPOKLPenaltyAgent.adaptive_kl_tolerance = 0.3
ppo_kl_penalty_agent.PPOKLPenaltyAgent.normalize_observations = True
ppo_kl_penalty_agent.PPOKLPenaltyAgent.normalize_rewards = True  # Best found in hparam search
ppo_kl_penalty_agent.PPOKLPenaltyAgent.use_gae = True
ppo_kl_penalty_agent.PPOKLPenaltyAgent.num_epochs = 25
ppo_kl_penalty_agent.PPOKLPenaltyAgent.debug_summaries = False
ppo_kl_penalty_agent.PPOKLPenaltyAgent.summarize_grads_and_vars = False

PySC2Environment.flatten_action_specs = False