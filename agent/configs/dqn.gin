#-*-Python-*-

import tf_agents.agents.dqn.dqn_agent
import tf_agents.drivers.dynamic_step_driver
import tf_agents.environments.suite_gym
import tf_agents.networks.q_network
import trainer
import tf_agents.replay_buffers.tf_uniform_replay_buffer
import rl_env.DoomEnviroment

import gin

include 'configs/base.gin'

AGENT_CLASS = @dqn_agent.DqnAgent
ENV_LOAD_FN = @tf_agents_env
ENV_NAME = '' # Change the environment used here.
DoomEnvironment.config_name = 'rl_env/basic.cfg'
NUM_GLOBAL_STEPS = 100000

trainer.train.train_checkpoint_interval = 20000
trainer.train.policy_checkpoint_interval = 20000
trainer.train.rb_checkpoint_interval = 20000

trainer.train.initial_collect_driver_class = \
    @initial_collect/dynamic_step_driver.DynamicStepDriver
initial_collect/dynamic_step_driver.DynamicStepDriver.num_steps = 1000

trainer.train.collect_driver_class = \
    @collect/dynamic_step_driver.DynamicStepDriver
collect/dynamic_step_driver.DynamicStepDriver.num_steps = 1

tf_uniform_replay_buffer.TFUniformReplayBuffer.batch_size = %NUM_TRAIN_ENVIRONMENTS
tf_uniform_replay_buffer.TFUniformReplayBuffer.max_length = 100000
tf_uniform_replay_buffer.TFUniformReplayBuffer.as_dataset.sample_batch_size = 64

dqn_agent.DqnAgent.td_errors_loss_fn = @tf.losses.mean_squared_error
dqn_agent.DqnAgent.gamma = 0.99
dqn_agent.DqnAgent.reward_scale_factor = 1.0
dqn_agent.DqnAgent.gradient_clipping = None
dqn_agent.DqnAgent.epsilon_greedy = 0.1
dqn_agent.DqnAgent.target_update_tau = 0.05
dqn_agent.DqnAgent.target_update_period = 5
dqn_agent.DqnAgent.debug_summaries = False
dqn_agent.DqnAgent.summarize_grads_and_vars = False

dqn_agent.DqnAgent.optimizer = @critic/tf.train.AdamOptimizer()
critic/tf.train.AdamOptimizer.learning_rate = 1e-2

dqn_agent.DqnAgent.q_network = @q_network.QNetwork()
q_network.QNetwork.input_tensor_spec = @environment_specs.observation_spec()
q_network.QNetwork.action_spec = @environment_specs.action_spec()
q_network.QNetwork.fc_layer_params = (100,)