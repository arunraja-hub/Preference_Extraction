#-*-Python-*-

import tf_agents.agents.ppo.ppo_kl_penalty_agent
import tf_agents.drivers.dynamic_episode_driver
import tf_agents.replay_buffers.tf_uniform_replay_buffer
import gin
import trainer
import rl_env.DoomEnviroment

include 'configs/base.gin'

AGENT_CLASS = @ppo_kl_penalty_agent.PPOKLPenaltyAgent
ENV_LOAD_FN = @tf_agents_env_with_ammo
ENV_NAME = '' # Change the environment used here.
NUM_TRAIN_ENVIRONMENTS = 1
COLLECT_EPISODES_PER_ITERATION = 10
NUM_GLOBAL_STEPS = 100000
NUM_EVAL_EPISODES = 10

REPLAY_BUFFER_CAPACITY = 1000
trainer.train.train_sequence_length = 500

# No initial collection.
trainer.train.initial_collect_random = False
trainer.train.initial_collect_driver_class = @initial_collect/dynamic_episode_driver.DynamicEpisodeDriver
initial_collect/dynamic_episode_driver.DynamicEpisodeDriver.num_episodes = 0

trainer.train.collect_driver_class = @collect/dynamic_episode_driver.DynamicEpisodeDriver
collect/dynamic_episode_driver.DynamicEpisodeDriver.num_episodes = %COLLECT_EPISODES_PER_ITERATION

tf_uniform_replay_buffer.TFUniformReplayBuffer.batch_size = %NUM_TRAIN_ENVIRONMENTS
tf_uniform_replay_buffer.TFUniformReplayBuffer.max_length = %REPLAY_BUFFER_CAPACITY

ppo_kl_penalty_agent.PPOKLPenaltyAgent.optimizer = @critic/tf.train.AdamOptimizer()
tf.compat.v1.train.AdamOptimizer.learning_rate = 1e-4
actor_distribution_network.ActorDistributionNetwork.input_tensor_spec = (@environment_specs.observation_spec())
actor_distribution_network.ActorDistributionNetwork.output_tensor_spec = (@environment_specs.action_spec())
actor_distribution_network.ActorDistributionNetwork.conv_layer_params = [[16, 3, 2], [32, 3, 2]]
actor_distribution_network.ActorDistributionNetwork.fc_layer_params = [64]
value_network.ValueNetwork.input_tensor_spec = (@environment_specs.observation_spec())
value_network.ValueNetwork.conv_layer_params = [[16, 3, 2], [32, 3, 2]]
value_network.ValueNetwork.fc_layer_params = [64]
ppo_kl_penalty_agent.PPOKLPenaltyAgent.actor_net = (@actor_distribution_network.ActorDistributionNetwork())
ppo_kl_penalty_agent.PPOKLPenaltyAgent.value_net = @value_network.ValueNetwork()
ppo_kl_penalty_agent.PPOKLPenaltyAgent.entropy_regularization = 0.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.kl_cutoff_factor = 2.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.kl_cutoff_coef = 100
ppo_kl_penalty_agent.PPOKLPenaltyAgent.initial_adaptive_kl_beta = 1.0
ppo_kl_penalty_agent.PPOKLPenaltyAgent.adaptive_kl_target = 0.01
ppo_kl_penalty_agent.PPOKLPenaltyAgent.adaptive_kl_tolerance = 0.3
ppo_kl_penalty_agent.PPOKLPenaltyAgent.normalize_observations = True
ppo_kl_penalty_agent.PPOKLPenaltyAgent.normalize_rewards = True
ppo_kl_penalty_agent.PPOKLPenaltyAgent.use_gae = True
ppo_kl_penalty_agent.PPOKLPenaltyAgent.num_epochs = 25
ppo_kl_penalty_agent.PPOKLPenaltyAgent.debug_summaries = False
ppo_kl_penalty_agent.PPOKLPenaltyAgent.summarize_grads_and_vars = False