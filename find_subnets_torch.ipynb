{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "find-subnets-torch-bugfix",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/master/find_subnets_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwPhoB4VkHs",
        "colab_type": "text"
      },
      "source": [
        "# Find subnets implemented using mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2PtbrFz01L5",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVH7LTvX00Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('Preference_Extraction'):\n",
        "    print(\"Setting up colab environment\")\n",
        "    !pip uninstall -y -q pyarrow\n",
        "    !pip install -q https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.8.0.dev5-cp36-cp36m-manylinux1_x86_64.whl\n",
        "    !pip install -q ray[debug]\n",
        "    !pip install 'ray[tune]' \n",
        "    !pip install bayesian-optimization\n",
        "\n",
        "    !git clone https://github.com/arunraja-hub/Preference_Extraction.git\n",
        "    # # A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "    # # Only after first time\n",
        "    os._exit(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd-5ibD8Zsfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## If you are running on Google Colab, please install TensorFlow 2.0 by uncommenting below..\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xmOhgVIHOwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms\n",
        "import torch.autograd as autograd\n",
        "from torchsummary import summary\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
        "\n",
        "sys.path.append('Preference_Extraction')\n",
        "from imports_data import all_load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hU3eIxdF6IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHxPPHXH-G-",
        "colab_type": "code",
        "outputId": "bbd33980-9b90-408f-c2da-3db35f8f57db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5pVqCzTrzc2",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvjWhMZVr2el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'num_train': 50,\n",
        "    'num_tune': 25,\n",
        "    'num_val': 400,\n",
        "    'batch_size': 10,\n",
        "    'val_batch_size': 10,\n",
        "    'num_epochs': 100,\n",
        "    'use_qnet_weights': True, # Flag for running models that use the weights of Qnet vs models that use random weights\n",
        "    'use_mnist': False,  # Flag for running models on MNIST. If False uses RL Preference Extraction data\n",
        "    'num_run': 5  # Number of runs (with different data sample) over which to average performance\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1R6bB1u1B-l",
        "colab_type": "text"
      },
      "source": [
        "## Subnets Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzdNqZBANfjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Original code from What's hidden in a randomly weighted neural network? paper\n",
        "    Implemented at https://github.com/allenai/hidden-networks\n",
        "    Remove weigths-initialisation since it is not relevant for us\n",
        "\"\"\"\n",
        "\n",
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "        # Get the supermask by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort()\n",
        "        j = int((1 - k) * scores.numel())\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten()\n",
        "        flat_out[idx[:j]] = 0\n",
        "        flat_out[idx[j:]] = 1\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # send the gradient g straight-through on the backward pass.\n",
        "        return g, None\n",
        "\n",
        "class SupermaskConv(nn.Conv2d):\n",
        "    def __init__(self, *args, k, scores_init='kaiming_uniform', **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "        self.scores_init = scores_init\n",
        "\n",
        "        # initialize the scores\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        if self.scores_init == 'kaiming_normal':\n",
        "          nn.init.kaiming_normal_(self.scores)\n",
        "        elif self.scores_init == 'kaiming_uniform':\n",
        "          nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "        elif self.scores_init == 'xavier_normal':\n",
        "          nn.init.xavier_normal_(self.scores)\n",
        "        elif self.scores_init == 'best_activation':\n",
        "          nn.init.ones_(self.scores)\n",
        "        else:\n",
        "          nn.init.uniform_(self.scores)\n",
        "\n",
        "        # initialize the weights\n",
        "        nn.init.uniform_(self.weight)\n",
        "        \n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        x = F.conv2d(\n",
        "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
        "        )\n",
        "        return x\n",
        "\n",
        "class SupermaskLinear(nn.Linear):\n",
        "    def __init__(self, *args, k, scores_init='kaiming_uniform', **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "        self.scores_init = scores_init\n",
        "\n",
        "        # initialize the scores and weights\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        if self.scores_init == 'kaiming_normal':\n",
        "          nn.init.kaiming_normal_(self.scores)\n",
        "        elif self.scores_init == 'kaiming_uniform':\n",
        "          nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "        elif self.scores_init == 'xavier_normal':\n",
        "          nn.init.xavier_normal_(self.scores)\n",
        "        elif self.scores_init == 'best_activation':\n",
        "          nn.init.ones_(self.scores)\n",
        "        else:\n",
        "          nn.init.uniform_(self.scores)\n",
        "\n",
        "        nn.init.uniform_(self.weight)\n",
        "\n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        return F.linear(x, w, self.bias)\n",
        "        return x\n",
        "\n",
        "# NOTE: not used here but we use NON-AFFINE Normalization!\n",
        "# So there is no learned parameters for your nomralization layer.\n",
        "class NonAffineBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, dim):\n",
        "        super(NonAffineBatchNorm, self).__init__(dim, affine=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVKe_Pozw0C",
        "colab_type": "text"
      },
      "source": [
        "## Define architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLgKTR1sqhB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrefQNet(nn.Module):\n",
        "    \"\"\"\n",
        "      If q_head_index is None, this uses a linear model on the normalized q outputs.\n",
        "      Otherwise, it gets the Q head with the specified index.\n",
        "    \"\"\" \n",
        "    def __init__(self, fine_tune, k, q_head_index, q_means_stds, use_last_linear, init_from_act_index=None):\n",
        "        super(PrefQNet, self).__init__()\n",
        "        \n",
        "        if not params['use_mnist']:\n",
        "            channels_in = 5\n",
        "            flattened_shape = 960\n",
        "        else:\n",
        "            channels_in = 1\n",
        "            flattened_shape = 4608\n",
        "\n",
        "        if fine_tune:\n",
        "            conv_layer = nn.Conv2d\n",
        "            dense_layer = nn.Linear\n",
        "            additional_args = {}\n",
        "            init_from_act_index = None\n",
        "        else:\n",
        "            conv_layer = SupermaskConv\n",
        "            dense_layer = SupermaskLinear\n",
        "            additional_args = {'k': k}\n",
        "            if init_from_act_index is not None:\n",
        "                additional_args['scores_init'] = 'best_activation'\n",
        "        \n",
        "        self.conv1 = conv_layer(in_channels=channels_in, out_channels=16, kernel_size=3, stride=1, bias=True, **additional_args)\n",
        "        self.conv2 = conv_layer(in_channels=16, out_channels=32, kernel_size=3, stride=2, bias=True, **additional_args)\n",
        "        self.fc1 = dense_layer(in_features=flattened_shape, out_features=64, bias=True, **additional_args)\n",
        "        self.fc2 = dense_layer(in_features=64, out_features=3, bias=True, **additional_args)\n",
        "        \n",
        "        if init_from_act_index is not None:\n",
        "            init_scores = np.zeros((3, 64))\n",
        "            init_scores[:, init_from_act_index] = 1.0\n",
        "            self.fc2.scores.data = torch.from_numpy(init_scores).float()\n",
        "\n",
        "        self.fc3 = dense_layer(in_features=3, out_features=1, bias=True, **additional_args)\n",
        "        self.linear = nn.Linear(1, 1, bias=True)\n",
        "\n",
        "        self.qix = q_head_index\n",
        "        self.qu_mu_s = q_means_stds\n",
        "        self.use_last_linear = use_last_linear\n",
        "\n",
        "    def fwd_conv1(self, x):\n",
        "        x = self.conv1(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_conv2(self, x):\n",
        "        x = self.fwd_conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_flat(self, x):\n",
        "        x = self.fwd_conv2(x)\n",
        "        return torch.flatten(torch.transpose(x, 1, 3), 1) # Pre-flattening transpose is necessary for TF-Torch conversion\n",
        "\n",
        "    def fwd_fc1(self, x):\n",
        "        x = self.fwd_flat(x)\n",
        "        x = self.fc1(x)\n",
        "        return F.relu(x)\n",
        "    \n",
        "    def fwd_fc2(self, x):\n",
        "        x = self.fwd_fc1(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fwd_fc2(x)\n",
        "\n",
        "        x -= torch.tensor(self.qu_mu_s[0], device=device)\n",
        "        x /= torch.tensor(self.qu_mu_s[1], device=device)\n",
        "\n",
        "        if self.qix == None:\n",
        "          x = self.fc3(x)\n",
        "        else:\n",
        "          x = x[: ,self.qix:self.qix+1]\n",
        "\n",
        "        if self.use_last_linear:\n",
        "          x = self.linear(x)\n",
        "\n",
        "        x = torch.sigmoid(x)\n",
        "        return x.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHGI0a71NVI",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4htablfz1umt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell for training with original RL Preference Extraction data\n",
        "if not params['use_mnist']:\n",
        "    all_raw_data = all_load_data(\"Preference_Extraction/data/simple_env_1/\")\n",
        "\n",
        "    activations = []\n",
        "    observations = []\n",
        "    preferences = []\n",
        "\n",
        "    for data in all_raw_data:\n",
        "        for i in range(data.observation.shape[0]):\n",
        "            observations.append(np.copy(data.observation[i]))\n",
        "            activations.append(np.copy(data.policy_info[\"activations\"][i]))\n",
        "            preferences.append((data.policy_info['satisfaction'].as_list()[i] > -6).astype(int))\n",
        "\n",
        "    activations = np.array(activations)\n",
        "\n",
        "    xs = np.rollaxis(np.array(observations), 3, 1) # Torch wants channel-first\n",
        "    ys = np.array(preferences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tLaYKbpSct4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell for training with MNIST\n",
        "if params['use_mnist']:\n",
        "    tr_data_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])), batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist', train=False, download=True, \n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])), batch_size=params['val_batch_size'], shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpSHf83VSgoJ",
        "colab_type": "text"
      },
      "source": [
        "## Loading Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD-16xFHSi5r",
        "colab_type": "code",
        "outputId": "8bceb66d-67ef-4901-c924-1c24d51da4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "new_save_path = \"Preference_Extraction/saved_model2\"\n",
        "restored_model = tf.keras.models.load_model(new_save_path)\n",
        "restored_model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncodingNetwork/conv2d (Conv (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/dense (Dense (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 67,075\n",
            "Trainable params: 67,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB2G-FiUTmr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_weights=restored_model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-CpUoIqlUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_weights(model):\n",
        "    if not params['use_mnist']:\n",
        "        model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0]))\n",
        "        model.fc1.weight.data = torch.from_numpy(np.transpose(original_weights[4]))\n",
        "    else:\n",
        "        model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0][:,:,:1,:]))\n",
        "        mnist_flt_weights = np.random.rand(64, 4608)\n",
        "        mnist_flt_weights[:, :original_weights[4].shape[0]] = np.transpose(original_weights[4])\n",
        "        mnist_flt_weights = mnist_flt_weights.astype(np.float32)\n",
        "        model.fc1.weight.data = torch.from_numpy(mnist_flt_weights)\n",
        "\n",
        "    model.conv1.bias.data = torch.from_numpy(original_weights[1])\n",
        "    model.conv2.weight.data = torch.from_numpy(np.transpose(original_weights[2]))\n",
        "    model.conv2.bias.data = torch.from_numpy(original_weights[3])\n",
        "    model.fc1.bias.data = torch.from_numpy(original_weights[5])\n",
        "    model.fc2.weight.data = torch.from_numpy(np.transpose(original_weights[6]))\n",
        "    model.fc2.bias.data = torch.from_numpy(original_weights[7])\n",
        "    model.fc3.weight.data = torch.from_numpy(np.ones(shape=[1,3], dtype=np.float32))\n",
        "    model.fc3.bias.data = torch.from_numpy(np.zeros(shape=[1], dtype=np.float32))\n",
        "    model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwWx3ioNqhhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model = PrefQNet(k=1, fine_tune=False, q_head_index=None, q_means_stds=[[0, 0, 0], [1, 1, 1]], use_last_linear=True).to(device)\n",
        "\n",
        "if params['use_qnet_weights']:\n",
        "    load_weights(test_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmX7Cyzes4Ct",
        "colab_type": "text"
      },
      "source": [
        "## Test the weights loaded properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXsgEQuAUwha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing that the models have identical observations for identical images\n",
        "tf_conv1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[0].output)\n",
        "tf_conv2_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[1].output)\n",
        "tf_flt_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[2].output)\n",
        "tf_fc1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[3].output)\n",
        "\n",
        "def npsigmoid(x):\n",
        "  return 1/(1 + np.exp(-x)) \n",
        "\n",
        "def check_same(torch_layer, tf_layer):\n",
        "    torch_out = np.transpose(torch_layer(single_observation_torch).detach().cpu().numpy())\n",
        "    torch_out = torch_out.reshape(torch_out.shape[:-1])\n",
        "    tf_out = tf_layer(single_observation)[0].numpy()\n",
        "    np.testing.assert_allclose(torch_out, tf_out, rtol=.1, atol=5)  \n",
        "\n",
        "# due to shape of original TF model this test can be done only when use_mnist = False\n",
        "if not params['use_mnist'] and params['use_qnet_weights']:\n",
        "    for i in range(len(all_raw_data[0].observation)):\n",
        "\n",
        "        single_observation = np.array([all_raw_data[0].observation[i]])\n",
        "        single_observation_torch = torch.Tensor(np.array([np.transpose(all_raw_data[0].observation[i])]))\n",
        "\n",
        "        single_observation_torch = single_observation_torch.to(device)\n",
        "\n",
        "        check_same(test_model.fwd_conv1, tf_conv1_fn)\n",
        "        check_same(test_model.fwd_conv2, tf_conv2_fn)\n",
        "        check_same(test_model.fwd_flat, tf_flt_fn)\n",
        "        check_same(test_model.fwd_flat, tf_flt_fn)\n",
        "\n",
        "        fc1_torch_out = np.transpose(test_model.fwd_fc1(single_observation_torch).detach().cpu().numpy())\n",
        "        fc1_torch_out = fc1_torch_out.reshape(fc1_torch_out.shape[:-1])\n",
        "        fc1_tf_out = tf_fc1_fn(single_observation)[0].numpy()\n",
        "        \n",
        "        np.testing.assert_allclose(fc1_torch_out, fc1_tf_out, rtol=.1, atol=5)\n",
        "        old_activations = all_raw_data[0].policy_info[\"activations\"][i]\n",
        "        np.testing.assert_allclose(fc1_torch_out, old_activations, rtol=.1, atol=5)\n",
        "        np.testing.assert_allclose(old_activations, fc1_tf_out, rtol=.1, atol=5)\n",
        "\n",
        "        check_same(test_model.fwd_fc2, restored_model)\n",
        "\n",
        "        torch_out = np.transpose(test_model.forward(single_observation_torch).detach().cpu().numpy())\n",
        "        torch_out = torch_out.reshape(torch_out.shape[:-1])\n",
        "        tf_out = npsigmoid(np.sum(restored_model(single_observation)[0].numpy()))\n",
        "        np.testing.assert_allclose(torch_out, tf_out, rtol=.1, atol=5)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAEh60PtAFW",
        "colab_type": "text"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4DfkOECSppn",
        "colab_type": "text"
      },
      "source": [
        "### Get data to normalize qHeads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbrjckDJn94C",
        "colab_type": "code",
        "outputId": "8a870a44-ee2e-4a71-b386-ee20e0ac3578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_q_heads_mu_and_sigma(model, all_obs, num_obs):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_obs = shuffle(all_obs)\n",
        "    obs_to_pass = all_obs[:num_obs]\n",
        "\n",
        "    obs_tensor = torch.Tensor(obs_to_pass)\n",
        "    obs_tensor = obs_tensor.to(device)\n",
        "    qheads_values = model.fwd_fc2(obs_tensor).detach().cpu().numpy()\n",
        "\n",
        "    mu = qheads_values.mean(axis=0)\n",
        "    s = qheads_values.std(axis=0)\n",
        "\n",
        "    print(\"mu\", mu, \"s\", s)\n",
        "    \n",
        "    return np.array([mu, s])\n",
        "\n",
        "if params['use_mnist']:\n",
        "    img_batch, label = iter(tr_data_loader).next()\n",
        "    xs = img_batch\n",
        "\n",
        "q_mu_s = get_q_heads_mu_and_sigma(test_model, xs, 10000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mu [ 92.63603  68.25072 138.24101] s [47.462715 50.686085 77.230034]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnfF0HPKKxC3",
        "colab_type": "text"
      },
      "source": [
        "### Methods to inspect performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXU2W5VKr2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_number_of_new_scores_in_top_k(new_scores, old_scores, k):\n",
        "    new_top_k_scores = set(new_scores[:int(len(new_scores) * k)])\n",
        "    old_top_k_scores = set(old_scores[:int(len(old_scores) * k)])\n",
        "\n",
        "    return len(old_top_k_scores) - len(new_top_k_scores.intersection(old_top_k_scores))\n",
        "\n",
        "def model_scores_to_dict(model):\n",
        "    return {\n",
        "        'conv1': model.conv1.scores.detach().cpu().numpy().flatten().argsort(),\n",
        "        'conv2': model.conv2.scores.detach().cpu().numpy().flatten().argsort(),\n",
        "        'fc1': model.fc1.scores.detach().cpu().numpy().flatten().argsort(),\n",
        "        'fc2': model.fc2.scores.detach().cpu().numpy().flatten().argsort(),\n",
        "        'fc3': model.fc3.scores.detach().cpu().numpy().flatten().argsort()\n",
        "    }\n",
        "\n",
        "def get_no_of_changed_scores(model, previous_scores, k):\n",
        "\n",
        "    new_scores_idxs = model_scores_to_dict(model)\n",
        "\n",
        "    score_changes = {}\n",
        "\n",
        "    for score in new_scores_idxs:\n",
        "        changed_scores_num = get_number_of_new_scores_in_top_k(new_scores_idxs[score], previous_scores[score], k)\n",
        "        score_changes[score] = changed_scores_num\n",
        "\n",
        "    return score_changes, new_scores_idxs\n",
        "\n",
        "def plot_metric(results_dict, metric):\n",
        "    plt.title(metric)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.plot(list(range(1, params['num_epochs'] + 1)), results_dict[f'train{metric}'], label=f'Train {metric}')\n",
        "    plt.plot(list(range(1, params['num_epochs'] + 1)), results_dict[f'test{metric}'], label=f'Test {metric}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_metric_multiple_runs(results_items, metric, train=True):\n",
        "    plt.title(metric)\n",
        "    plt.xlabel('Epochs')\n",
        "    for res_key, res_dict in results_items.items():\n",
        "        if train:\n",
        "            plt.plot(list(range(1, params['num_epochs'] + 1)), res_dict[f'train{metric}'], label=f'Train {metric} - {res_key}')\n",
        "        else:\n",
        "            plt.plot(list(range(1, params['num_epochs'] + 1)), res_dict[f'test{metric}'], label=f'Test {metric} - {res_key}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_score_changes(score_changes_dict):\n",
        "    plt.title('Layer-wise score changes')\n",
        "    plt.xlabel('Optimisation steps (num_train / batch_size * epochs)')\n",
        "    for layer in score_changes_dict:\n",
        "        plt.plot(list(range(1, len(score_changes_dict[layer]) + 1)), score_changes_dict[layer], label=f'{layer}')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjzhnnt9UqBB",
        "colab_type": "text"
      },
      "source": [
        "### Method to get data for one sample run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv8Z2oPJb270",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_sample(xs=None, ys=None):\n",
        "\n",
        "    if not params['use_mnist']:\n",
        "        xs, ys = shuffle(xs, ys)\n",
        "        \n",
        "        train_split = params['num_train']\n",
        "        tune_split = params['num_train']+params['num_tune']\n",
        "        test_split = params['num_train']+params['num_tune']+params['num_val']\n",
        "\n",
        "        tr_data_loader = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(torch.Tensor(xs[:train_split]), torch.Tensor(ys[:train_split])),\n",
        "            batch_size=params['batch_size'])\n",
        "\n",
        "        tune_data_loader = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(torch.Tensor(xs[train_split:tune_split]), torch.Tensor(ys[train_split:tune_split])),\n",
        "            batch_size=params['batch_size'])\n",
        "\n",
        "        val_data_loader = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(torch.Tensor(xs[tune_split:test_split]), torch.Tensor(ys[tune_split:test_split])),\n",
        "            batch_size=params['val_batch_size'])\n",
        "    else:\n",
        "        tr_data_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('mnist', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])), batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "        tune_data_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('mnist', train=False, download=True, \n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])), batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "        val_data_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('mnist', train=False, download=True, \n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])), batch_size=params['val_batch_size'], shuffle=True)\n",
        "        \n",
        "    return tr_data_loader, tune_data_loader, val_data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th-WxNDGTKth",
        "colab_type": "text"
      },
      "source": [
        "### Single run train/test methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEj7Qn1-H-Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Train/Test function for Randomly Weighted Hidden Neural Networks Techniques\n",
        "    Adapted from https://github.com/NesterukSergey/hidden-networks/blob/master/demos/mnist.ipynb\n",
        "\"\"\"\n",
        "\n",
        "def compute_metrics(predictions, true_labels):\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "    accuracy = np.sum(np.equal((predictions > 0.5).astype(int), true_labels)) / len(true_labels)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "    return accuracy, auc\n",
        "\n",
        "def train(model, k, device, train_loader, optimizer, criterion):\n",
        "    \n",
        "    train_loss = 0\n",
        "    true_labels = []\n",
        "    predictions = [] # labels\n",
        "\n",
        "    model.train()\n",
        "    train_score_changes = {}\n",
        "    if k is not None:\n",
        "        scores = model_scores_to_dict(model)\n",
        "        train_score_changes = {k: [] for k in scores}\n",
        "\n",
        "    for data, target in itertools.islice(train_loader, params['num_train']):\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        if params['use_mnist']:\n",
        "            target = (target > 0).float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if k is not None:\n",
        "            score_changes, scores = get_no_of_changed_scores(model, scores, k)\n",
        "            for layer_changes in score_changes:\n",
        "                train_score_changes[layer_changes].append(score_changes[layer_changes])\n",
        "\n",
        "        train_loss += loss\n",
        "        predictions.extend(output.detach().cpu().numpy())\n",
        "        true_labels.extend(target.detach().cpu().numpy())\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    accuracy, auc = compute_metrics(predictions, true_labels)\n",
        "\n",
        "    return train_loss.item(), accuracy, auc, train_score_changes\n",
        "\n",
        "\n",
        "def test(model, device, criterion, test_loader, num_test):\n",
        "    true_labels = []\n",
        "    predictions = [] # labels\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in itertools.islice(test_loader, num_test):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if params['use_mnist']:\n",
        "                target = (target > 0).float()\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "\n",
        "            predictions.extend(output.detach().cpu().numpy())\n",
        "            true_labels.extend(target.detach().cpu().numpy())\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy, auc = compute_metrics(predictions, true_labels)\n",
        "\n",
        "    return test_loss.item(), accuracy, auc\n",
        "\n",
        "def run_model(model, k, learning_rate, weight_decay, num_epochs):\n",
        "\n",
        "  tr_data_loader, tune_data_loader, val_data_loader = get_data_sample(xs, ys)\n",
        "\n",
        "  optimizer = optim.Adam(\n",
        "      [p for p in model.parameters() if p.requires_grad],\n",
        "      lr=learning_rate,\n",
        "      weight_decay=weight_decay\n",
        "  )\n",
        "\n",
        "  criterion = nn.BCELoss().to(device)\n",
        "  scheduler = CosineAnnealingLR(optimizer, T_max=len(tr_data_loader))\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  tune_losses = []\n",
        "  train_accs = []\n",
        "  train_aucs = []\n",
        "  test_accs = []\n",
        "  test_aucs = []\n",
        "  score_changes = []\n",
        "  \n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss, train_accuracy, train_auc, train_score_changes = train(model, k, device, tr_data_loader, optimizer, criterion)\n",
        "      tune_loss, _, _ = test(model, device, criterion, tune_data_loader, params['num_tune'])\n",
        "      test_loss, test_accuracy, test_auc = test(model, device, criterion, val_data_loader, params['num_val'])\n",
        "      scheduler.step()\n",
        "\n",
        "      score_changes.append(train_score_changes)\n",
        "      train_losses.append(train_loss)\n",
        "      tune_losses.append(tune_loss)\n",
        "      test_losses.append(test_loss)\n",
        "      train_accs.append(train_accuracy)\n",
        "      train_aucs.append(train_auc)\n",
        "      test_accs.append(test_accuracy)\n",
        "      test_aucs.append(test_auc)\n",
        "\n",
        "  merged_score_changes = {k: [] for k in score_changes[0].keys()}\n",
        "  for d in score_changes:\n",
        "    for k in d:\n",
        "        merged_score_changes[k].extend(d[k])\n",
        "\n",
        "  return {'trainLoss': train_losses, 'testLoss': test_losses, 'tuneLoss': tune_losses,\n",
        "          'trainAccuracy': train_accs, 'testAccuracy': test_accs,\n",
        "          'trainAUC': train_aucs, 'testAUC': test_aucs, 'scoreChanges': merged_score_changes}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIBJ-Xbe5IaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_runs(fine_tune, K, q_head_index, q_means_stds, use_last_linear, init_from_act_index, \n",
        "               learning_rate, weight_decay, plots=False):\n",
        "\n",
        "    averaged_results = {}    \n",
        "    for run_ix in range(params['num_run']):\n",
        "        \n",
        "        model = PrefQNet(fine_tune=fine_tune, k=K, q_head_index=q_head_index, q_means_stds=q_means_stds,\n",
        "                         use_last_linear=use_last_linear, init_from_act_index=init_from_act_index)\n",
        "        \n",
        "        if params['use_qnet_weights']:\n",
        "            load_weights(model)\n",
        "\n",
        "        results = run_model(model, K, learning_rate=learning_rate, weight_decay=weight_decay, num_epochs=params['num_epochs'])\n",
        "        \n",
        "        print(f'Train pass no. {run_ix+1}')\n",
        "        if (run_ix == 0) and plots:\n",
        "            print('Debug charts for first training run')\n",
        "            plot_metric(results, 'Loss')\n",
        "            plot_metric(results, 'Accuracy')\n",
        "            plot_metric(results, 'AUC')\n",
        "\n",
        "        for val in results:\n",
        "            if len(results[val]) > 0 and val != 'scoreChanges':\n",
        "                if val not in averaged_results:\n",
        "                    averaged_results[val] = [results[val][-1]]\n",
        "                else:\n",
        "                    averaged_results[val].append(results[val][-1])         \n",
        "    \n",
        "    return averaged_results, {x: sum(averaged_results[x]) / params['num_run'] for x in averaged_results}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clre1lUXPXYg",
        "colab_type": "text"
      },
      "source": [
        "## Initialise Subnets Search with activation that obtained optimal AUC in previous experiment\n",
        "\n",
        "We do this both as a sanity check as well as a potential improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO5NwFAmPh2l",
        "colab_type": "code",
        "outputId": "934c0786-040b-4462-dee0-a533b1ed5702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn import metrics \n",
        "\n",
        "acts = []\n",
        "prefs = []\n",
        "\n",
        "for data in all_raw_data:\n",
        "    for i in range(data.observation.shape[0]):\n",
        "        acts.append(np.copy(data.policy_info[\"activations\"][i]))\n",
        "        prefs.append((data.policy_info['satisfaction'].as_list()[i] > -6).astype(int))\n",
        "\n",
        "acts = np.array(acts)\n",
        "prefs = np.array(prefs)\n",
        "\n",
        "def display_auc_info(xs, ys):\n",
        "    \n",
        "    def calc_auc(xs, ys, i):\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(ys, xs[:,i], pos_label=1)\n",
        "        return metrics.auc(fpr, tpr)\n",
        "\n",
        "\n",
        "    multi_runs_aucs = []\n",
        "    for run_ix in range(50):\n",
        "        xs, ys = shuffle(xs, ys)\n",
        "        flat_xs = np.reshape(xs, (xs.shape[0], -1))\n",
        "        aucs = []    \n",
        "        \n",
        "        for i in range(flat_xs.shape[1]):\n",
        "            auc = calc_auc(flat_xs[:params['num_train']], ys[:params['num_train']], i)\n",
        "            aucs.append(auc)  \n",
        "\n",
        "        aucs = np.array(aucs)\n",
        "        multi_runs_aucs.append(aucs)\n",
        "\n",
        "    aucs = np.array(multi_runs_aucs)\n",
        "    aucs = aucs.mean(axis=0)\n",
        "\n",
        "    print(\"AUC from only picking a single activation\")\n",
        "    print(np.argmin(aucs), \"train\", 1-np.min(aucs), \"val\", 1-calc_auc(flat_xs[params['num_train']:], ys[params['num_train']:], np.argmin(aucs)))\n",
        "    print(np.argmax(aucs), \"train\", np.max(aucs), \"val\", calc_auc(flat_xs[params['num_train']:], ys[params['num_train']:], np.argmax(aucs)))\n",
        "  \n",
        "display_auc_info(acts, prefs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC from only picking a single activation\n",
            "34 train 0.8168389427248516 val 0.8203183087179845\n",
            "13 train 0.6133060108723783 val 0.6179787990821131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Yw8LJRYEnL",
        "colab_type": "code",
        "outputId": "9dbfdd4e-a3a5-4d67-9017-e75a952d36eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "best_act_index = 34\n",
        "K = 66774 / 67152  # Num of weigths with all dense activations except one set to 0 / Number of total weights\n",
        "\n",
        "params['num_epochs'] = 1\n",
        "results = multi_runs(fine_tune=False, K=1, q_head_index=None, q_means_stds=q_mu_s, \n",
        "                     use_last_linear=False, init_from_act_index=best_act_index,\n",
        "                     learning_rate=0.000, weight_decay=0.000, plots=False)\n",
        "\n",
        "####results is a tuple where the first element is the dict\n",
        "print(1-max(results[0]['testAUC']))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n",
            "0.8254807323986824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UgIPiAeBYcR",
        "colab_type": "text"
      },
      "source": [
        "## Getting Results for Different Combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY1CfExmBde-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params['num_epochs'] = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gSlNktRBfZh",
        "colab_type": "code",
        "outputId": "8be52f2e-36e4-49fe-a762-970990f39f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.6906836808797593,\n",
              "   0.8151207766592383,\n",
              "   0.7207354027144712,\n",
              "   0.8085122938064115,\n",
              "   0.7416841396805676],\n",
              "  'testAccuracy': [0.665, 0.745, 0.655, 0.7375, 0.71],\n",
              "  'testLoss': [0.12993000447750092,\n",
              "   0.15457528829574585,\n",
              "   0.11470552533864975,\n",
              "   0.09829843044281006,\n",
              "   0.12614212930202484],\n",
              "  'trainAUC': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainAccuracy': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainLoss': [0.0003728709416463971,\n",
              "   0.00011889402230735868,\n",
              "   0.0003117910528089851,\n",
              "   0.0003292688634246588,\n",
              "   0.00023946560395415872],\n",
              "  'tuneLoss': [0.07673028111457825,\n",
              "   0.13663232326507568,\n",
              "   0.1911773979663849,\n",
              "   0.24703745543956757,\n",
              "   0.1049480214715004]},\n",
              " {'testAUC': 0.7553472587480897,\n",
              "  'testAccuracy': 0.7025,\n",
              "  'testLoss': 0.12473027557134628,\n",
              "  'trainAUC': 1.0,\n",
              "  'trainAccuracy': 1.0,\n",
              "  'trainLoss': 0.0002744580968283117,\n",
              "  'tuneLoss': 0.15130509585142135})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38IW5SumDoNy",
        "colab_type": "code",
        "outputId": "6de34f5b-b88f-4bf9-948d-e53e834dc544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=[[0,0,0],[1,1,1]], \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.8995625275038182,\n",
              "   0.8441666666666667,\n",
              "   0.7726049027982707,\n",
              "   0.7617784380305602,\n",
              "   0.8433413536980856],\n",
              "  'testAccuracy': [0.8, 0.8, 0.73, 0.715, 0.74],\n",
              "  'testLoss': [0.12531328201293945,\n",
              "   0.37798964977264404,\n",
              "   0.3866333067417145,\n",
              "   0.4151870012283325,\n",
              "   0.10402786731719971],\n",
              "  'trainAUC': [1.0, 0.9740259740259741, 1.0, 1.0, 1.0],\n",
              "  'trainAccuracy': [1.0, 0.88, 1.0, 1.0, 1.0],\n",
              "  'trainLoss': [3.7426249036798254e-05,\n",
              "   0.0353105403482914,\n",
              "   2.5645133064244874e-05,\n",
              "   1.9554263417376205e-05,\n",
              "   7.25729696569033e-05],\n",
              "  'tuneLoss': [0.21842992305755615,\n",
              "   0.4248124957084656,\n",
              "   0.6506084203720093,\n",
              "   0.697060227394104,\n",
              "   0.32593879103660583]},\n",
              " {'testAUC': 0.8242907777394801,\n",
              "  'testAccuracy': 0.757,\n",
              "  'testLoss': 0.28183022141456604,\n",
              "  'trainAUC': 0.9948051948051948,\n",
              "  'trainAccuracy': 0.976,\n",
              "  'trainLoss': 0.007093147792693344,\n",
              "  'tuneLoss': 0.46336997151374815})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuYkmycDEIEY",
        "colab_type": "code",
        "outputId": "684f7e41-eaa0-42c1-9634-b6644fd4f0c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=False, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.7748549323017409,\n",
              "   0.8385740801397531,\n",
              "   0.755226480836237,\n",
              "   0.7283735714826308,\n",
              "   0.7619310734753669],\n",
              "  'testAccuracy': [0.7275, 0.775, 0.6975, 0.6675, 0.71],\n",
              "  'testLoss': [0.09886088222265244,\n",
              "   0.08169195055961609,\n",
              "   0.10054851323366165,\n",
              "   0.12835179269313812,\n",
              "   0.13617901504039764],\n",
              "  'trainAUC': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainAccuracy': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainLoss': [0.0002572555677033961,\n",
              "   0.00021848625328857452,\n",
              "   0.00031790323555469513,\n",
              "   0.00042096912511624396,\n",
              "   0.0002733402361627668],\n",
              "  'tuneLoss': [0.18685626983642578,\n",
              "   0.1722647249698639,\n",
              "   0.16332288086414337,\n",
              "   0.07844734191894531,\n",
              "   0.23171424865722656]},\n",
              " {'testAUC': 0.7717920276471457,\n",
              "  'testAccuracy': 0.7155,\n",
              "  'testLoss': 0.10912643074989319,\n",
              "  'trainAUC': 1.0,\n",
              "  'trainAccuracy': 1.0,\n",
              "  'trainLoss': 0.0002975908835651353,\n",
              "  'tuneLoss': 0.166521093249321})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgO0pk1BEiN8",
        "colab_type": "code",
        "outputId": "8c2d40ea-3422-4682-df08-1548a1b604b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.005, weight_decay=0.005, plots=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.7517241379310344,\n",
              "   0.8531231697690408,\n",
              "   0.8105423186810307,\n",
              "   0.8253530478306392,\n",
              "   0.8597676654798722],\n",
              "  'testAccuracy': [0.7175, 0.8025, 0.7525, 0.7525, 0.7975],\n",
              "  'testLoss': [0.12750987708568573,\n",
              "   0.08972541242837906,\n",
              "   0.12522943317890167,\n",
              "   0.11508839577436447,\n",
              "   0.10832610726356506],\n",
              "  'trainAUC': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainAccuracy': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainLoss': [0.0006767626036889851,\n",
              "   0.000630376860499382,\n",
              "   0.0005270104738883674,\n",
              "   0.0005537134129554033,\n",
              "   0.00028094794834032655],\n",
              "  'tuneLoss': [0.12058184295892715,\n",
              "   0.04821425676345825,\n",
              "   0.23647792637348175,\n",
              "   0.04430011287331581,\n",
              "   0.0719020813703537]},\n",
              " {'testAUC': 0.8201020679383234,\n",
              "  'testAccuracy': 0.7645,\n",
              "  'testLoss': 0.1131758451461792,\n",
              "  'trainAUC': 1.0,\n",
              "  'trainAccuracy': 1.0,\n",
              "  'trainLoss': 0.0005337622598744929,\n",
              "  'tuneLoss': 0.10429524406790733})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWdGu2arEm3w",
        "colab_type": "code",
        "outputId": "53f49e13-85f3-41ad-eb73-d22293f5acac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.005, plots=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.8118566983164446,\n",
              "   0.5558600172238315,\n",
              "   0.789559748427673,\n",
              "   0.8292198581560284,\n",
              "   0.819068144499179],\n",
              "  'testAccuracy': [0.7525, 0.5725, 0.7875, 0.7825, 0.73],\n",
              "  'testLoss': [0.11455755680799484,\n",
              "   0.14648042619228363,\n",
              "   0.1544840782880783,\n",
              "   0.06819981336593628,\n",
              "   0.11620546132326126],\n",
              "  'trainAUC': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainAccuracy': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
              "  'trainLoss': [0.0011434376938268542,\n",
              "   0.0016285425517708063,\n",
              "   0.00031225051498040557,\n",
              "   0.0012264150427654386,\n",
              "   0.0007125942502170801],\n",
              "  'tuneLoss': [0.11026094108819962,\n",
              "   0.1816493421792984,\n",
              "   0.18949778378009796,\n",
              "   0.12107405066490173,\n",
              "   0.10686139762401581]},\n",
              " {'testAUC': 0.7611128933246312,\n",
              "  'testAccuracy': 0.7249999999999999,\n",
              "  'testLoss': 0.11998546719551087,\n",
              "  'trainAUC': 1.0,\n",
              "  'trainAccuracy': 1.0,\n",
              "  'trainLoss': 0.0010046480107121169,\n",
              "  'tuneLoss': 0.1418687030673027})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0w6BT2lE9ik",
        "colab_type": "code",
        "outputId": "7697e548-0830-4138-f8c7-754d130f7ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.02, weight_decay=0.02, plots=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.6674976227801908,\n",
              "   0.758662724348999,\n",
              "   0.7199561575197682,\n",
              "   0.632303488860866,\n",
              "   0.7362480909114442],\n",
              "  'testAccuracy': [0.695, 0.685, 0.6775, 0.6325, 0.705],\n",
              "  'testLoss': [0.14450573921203613,\n",
              "   0.10684135556221008,\n",
              "   0.15463560819625854,\n",
              "   0.13950411975383759,\n",
              "   0.08086144179105759],\n",
              "  'trainAUC': [0.8596491228070176, 1.0, 1.0, 1.0, 0.9930555555555556],\n",
              "  'trainAccuracy': [0.82, 1.0, 0.98, 1.0, 0.96],\n",
              "  'trainLoss': [0.051631756126880646,\n",
              "   0.004716536495834589,\n",
              "   0.00461394852027297,\n",
              "   0.0012791382614523172,\n",
              "   0.01370335929095745],\n",
              "  'tuneLoss': [0.25260892510414124,\n",
              "   0.11049012839794159,\n",
              "   0.16391296684741974,\n",
              "   0.18502095341682434,\n",
              "   0.12671466171741486]},\n",
              " {'testAUC': 0.7029336168842535,\n",
              "  'testAccuracy': 0.679,\n",
              "  'testLoss': 0.12526965290307998,\n",
              "  'trainAUC': 0.9705409356725146,\n",
              "  'trainAccuracy': 0.952,\n",
              "  'trainLoss': 0.015188947739079594,\n",
              "  'tuneLoss': 0.16774952709674834})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qveadK_2FWne",
        "colab_type": "code",
        "outputId": "5b8de2ef-6590-456c-a0f7-774fd358f047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.9, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.39635416666666673,\n",
              "   0.768603706324867,\n",
              "   0.8624240551951394,\n",
              "   0.8485924013874325,\n",
              "   0.8491711540492027],\n",
              "  'testAccuracy': [0.5875, 0.6775, 0.7875, 0.7975, 0.775],\n",
              "  'testLoss': [0.07246517390012741,\n",
              "   0.0593697614967823,\n",
              "   0.053132064640522,\n",
              "   0.05517274886369705,\n",
              "   0.05698756128549576],\n",
              "  'trainAUC': [0.6397058823529411,\n",
              "   0.8269230769230769,\n",
              "   0.8181818181818181,\n",
              "   0.8833333333333333,\n",
              "   0.965909090909091],\n",
              "  'trainAccuracy': [0.62, 0.74, 0.74, 0.8, 0.92],\n",
              "  'trainLoss': [0.06489463150501251,\n",
              "   0.05627024918794632,\n",
              "   0.055982112884521484,\n",
              "   0.041007645428180695,\n",
              "   0.025145143270492554],\n",
              "  'tuneLoss': [0.1018923819065094,\n",
              "   0.0648772194981575,\n",
              "   0.04325742647051811,\n",
              "   0.1011604443192482,\n",
              "   0.07257822155952454]},\n",
              " {'testAUC': 0.7450290967246618,\n",
              "  'testAccuracy': 0.725,\n",
              "  'testLoss': 0.059425462037324905,\n",
              "  'trainAUC': 0.8268106403400521,\n",
              "  'trainAccuracy': 0.7639999999999999,\n",
              "  'trainLoss': 0.04865995645523071,\n",
              "  'tuneLoss': 0.07675313875079155})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UC_ukNWF6_Q",
        "colab_type": "code",
        "outputId": "9d1c1ebf-2406-4a87-d6aa-22f602e3c3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.6, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.5978670177748517,\n",
              "   0.8302399627242371,\n",
              "   0.8734820824881677,\n",
              "   0.8253927242662258,\n",
              "   0.7136785978249393],\n",
              "  'testAccuracy': [0.5825, 0.705, 0.7825, 0.795, 0.61],\n",
              "  'testLoss': [0.06931723654270172,\n",
              "   0.05731850862503052,\n",
              "   0.048672523349523544,\n",
              "   0.05553197115659714,\n",
              "   0.06524057686328888],\n",
              "  'trainAUC': [0.7069243156199678,\n",
              "   0.9259259259259259,\n",
              "   0.8455882352941176,\n",
              "   0.9322638146167558,\n",
              "   0.8045977011494252],\n",
              "  'trainAccuracy': [0.68, 0.88, 0.82, 0.88, 0.82],\n",
              "  'trainLoss': [0.06171843782067299,\n",
              "   0.03910035640001297,\n",
              "   0.05351971462368965,\n",
              "   0.03264171630144119,\n",
              "   0.05738341063261032],\n",
              "  'tuneLoss': [0.06355435401201248,\n",
              "   0.06309343129396439,\n",
              "   0.07828254252672195,\n",
              "   0.06203703582286835,\n",
              "   0.08579516410827637]},\n",
              " {'testAUC': 0.7681320770156842,\n",
              "  'testAccuracy': 0.6950000000000001,\n",
              "  'testLoss': 0.05921616330742836,\n",
              "  'trainAUC': 0.8430599985212386,\n",
              "  'trainAccuracy': 0.8160000000000001,\n",
              "  'trainLoss': 0.04887272715568543,\n",
              "  'tuneLoss': 0.07055250555276871})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4hbqN36GWOw",
        "colab_type": "code",
        "outputId": "6714e86a-0a82-4379-be10-38061f910302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.6, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.001, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.8916292003430443,\n",
              "   0.8289628914628914,\n",
              "   0.7654018767849857,\n",
              "   0.8291145833333333,\n",
              "   0.872281564589257],\n",
              "  'testAccuracy': [0.5825, 0.48, 0.695, 0.7675, 0.78],\n",
              "  'testLoss': [0.06362002342939377,\n",
              "   0.06852685660123825,\n",
              "   0.06734263896942139,\n",
              "   0.05265888571739197,\n",
              "   0.05053317919373512],\n",
              "  'trainAUC': [0.7934027777777778,\n",
              "   0.6983333333333334,\n",
              "   0.833616298811545,\n",
              "   0.9233333333333333,\n",
              "   0.9504],\n",
              "  'trainAccuracy': [0.68, 0.62, 0.82, 0.82, 0.88],\n",
              "  'trainLoss': [0.06113786622881889,\n",
              "   0.0658850371837616,\n",
              "   0.048361096531152725,\n",
              "   0.03763493150472641,\n",
              "   0.03457426652312279],\n",
              "  'tuneLoss': [0.08563832938671112,\n",
              "   0.08356793969869614,\n",
              "   0.0528334379196167,\n",
              "   0.06560914218425751,\n",
              "   0.056474268436431885]},\n",
              " {'testAUC': 0.8374780233027023,\n",
              "  'testAccuracy': 0.6609999999999999,\n",
              "  'testLoss': 0.0605363167822361,\n",
              "  'trainAUC': 0.8398171486511978,\n",
              "  'trainAccuracy': 0.764,\n",
              "  'trainLoss': 0.04951863959431648,\n",
              "  'tuneLoss': 0.06882462352514267})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR3H0Y6YGvbp",
        "colab_type": "code",
        "outputId": "75791a3d-69a0-4682-d3e2-de8b11d88b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.9, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=False, init_from_act_index=None,\n",
        "           learning_rate=0.005, weight_decay=0.001, plots=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train pass no. 1\n",
            "Train pass no. 2\n",
            "Train pass no. 3\n",
            "Train pass no. 4\n",
            "Train pass no. 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'testAUC': [0.3935055503682955,\n",
              "   0.25936814958091553,\n",
              "   0.45587912087912086,\n",
              "   0.43270948202221016,\n",
              "   0.38731640146878826],\n",
              "  'testAccuracy': [0.5175, 0.38, 0.6175, 0.5725, 0.545],\n",
              "  'testLoss': [0.12048670649528503,\n",
              "   0.11617061495780945,\n",
              "   0.0905999168753624,\n",
              "   0.10561389476060867,\n",
              "   0.10739358514547348],\n",
              "  'trainAUC': [0.541871921182266,\n",
              "   0.3397745571658615,\n",
              "   0.5566502463054187,\n",
              "   0.44871794871794873,\n",
              "   0.3246527777777778],\n",
              "  'trainAccuracy': [0.6, 0.48, 0.62, 0.52, 0.54],\n",
              "  'trainLoss': [0.08359470963478088,\n",
              "   0.1006491556763649,\n",
              "   0.09294570982456207,\n",
              "   0.10623269528150558,\n",
              "   0.1131293773651123],\n",
              "  'tuneLoss': [0.12242378294467926,\n",
              "   0.14295832812786102,\n",
              "   0.11155940592288971,\n",
              "   0.1432107388973236,\n",
              "   0.13935348391532898]},\n",
              " {'testAUC': 0.38575574086386605,\n",
              "  'testAccuracy': 0.5265000000000001,\n",
              "  'testLoss': 0.1080529436469078,\n",
              "  'trainAUC': 0.4423334902298546,\n",
              "  'trainAccuracy': 0.552,\n",
              "  'trainLoss': 0.09931032955646515,\n",
              "  'tuneLoss': 0.1319011479616165})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V82UXi-HVP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.01, weight_decay=0.0001, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDNkwRn5HwM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.01, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C86bMWuAH74M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=0, q_means_stds=[[0,0,0], [1,1,1]], \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.01, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkNyZbBLIHV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.5, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.01, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRPVYsOXIOI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.5, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HxO2zv1Kq-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTRf8ChrLHNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.1, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMBOoB0dLbcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.95, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=4,\n",
        "           learning_rate=0.1, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Bh1fGALcQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=False, K=0.99, q_head_index=0, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.1, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmCvdgrMbcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.1, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzhx0WPsM-Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.1, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHlE79fiNLbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.01, weight_decay=0.0001, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrrRp9kNOF0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temporarily added dropout in the code (will not be able to reproduce)\n",
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=None,\n",
        "           learning_rate=0.005, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su5YANhwOvBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temporarily added dropout in the code (will not be able to reproduce)\n",
        "multi_runs(fine_tune=True, K=None, q_head_index=None, q_means_stds=q_mu_s, \n",
        "           use_last_linear=True, init_from_act_index=34,\n",
        "           learning_rate=0.005, weight_decay=0.0005, plots=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BXsLGpzDq6K",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "All runs use the following parameters\n",
        "\n",
        "```\n",
        "{'batch_size': 10,\n",
        " 'num_epochs': 100,\n",
        " 'num_run': 5,\n",
        " 'num_train': 50,\n",
        " 'num_tune': 25,\n",
        " 'num_val': 400,\n",
        " 'use_mnist': False,\n",
        " 'use_qnet_weights': True,\n",
        " 'val_batch_size': 10}\n",
        " ```\n",
        "\n",
        "| fine_tune      | q_head_index | norm_q_means | use_last_linear | init_from_act_index | K     | Optimiser | learning_rate | momentum | weight_decay | testAUC |\n",
        "|----------------|--------------|--------------|-----------------|---------------------|-------|-----------|---------------|----------|--------------|---------|\n",
        "| False          | None (fc3)   | True         | True            | Activation 34       | ~0.99 | Adam      | 0.000         | None     | 0.000        | 0.83    |\n",
        "| True           | None (fc3)   | True         | True            | None                | 1     | Adam      | 0.01          | None     | 0.001        | 0.83    |\n",
        "| True           | None (fc3)   | False        | True            | None                | 1     | Adam      | 0.01          | None     | 0.001        | 0.76    |\n",
        "| True           | None (fc3)   | True         | False           | None                | 1     | Adam      | 0.01          | None     | 0.001        | 0.68    |\n",
        "| True           | None (fc3)   | True         | False           | None                | 1     | Adam      | 0.005         | None     | 0.005        | 0.81    |\n",
        "| True           | 0            | True         | True            | None                | 1     | Adam      | 0.01          | None     | 0.005        | 0.76    |\n",
        "| True           | None (fc3)   | True         | True            | None                | 1     | Adam      | 0.1           | None     | 0.001        | 0.51    |\n",
        "| True           | None (fc3)   | True         | True            | None                | 1     | Adam      | 0.02          | None     | 0.02         | 0.71    |\n",
        "| False          | None (fc3)   | True         | True            | None                | 0.9   | Adam      | 0.01          | None     | 0.001        | 0.75    |\n",
        "| False          | None (fc3)   | True         | True            | None                | 0.6   | Adam      | 0.01          | None     | 0.001        | 0.67    |\n",
        "| False          | None (fc3)   | True         | True            | None                | 0.6   | Adam      | 0.001         | None     | 0.001        | 0.68    |\n",
        "| False          | None (fc3)   | True         | False           | None                | 0.9   | Adam      | 0.005         | None     | 0.001        | 0.4     |\n",
        "| False          | None (fc3)   | True         | True            | Activation 34       | 0.95  | Adam      | 0.01          | None     | 0.0001       | 0.77    |\n",
        "| False          | 0            | True         | True            | Activation 34       | 0.95  | Adam      | 0.01          | None     | 0.0005       | 0.81    |\n",
        "| False          | 0            | False        | True            | Activation 34       | 0.95  | Adam      | 0.01          | None     | 0.0005       | 0.71    |\n",
        "| False          | 0            | True         | True            | Activation 34       | 0.5   | Adam      | 0.01          | None     | 0.0005       | 0.55    |\n",
        "| False          | 0            | True         | True            | None                | 0.5   | SDG       | 0.1           | 0.9      | 0.0005       | 0.73    |\n",
        "| False          | 0            | True         | True            | None                | 0.95  | SDG       | 0.1           | 0.9      | 0.0005       | 0.79    |\n",
        "| False          | 0            | True         | True            | Activation 34       | 0.95  | SDG       | 0.1           | 0.9      | 0.0005       | 0.66    |\n",
        "| False          | 0            | True         | True            | Activation 4        | 0.95  | SDG       | 0.1           | 0.9      | 0.0005       | 0.57    |\n",
        "| False          | 0            | True         | True            | Activation 34       | 0.99  | SDG       | 0.1           | 0.9      | 0.0005       | 0.66    |\n",
        "| True           | None (fc3)   | True         | True            | None                | 1     | SDG       | 0.1           | 0.9      | 0.0005       | 0.5     |\n",
        "| True           | None (fc3)   | True         | True            | None                | 1     | Adam      | 0.1           | None     | 0.0005       | 0.51    |\n",
        "| True w/DropOut | None (fc3)   | True         | True            | None                | 1     | Adam      | 0.005         | None     | 0.0005       | 0.81    |\n",
        "| True w/DropOut | None (fc3)   | True         | True            | Activation 34       | 1     | Adam      | 0.005         | None     | 0.005        | 0.78    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzVb5sW-U6Gs",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters Tuning (not used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj4MIa8iZEf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_runs_tune(config, reporter):\n",
        "\n",
        "    averaged_results = {}\n",
        "    tuning_losses = []\n",
        "    for run_ix in range(params['num_run']):\n",
        "        if params['fine_tune']:\n",
        "            K = None\n",
        "        else:\n",
        "            K = config['k']\n",
        "\n",
        "        model = PrefQNet(fine_tune=params['fine_tune'], k=K, \n",
        "                         q_head_index=None, q_means_stds=q_mu_s, use_last_linear=True, init_from_act_index=34).to(device)\n",
        "\n",
        "        if params['use_qnet_weights']:\n",
        "            load_weights(model)\n",
        "        \n",
        "        results = run_model(model, K, config['lr'], config['decay'], num_epochs=params['num_epochs'])\n",
        "\n",
        "        if reporter is not None:  # Hyperp-tuning pass\n",
        "            tuning_losses.append(results['tuneLoss'][-1])\n",
        "            reporter(timesteps_total=run_ix, mean_loss=sum(tuning_losses)/len(tuning_losses))\n",
        "\n",
        "def launch_tune():\n",
        "\n",
        "    space = {\n",
        "        \"k\": (0.05, 0.95), \n",
        "        \"lr\": (0.001, 0.1), \n",
        "        'decay': (0.0001, 0.05)\n",
        "    }\n",
        "\n",
        "    if params['fine_tune']:\n",
        "        space['k'] = (1, 1)\n",
        "\n",
        "    config = {\"num_samples\": params['num_tune_iters'], \"stop\": {\"timesteps_total\": params['num_run']}}\n",
        "\n",
        "    algo = BayesOptSearch(space, metric=\"mean_loss\", mode=\"min\", utility_kwargs={\n",
        "        \"kind\": \"ucb\", \"kappa\": 2.5, \"xi\": 0.0})\n",
        "\n",
        "    scheduler = AsyncHyperBandScheduler(metric=\"mean_loss\", mode=\"min\")\n",
        "\n",
        "    return tune.run(multi_runs_tune, resources_per_trial={'gpu': 1, 'cpu': 2}, verbose=1, \n",
        "                    name=\"tune_exp\", search_alg=algo, scheduler=scheduler, **config)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}