{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "find-subnets-torch",
      "provenance": [],
      "collapsed_sections": [
        "O2PtbrFz01L5",
        "uIHGI0a71NVI",
        "wpSHf83VSgoJ",
        "MmX7Cyzes4Ct"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwPhoB4VkHs",
        "colab_type": "text"
      },
      "source": [
        "# Everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2PtbrFz01L5",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVH7LTvX00Y5",
        "colab_type": "code",
        "outputId": "a0fac2f9-e1e2-464a-d6b5-0f62c52afd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!git clone https://github.com/arunraja-hub/Preference_Extraction.git"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Preference_Extraction' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xmOhgVIHOwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import datasets, transforms\n",
        "import torch.autograd as autograd\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow as tf\n",
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "sys.path.append('Preference_Extraction')\n",
        "from imports_data import all_load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hU3eIxdF6IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHxPPHXH-G-",
        "colab_type": "code",
        "outputId": "c4764b24-7f3a-4e72-b35c-180338ae521c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5pVqCzTrzc2",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvjWhMZVr2el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    'num_train': 50,\n",
        "    'num_val': 400,\n",
        "    'batch_size': 10,\n",
        "    'val_batch_size': 10,\n",
        "    'use_qnet_weights': True, # Flag for running models that use the weights of Qnet vs models that use random weights\n",
        "    'use_mnist': True  # Flag for running models on MNIST. If False uses RL Preference Extraction data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1R6bB1u1B-l",
        "colab_type": "text"
      },
      "source": [
        "## Subnets Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzdNqZBANfjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Original code from What's hidden in a randomly weighted neural network? paper\n",
        "    Implemented at https://github.com/allenai/hidden-networks\n",
        "    Remove weigths-initialisation since it is not relevant for us\n",
        "\"\"\"\n",
        "\n",
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "        # Get the supermask by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort()\n",
        "        j = int((1 - k) * scores.numel())\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten()\n",
        "        flat_out[idx[:j]] = 0\n",
        "        flat_out[idx[j:]] = 1\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # send the gradient g straight-through on the backward pass.\n",
        "        return g, None\n",
        "\n",
        "class SupermaskConv(nn.Conv2d):\n",
        "    def __init__(self, *args, k, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "        # initialize the scores\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "\n",
        "        # initialize the weights\n",
        "        nn.init.uniform_(self.weight)\n",
        "        \n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        x = F.conv2d(\n",
        "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
        "        )\n",
        "        return x\n",
        "\n",
        "class SupermaskLinear(nn.Linear):\n",
        "    def __init__(self, *args, k, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "        # initialize the scores and weights\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "        nn.init.uniform_(self.weight)\n",
        "\n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        return F.linear(x, w, self.bias)\n",
        "        return x\n",
        "\n",
        "# NOTE: not used here but we use NON-AFFINE Normalization!\n",
        "# So there is no learned parameters for your nomralization layer.\n",
        "class NonAffineBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, dim):\n",
        "        super(NonAffineBatchNorm, self).__init__(dim, affine=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHGI0a71NVI",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4htablfz1umt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell for training with original RL Preference Extraction data\n",
        "if not params['use_mnist']:\n",
        "    all_raw_data = all_load_data(\"Preference_Extraction/data/simple_env_1/\")\n",
        "\n",
        "    activations = []\n",
        "    observations = []\n",
        "    preferences = []\n",
        "\n",
        "    for data in all_raw_data:\n",
        "        for i in range(data.observation.shape[0]):\n",
        "            observations.append(np.copy(data.observation[i]))\n",
        "            activations.append(np.copy(data.policy_info[\"activations\"][i]))\n",
        "            preferences.append((data.policy_info['satisfaction'].as_list()[i] > -6).astype(int))\n",
        "\n",
        "    activations = np.array(activations)\n",
        "\n",
        "    xs = np.rollaxis(np.array(observations), 3, 1) # Torch wants channel-first\n",
        "    ys = np.array(preferences)\n",
        "    xs, ys = shuffle(xs, ys)\n",
        "\n",
        "    xs_tr = xs[:params['num_train']]\n",
        "    ys_tr = ys[:params['num_train']]\n",
        "    xs_val = xs[params['num_train']:params['num_train']+params['num_val']]\n",
        "    ys_val = ys[params['num_train']:params['num_train']+params['num_val']]\n",
        "\n",
        "    tr_data_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(torch.Tensor(xs_tr), torch.Tensor(ys_tr)),\n",
        "        batch_size=params['batch_size'])\n",
        "\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(torch.Tensor(xs_val), torch.Tensor(ys_val)),\n",
        "        batch_size=params['val_batch_size'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ABp8a35st5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell for training with MNIST data\n",
        "if params['use_mnist']:\n",
        "    tr_data_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])), batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist', train=False, download=True, \n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])), batch_size=params['val_batch_size'], shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVKe_Pozw0C",
        "colab_type": "text"
      },
      "source": [
        "## Define architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLgKTR1sqhB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SuperMaskQNets(nn.Module):\n",
        "    \"\"\"\n",
        "      If q_head_index is None, this uses a linear model on the normalized q outputs.\n",
        "      Otherwise, it gets the Q head with the specified index.\n",
        "    \"\"\" \n",
        "    def __init__(self, k, q_head_index, q_means_stds):\n",
        "        super(SuperMaskQNets, self).__init__()\n",
        "        \n",
        "        if not params['use_mnist']:\n",
        "            channels_in = 5\n",
        "            flattened_shape = 960\n",
        "        else:\n",
        "            channels_in = 1\n",
        "            flattened_shape = 4608\n",
        "\n",
        "        self.conv1 = SupermaskConv(in_channels=channels_in, out_channels=16, kernel_size=3, stride=1, bias=True, k=k)\n",
        "        self.conv2 = SupermaskConv(in_channels=16, out_channels=32, kernel_size=3, stride=2, bias=True, k=k)\n",
        "        self.fc1 = SupermaskLinear(in_features=flattened_shape, out_features=64, bias=True, k=k)\n",
        "        self.fc2 = SupermaskLinear(in_features=64, out_features=3, bias=True, k=k)\n",
        "        self.fc3 = SupermaskLinear(in_features=3, out_features=1, bias=True, k=k)\n",
        "\n",
        "        self.qix = q_head_index\n",
        "        self.qu_mu_s = q_means_stds\n",
        "\n",
        "    def fwd_conv1(self, x):\n",
        "        x = self.conv1(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_conv2(self, x):\n",
        "        x = self.fwd_conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_flat(self, x):\n",
        "        x = self.fwd_conv2(x)\n",
        "        return torch.flatten(torch.transpose(x, 1, 3), 1) # Pre-flattening transpose is necessary for TF-Torch conversion\n",
        "\n",
        "    def fwd_fc1(self, x):\n",
        "        x = self.fwd_flat(x)\n",
        "        x = self.fc1(x)\n",
        "        return F.relu(x)\n",
        "    \n",
        "    def fwd_fc2(self, x):\n",
        "        x = self.fwd_fc1(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fwd_fc2(x)\n",
        "\n",
        "        x -= torch.tensor(self.qu_mu_s[0], device=device)\n",
        "        x /= torch.tensor(self.qu_mu_s[1], device=device)\n",
        "\n",
        "        if self.qix == None:\n",
        "          x = self.fc3(x).flatten()\n",
        "        else:\n",
        "          x = x[: ,self.qix]\n",
        "        \n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpSHf83VSgoJ",
        "colab_type": "text"
      },
      "source": [
        "## Loading Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD-16xFHSi5r",
        "colab_type": "code",
        "outputId": "c0b73a5f-c04d-4e9e-e7de-158a58204a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "new_save_path = \"Preference_Extraction/saved_model2\"\n",
        "restored_model = tf.keras.models.load_model(new_save_path)\n",
        "restored_model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncodingNetwork/conv2d (Conv (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/dense (Dense (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 67,075\n",
            "Trainable params: 67,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB2G-FiUTmr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_weights=restored_model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-CpUoIqlUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_weights(model):\n",
        "    if not params['use_mnist']:\n",
        "        model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0]))\n",
        "        model.fc1.weight.data = torch.from_numpy(np.transpose(original_weights[4]))\n",
        "    else:\n",
        "        model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0][:,:,:1,:]))\n",
        "        mnist_flt_weights = np.random.rand(64, 4608)\n",
        "        mnist_flt_weights[:, :original_weights[4].shape[0]] = np.transpose(original_weights[4])\n",
        "        mnist_flt_weights = mnist_flt_weights.astype(np.float32)\n",
        "        model.fc1.weight.data = torch.from_numpy(mnist_flt_weights)\n",
        "\n",
        "    model.conv1.bias.data = torch.from_numpy(original_weights[1])\n",
        "    model.conv2.weight.data = torch.from_numpy(np.transpose(original_weights[2]))\n",
        "    model.conv2.bias.data = torch.from_numpy(original_weights[3])\n",
        "    model.fc1.bias.data = torch.from_numpy(original_weights[5])\n",
        "    model.fc2.weight.data = torch.from_numpy(np.transpose(original_weights[6]))\n",
        "    model.fc2.bias.data = torch.from_numpy(original_weights[7])\n",
        "    model.fc3.weight.data = torch.from_numpy(np.ones(shape=[1,3], dtype=np.float32))\n",
        "    model.fc3.bias.data = torch.from_numpy(np.zeros(shape=[1], dtype=np.float32))\n",
        "    model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwWx3ioNqhhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "supermask_test_model = SuperMaskQNets(k=1, q_head_index=None, q_means_stds=[[0, 0, 0], [1, 1, 1]]).to(device)\n",
        "\n",
        "if params['use_qnet_weights']:\n",
        "    load_weights(supermask_test_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmX7Cyzes4Ct",
        "colab_type": "text"
      },
      "source": [
        "## Test the weights loaded properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXsgEQuAUwha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing that the models have identical observations for identical images\n",
        "tf_conv1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[0].output)\n",
        "tf_conv2_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[1].output)\n",
        "tf_flt_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[2].output)\n",
        "tf_fc1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[3].output)\n",
        "\n",
        "def npsigmoid(x):\n",
        "  return 1/(1 + np.exp(-x)) \n",
        "\n",
        "def check_same(torch_layer, tf_layer):\n",
        "    torch_out = np.transpose(torch_layer(single_observation_torch).detach().cpu().numpy())\n",
        "    torch_out = torch_out.reshape(torch_out.shape[:-1])\n",
        "    tf_out = tf_layer(single_observation)[0].numpy()\n",
        "    np.testing.assert_allclose(torch_out, tf_out, rtol=.1)  \n",
        "\n",
        "# due to shape of original TF model this test can be done only when use_mnist = False\n",
        "if not params['use_mnist'] and params['use_qnet_weights']:\n",
        "    for i in range(len(all_raw_data[0].observation)):\n",
        "\n",
        "        single_observation = np.array([all_raw_data[0].observation[i]])\n",
        "        single_observation_torch = torch.Tensor(np.array([np.transpose(all_raw_data[0].observation[i])]))\n",
        "\n",
        "        single_observation_torch = single_observation_torch.to(device)\n",
        "\n",
        "        check_same(supermask_test_model.fwd_conv1, tf_conv1_fn)\n",
        "        check_same(supermask_test_model.fwd_conv2, tf_conv2_fn)\n",
        "        check_same(supermask_test_model.fwd_flat, tf_flt_fn)\n",
        "        check_same(supermask_test_model.fwd_flat, tf_flt_fn)\n",
        "\n",
        "        fc1_torch_out = np.transpose(supermask_test_model.fwd_fc1(single_observation_torch).detach().cpu().numpy())\n",
        "        fc1_torch_out = fc1_torch_out.reshape(fc1_torch_out.shape[:-1])\n",
        "        fc1_tf_out = tf_fc1_fn(single_observation)[0].numpy()\n",
        "        \n",
        "        np.testing.assert_allclose(fc1_torch_out, fc1_tf_out, rtol=.1)\n",
        "        old_activations = all_raw_data[0].policy_info[\"activations\"][i]\n",
        "        np.testing.assert_allclose(fc1_torch_out, old_activations, rtol=.1)\n",
        "        np.testing.assert_allclose(old_activations, fc1_tf_out, rtol=.1)\n",
        "\n",
        "        check_same(supermask_test_model.fwd_fc2, restored_model)\n",
        "\n",
        "        torch_out = np.transpose(supermask_test_model.forward(single_observation_torch).detach().cpu().numpy())\n",
        "        torch_out = torch_out.reshape(torch_out.shape[:-1])\n",
        "        tf_out = npsigmoid(np.sum(restored_model(single_observation)[0].numpy()))\n",
        "        np.testing.assert_allclose(torch_out, tf_out, rtol=.1)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAEh60PtAFW",
        "colab_type": "text"
      },
      "source": [
        "## Create models for each q net head. And load weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbrjckDJn94C",
        "colab_type": "code",
        "outputId": "ce79ad8a-e5d7-4938-bfef-cf6c2aa1f312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_q_heads_mu_and_sigma(model, all_obs, num_obs):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_obs = shuffle(all_obs)\n",
        "    obs_to_pass = all_obs[:num_obs]\n",
        "\n",
        "    obs_tensor = torch.Tensor(obs_to_pass)\n",
        "    obs_tensor = obs_tensor.to(device)\n",
        "    qheads_values = model.fwd_fc2(obs_tensor).detach().cpu().numpy()\n",
        "\n",
        "    mu = qheads_values.mean(axis=0)\n",
        "    s = qheads_values.std(axis=0)\n",
        "\n",
        "    print(\"mu\", mu, \"s\", s)\n",
        "    \n",
        "    return np.array([mu, s])\n",
        "\n",
        "if params['use_mnist']:\n",
        "    img_batch, label = iter(tr_data_loader).next()\n",
        "    xs = img_batch\n",
        "\n",
        "q_mu_s = get_q_heads_mu_and_sigma(supermask_test_model, xs, 10000)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mu [21887.83 26996.55 39035.56] s [2435.595  3031.5972 4086.1343]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyUDDJgZrZN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = 0.5\n",
        "\n",
        "spmsk_model_q_all = SuperMaskQNets(k=K, q_head_index=None, q_means_stds=q_mu_s).to(device)\n",
        "spmsk_model_q0 = SuperMaskQNets(k=K, q_head_index=0, q_means_stds=q_mu_s).to(device)\n",
        "spmsk_model_q1 = SuperMaskQNets(k=K, q_head_index=1, q_means_stds=q_mu_s).to(device)\n",
        "spmsk_model_q2 = SuperMaskQNets(k=K, q_head_index=2, q_means_stds=q_mu_s).to(device)\n",
        "\n",
        "if params['use_qnet_weights']:\n",
        "    load_weights(spmsk_model_q_all)\n",
        "    load_weights(spmsk_model_q0)\n",
        "    load_weights(spmsk_model_q1)\n",
        "    load_weights(spmsk_model_q1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmBC-kAotIBQ",
        "colab_type": "text"
      },
      "source": [
        "## Train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEj7Qn1-H-Es",
        "colab_type": "code",
        "outputId": "f4272bd4-5e3a-483f-97f2-169009d5f2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "source": [
        "\"\"\"\n",
        "    Train/Test function for Randomly Weighted Hidden Neural Networks Techniques\n",
        "    Adapted from https://github.com/NesterukSergey/hidden-networks/blob/master/demos/mnist.ipynb\n",
        "\"\"\"\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "    \n",
        "    train_loss = 0\n",
        "    true_labels = []\n",
        "    predictions = [] # labels\n",
        "    outputs = [] # probabilities\n",
        "\n",
        "    model.train()\n",
        "    for data, target in itertools.islice(train_loader, params['num_train']):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        if params['use_mnist']:\n",
        "            target = (target > 0).float()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss\n",
        "        output_value = output.detach().cpu().numpy()\n",
        "        outputs.append(output)\n",
        "        pred = (output_value > 0.5).astype(float)\n",
        "        predictions.extend(pred)\n",
        "        true_labels.extend(target.detach().cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "    outputs = np.array(outputs)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    accuracy = np.sum(np.equal(predictions, true_labels)) / len(true_labels)\n",
        "    auc = roc_auc_score(true_labels, predictions)\n",
        "\n",
        "    return train_loss.item(), accuracy, auc\n",
        "\n",
        "\n",
        "def test(model, device, criterion, test_loader):\n",
        "    true_labels = []\n",
        "    predictions = [] # labels\n",
        "    outputs = [] # probabilities\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in itertools.islice(test_loader, params['num_val']):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if params['use_mnist']:\n",
        "                target = (target > 0).float()\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "\n",
        "            output_value = output.detach().cpu().numpy()\n",
        "            outputs.append(output)\n",
        "            pred = (output_value > 0.5).astype(float)\n",
        "            predictions.extend(pred)\n",
        "            true_labels.extend(target.detach().cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "    outputs = np.array(outputs)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = np.sum(np.equal(predictions, true_labels)) / len(true_labels)\n",
        "    auc = roc_auc_score(true_labels, predictions)\n",
        "\n",
        "    return test_loss.item(), accuracy, auc\n",
        "\n",
        "def run_model(model, num_epochs, verbose=False):\n",
        "  # NOTE: only pass the parameters where p.requires_grad == True to the optimizer! Important!\n",
        "  optimizer = optim.SGD(\n",
        "      [p for p in model.parameters() if p.requires_grad],\n",
        "      lr=0.1,\n",
        "      momentum=0.9,\n",
        "      weight_decay=0.0005,\n",
        "  )\n",
        "\n",
        "  criterion = nn.BCELoss().to(device)\n",
        "  scheduler = CosineAnnealingLR(optimizer, T_max=14)\n",
        "\n",
        "  train_accs = []\n",
        "  train_aucs = []\n",
        "  test_accs = []\n",
        "  test_aucs = []\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      train_loss, train_accuracy, train_auc = train(model, device, tr_data_loader, optimizer, criterion)\n",
        "      test_loss, test_accuracy, test_auc = test(model, device, criterion, val_data_loader)\n",
        "      if verbose:\n",
        "        print(f'Epoch {epoch}: train loss - {train_loss} / test loss {test_loss}')\n",
        "      scheduler.step()\n",
        "\n",
        "      train_accs.append(train_accuracy)\n",
        "      train_aucs.append(train_auc)\n",
        "      test_accs.append(test_accuracy)\n",
        "      test_aucs.append(test_auc)\n",
        "\n",
        "\n",
        "  print('Hyperparameters', params)\n",
        "\n",
        "  print('Train accuracy: ', train_accs[-1])\n",
        "  print('Test accuracy: ', test_accs[-1])\n",
        "\n",
        "  print('Train AUC: ', train_aucs[-1])  \n",
        "  print('Test AUC: ', test_aucs[-1])\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "run_model(spmsk_model_q_all, num_epochs=num_epochs)\n",
        "print()\n",
        "run_model(spmsk_model_q0, num_epochs=num_epochs)\n",
        "print()\n",
        "run_model(spmsk_model_q1, num_epochs=num_epochs)\n",
        "print()\n",
        "run_model(spmsk_model_q2, num_epochs=num_epochs)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:37<00:00,  2.18s/it]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters {'num_train': 50, 'num_val': 400, 'batch_size': 10, 'val_batch_size': 10, 'use_qnet_weights': True, 'use_mnist': True}\n",
            "Train accuracy:  0.994\n",
            "Test accuracy:  0.993\n",
            "Train AUC:  0.9693877551020409\n",
            "Test AUC:  0.978999648244108\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:24<00:00,  2.05s/it]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters {'num_train': 50, 'num_val': 400, 'batch_size': 10, 'val_batch_size': 10, 'use_qnet_weights': True, 'use_mnist': True}\n",
            "Train accuracy:  0.992\n",
            "Test accuracy:  0.99325\n",
            "Train AUC:  0.9545454545454545\n",
            "Test AUC:  0.9770928027230549\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:24<00:00,  2.05s/it]\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters {'num_train': 50, 'num_val': 400, 'batch_size': 10, 'val_batch_size': 10, 'use_qnet_weights': True, 'use_mnist': True}\n",
            "Train accuracy:  0.996\n",
            "Test accuracy:  0.9935\n",
            "Train AUC:  0.9877899877899878\n",
            "Test AUC:  0.9830924217658132\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:24<00:00,  2.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters {'num_train': 50, 'num_val': 400, 'batch_size': 10, 'val_batch_size': 10, 'use_qnet_weights': True, 'use_mnist': True}\n",
            "Train accuracy:  0.892\n",
            "Test accuracy:  0.901\n",
            "Train AUC:  0.5\n",
            "Test AUC:  0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}