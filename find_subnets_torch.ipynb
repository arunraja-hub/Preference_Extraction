{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "find-subnets-torch",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/fine_tune/find_subnets_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2PtbrFz01L5",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVH7LTvX00Y5",
        "colab_type": "code",
        "outputId": "958ccaef-b4a9-480d-b05c-c2760e435661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!git clone https://github.com/arunraja-hub/Preference_Extraction.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Preference_Extraction'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 669 (delta 40), reused 31 (delta 25), pack-reused 608\u001b[K\n",
            "Receiving objects: 100% (669/669), 21.61 MiB | 12.62 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xmOhgVIHOwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.autograd as autograd\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "\n",
        "import sys\n",
        "sys.path.append('Preference_Extraction/utils')\n",
        "import data_loading"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hU3eIxdF6IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1R6bB1u1B-l",
        "colab_type": "text"
      },
      "source": [
        "## Subnets Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzdNqZBANfjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Original code from What's hidden in a randomly weighted neural network? paper\n",
        "    Implemented at https://github.com/allenai/hidden-networks\n",
        "    Remove weigths-initialisation since it is not relevant for us\n",
        "\"\"\"\n",
        "\n",
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "        # Get the supermask by sorting the scores and using the top k%\n",
        "        out = scores.clone()\n",
        "        _, idx = scores.flatten().sort()\n",
        "        j = int((1 - k) * scores.numel())\n",
        "\n",
        "        # flat_out and out access the same memory.\n",
        "        flat_out = out.flatten()\n",
        "        flat_out[idx[:j]] = 0\n",
        "        flat_out[idx[j:]] = 1\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # send the gradient g straight-through on the backward pass.\n",
        "        return g, None\n",
        "\n",
        "class SupermaskConv(nn.Conv2d):\n",
        "    def __init__(self, *args, k, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "        # initialize the scores\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "\n",
        "        # initialize the weights\n",
        "        nn.init.uniform_(self.weight)\n",
        "        \n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        x = F.conv2d(\n",
        "            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
        "        )\n",
        "        return x\n",
        "\n",
        "class SupermaskLinear(nn.Linear):\n",
        "    def __init__(self, *args, k, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.k = k\n",
        "\n",
        "        # initialize the scores\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "\n",
        "        nn.init.uniform_(self.weight)\n",
        "\n",
        "        # NOTE: turn the gradient on the weights off\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        subnet = GetSubnet.apply(self.scores.abs(), self.k)\n",
        "        w = self.weight * subnet\n",
        "        return F.linear(x, w, self.bias)\n",
        "        return x\n",
        "\n",
        "# NOTE: not used here but we use NON-AFFINE Normalization!\n",
        "# So there is no learned parameters for your nomralization layer.\n",
        "class NonAffineBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, dim):\n",
        "        super(NonAffineBatchNorm, self).__init__(dim, affine=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLEWICr1H6K",
        "colab_type": "text"
      },
      "source": [
        "## Define Supermask Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Bbsd8pQPu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Original code from https://github.com/allenai/hidden-networks/blob/master/simple_mnist_example.py\n",
        "    Highly modified to work with PrefExtraction agent\n",
        "\"\"\"\n",
        "\n",
        "class SuperMaskNet(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(SuperMaskNet, self).__init__()\n",
        "        self.conv1 = SupermaskConv(in_channels=5, out_channels=16, kernel_size=3, stride=1, bias=True, k=k)\n",
        "        self.conv2 = SupermaskConv(in_channels=16, out_channels=32, kernel_size=3, stride=2, bias=True, k=k)\n",
        "        self.fc1 = SupermaskLinear(in_features=960, out_features=64, bias=True, k=k)\n",
        "        self.fc2 = SupermaskLinear(in_features=64, out_features=3, bias=True, k=k)  # 3 qHeads ouput\n",
        "\n",
        "    def fwd_conv1(self, x):\n",
        "        x = self.conv1(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_conv2(self, x):\n",
        "        x = self.fwd_conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return F.relu(x)\n",
        "\n",
        "    def fwd_flat(self, x):\n",
        "        x = self.fwd_conv2(x)\n",
        "        return torch.flatten(torch.transpose(x, 1, 3), 1) # Pre-flattening transpose is necessary for TF-Torch conversion\n",
        "\n",
        "    def fwd_dense(self, x):\n",
        "        x = self.fwd_flat(x)\n",
        "        x = self.fc1(x)\n",
        "        return F.relu(x)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fwd_dense(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.sigmoid(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHxPPHXH-G-",
        "colab_type": "code",
        "outputId": "0c8eb9a8-4afa-42f0-f1c1-eb94b8f2c444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYPowxHowZcH",
        "colab_type": "code",
        "outputId": "21470e93-e7c1-4c58-b425-fff8fdc1553e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Using a mock supermask model with k=1 because we want to first test that the two models are equivalent \n",
        "supermask_test_model = SuperMaskNet(k=1).to(device)\n",
        "summary(supermask_test_model, (5, 14, 16))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "     SupermaskConv-1           [-1, 16, 12, 14]             736\n",
            "     SupermaskConv-2             [-1, 32, 5, 6]           4,640\n",
            "   SupermaskLinear-3                   [-1, 64]          61,504\n",
            "   SupermaskLinear-4                    [-1, 3]             195\n",
            "================================================================\n",
            "Total params: 67,075\n",
            "Trainable params: 0\n",
            "Non-trainable params: 67,075\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.03\n",
            "Params size (MB): 0.26\n",
            "Estimated Total Size (MB): 0.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIHGI0a71NVI",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YikO82Jj1aWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_raw_data = all_load_data(\"Preference_Extraction/data/simple_env_1/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4htablfz1umt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activations = []\n",
        "observations = []\n",
        "preferences = []\n",
        "\n",
        "for data in all_raw_data:\n",
        "    for i in range(data.observation.shape[0]):\n",
        "        observations.append(np.copy(data.observation[i]))\n",
        "        activations.append(np.copy(data.policy_info[\"activations\"][i]))\n",
        "        preferences.append((data.policy_info['satisfaction'].as_list()[i] > -6).astype(int))\n",
        "\n",
        "activations = np.array(activations)\n",
        "observations = np.array(observations)\n",
        "preferences = np.array(preferences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpSHf83VSgoJ",
        "colab_type": "text"
      },
      "source": [
        "## Loading Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD-16xFHSi5r",
        "colab_type": "code",
        "outputId": "1f7cb06f-da9c-43dc-c97b-10d540c4a60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "new_save_path = \"Preference_Extraction/saved_model2\"\n",
        "restored_model = tf.keras.models.load_model(new_save_path)\n",
        "restored_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncodingNetwork/conv2d (Conv (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/dense (Dense (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 67,075\n",
            "Trainable params: 67,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB2G-FiUTmr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_weights=restored_model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL1eIitRUXYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check Shapes\n",
        "assert tuple(supermask_test_model.conv1.weight.data.shape) == np.transpose(original_weights[0]).shape\n",
        "assert tuple(supermask_test_model.conv1.bias.data.shape) == original_weights[1].shape\n",
        "assert tuple(supermask_test_model.conv2.weight.data.shape) == np.transpose(original_weights[2]).shape\n",
        "assert tuple(supermask_test_model.conv2.bias.data.shape) == original_weights[3].shape\n",
        "assert tuple(supermask_test_model.fc1.weight.data.shape) == np.transpose(original_weights[4]).shape\n",
        "assert tuple(supermask_test_model.fc1.bias.data.shape) == original_weights[5].shape\n",
        "assert tuple(supermask_test_model.fc2.weight.data.shape) == np.transpose(original_weights[6]).shape\n",
        "assert tuple(supermask_test_model.fc2.bias.data.shape) == original_weights[7].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oCGiuScTvXK",
        "colab_type": "code",
        "outputId": "0e50dc3c-4c3a-4781-b4bc-e99dce8b4699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Load Weights\n",
        "supermask_test_model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0]))\n",
        "supermask_test_model.conv1.bias.data = torch.from_numpy(original_weights[1])\n",
        "supermask_test_model.conv2.weight.data = torch.from_numpy(np.transpose(original_weights[2]))\n",
        "supermask_test_model.conv2.bias.data = torch.from_numpy(original_weights[3])\n",
        "\n",
        "supermask_test_model.fc1.weight.data = torch.from_numpy(np.transpose(original_weights[4]))\n",
        "supermask_test_model.fc1.bias.data = torch.from_numpy(original_weights[5])\n",
        "supermask_test_model.fc2.weight.data = torch.from_numpy(np.transpose(original_weights[6]))\n",
        "supermask_test_model.fc2.bias.data = torch.from_numpy(original_weights[7])\n",
        "\n",
        "supermask_test_model.to(device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuperMaskNet(\n",
              "  (conv1): SupermaskConv(5, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): SupermaskConv(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (fc1): SupermaskLinear(in_features=960, out_features=64, bias=True)\n",
              "  (fc2): SupermaskLinear(in_features=64, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXsgEQuAUwha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing that the models have identical observations for identical images\n",
        "tf_conv1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[0].output)\n",
        "tf_conv2_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[1].output)\n",
        "tf_flt_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[2].output)\n",
        "tf_fc1_fn = tf.keras.models.Model(inputs=restored_model.input, outputs=restored_model.layers[3].output)\n",
        "\n",
        "for i in range(len(all_raw_data[0].observation)):\n",
        "\n",
        "    single_observation = np.array([all_raw_data[0].observation[i]])\n",
        "    single_observation_torch = torch.Tensor(np.array([np.transpose(all_raw_data[0].observation[i])]))\n",
        "    single_observation_torch = single_observation_torch.to(device)\n",
        "    \n",
        "    conv1_torch_out = np.transpose(supermask_test_model.fwd_conv1(single_observation_torch).detach().cpu().numpy())\n",
        "    conv1_torch_out = conv1_torch_out.reshape(conv1_torch_out.shape[:-1])\n",
        "    conv1_tf_out = tf_conv1_fn(single_observation)[0].numpy()\n",
        "    np.testing.assert_allclose(conv1_torch_out, conv1_tf_out, rtol=.1)\n",
        "\n",
        "    conv2_torch_out = np.transpose(supermask_test_model.fwd_conv2(single_observation_torch).detach().cpu().numpy())\n",
        "    conv2_torch_out = conv2_torch_out.reshape(conv2_torch_out.shape[:-1])\n",
        "    conv2_tf_out = tf_conv2_fn(single_observation)[0].numpy()\n",
        "    np.testing.assert_allclose(conv2_torch_out, conv2_tf_out, rtol=.1)\n",
        "\n",
        "    flt_torch_out = np.transpose(supermask_test_model.fwd_flat(single_observation_torch).detach().cpu().numpy())\n",
        "    flt_torch_out = flt_torch_out.reshape(flt_torch_out.shape[:-1])\n",
        "    tf_flt_out = tf_flt_fn(single_observation)[0].numpy()\n",
        "    np.testing.assert_allclose(flt_torch_out, tf_flt_out, rtol=.1)\n",
        "\n",
        "    fc1_torch_out = np.transpose(supermask_test_model.fwd_dense(single_observation_torch).detach().cpu().numpy())\n",
        "    fc1_torch_out = fc1_torch_out.reshape(fc1_torch_out.shape[:-1])\n",
        "    fc1_tf_out = tf_fc1_fn(single_observation)[0].numpy()\n",
        "    \n",
        "    old_activations = all_raw_data[0].policy_info[\"activations\"][i]\n",
        "\n",
        "    np.testing.assert_allclose(fc1_torch_out, fc1_tf_out, rtol=.1)\n",
        "    np.testing.assert_allclose(fc1_torch_out, old_activations, rtol=.1)\n",
        "    np.testing.assert_allclose(old_activations, fc1_tf_out, rtol=.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVKe_Pozw0C",
        "colab_type": "text"
      },
      "source": [
        "## Training Supermask (incomplete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhAP7DAzy9Z",
        "colab_type": "code",
        "outputId": "f20f9342-6c4e-4068-da25-2fd1ab626fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Creating and loading weights\n",
        "spmsk_model = SuperMaskNet(k=1).to(device)\n",
        "\n",
        "spmsk_model.conv1.weight.data = torch.from_numpy(np.transpose(original_weights[0]))\n",
        "spmsk_model.conv1.bias.data = torch.from_numpy(original_weights[1])\n",
        "spmsk_model.conv2.weight.data = torch.from_numpy(np.transpose(original_weights[2]))\n",
        "spmsk_model.conv2.bias.data = torch.from_numpy(original_weights[3])\n",
        "spmsk_model.fc1.weight.data = torch.from_numpy(np.transpose(original_weights[4]))\n",
        "spmsk_model.fc1.bias.data = torch.from_numpy(original_weights[5])\n",
        "spmsk_model.fc2.weight.data = torch.from_numpy(np.transpose(original_weights[6]))\n",
        "spmsk_model.fc2.bias.data = torch.from_numpy(original_weights[7])\n",
        "\n",
        "spmsk_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuperMaskNet(\n",
              "  (conv1): SupermaskConv(5, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): SupermaskConv(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "  (fc1): SupermaskLinear(in_features=960, out_features=64, bias=True)\n",
              "  (fc2): SupermaskLinear(in_features=64, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66aDupQf2OAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataset iterators\n",
        "num_train = 50\n",
        "num_val = 400\n",
        "batch_size = 50\n",
        "val_batch_size = 50\n",
        "\n",
        "xs = np.rollaxis(observations, 3, 1) # Torch wants channel-first\n",
        "ys = preferences\n",
        "xs, ys = shuffle(xs, ys)\n",
        "\n",
        "xs_tr = xs[:num_train]\n",
        "ys_tr = ys[:num_train]\n",
        "xs_val = xs[num_train:num_train+num_val]\n",
        "ys_val = ys[num_train:num_train+num_val]\n",
        "\n",
        "tr_data_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.Tensor(xs_tr), torch.Tensor(ys_tr)),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.Tensor(xs_val), torch.Tensor(ys_val)),\n",
        "    batch_size=val_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEj7Qn1-H-Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Train/Test function for Randomly Weighted Hidden Neural Networks Techniques\n",
        "    Adapted from https://github.com/NesterukSergey/hidden-networks/blob/master/demos/mnist.ipynb\n",
        "\"\"\"\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch, verbose=False):\n",
        "    train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss\n",
        "\n",
        "        if verbose:\n",
        "          if batch_idx % 5 == 0:\n",
        "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\n",
        "              \n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def test(model, device, criterion, test_loader):\n",
        "    true_labels = []\n",
        "    predictions = [] # labels\n",
        "    outputs = [] # probabilities\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            outputs.append(output.detach().cpu().numpy())\n",
        "            pred = output > 0.5\n",
        "\n",
        "            predictions.extend(pred)\n",
        "            true_labels.extend(target.detach().cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = np.sum(np.equal(predictions, true_labels)) / len(true_labels)\n",
        "    \n",
        "    return test_loss.item(), test_accuracy\n",
        "\n",
        "def run_model(k, model, num_epochs):\n",
        "  # NOTE: only pass the parameters where p.requires_grad == True to the optimizer! Important!\n",
        "  optimizer = optim.SGD(\n",
        "      [p for p in model.parameters() if p.requires_grad],\n",
        "      lr=0.1,\n",
        "      momentum=0.9,\n",
        "      weight_decay=0.0005,\n",
        "  )\n",
        "\n",
        "  criterion = nn.BCELoss().to(device)\n",
        "  scheduler = CosineAnnealingLR(optimizer, T_max=14)\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  test_acc = []\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "      train_loss = train(model, device, tr_data_loader, optimizer, criterion, epoch, verbose=False)\n",
        "      test_loss, test_accuracy = test(model, device, criterion, val_data_loader)\n",
        "      scheduler.step()\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      test_losses.append(test_loss)\n",
        "      test_acc.append(test_accuracy)\n",
        "\n",
        "  print('k: ', k, '  init: ', init)\n",
        "  print('Test loss: ', test_losses[-1])\n",
        "  print('Test accuracy: ', test_acc[-1])\n",
        "\n",
        "# run_model(0.3, spmsk_model, num_epochs=400)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}