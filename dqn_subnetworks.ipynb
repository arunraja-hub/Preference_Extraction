{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn-subnetworks.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "1u9QVVsShC9X"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/dqn/dqn_subnetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEHR2Ui-lo8O",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install --upgrade tensorflow-probability\n",
        "!pip install tf-agents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMitx5qSgJk1",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J6HsdS5GbSjd",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "tf.version.VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ",
        "colab": {}
      },
      "source": [
        "num_iterations = 2000\n",
        "initial_collect_steps = 1000\n",
        "collect_steps_per_iteration = 1\n",
        "replay_buffer_max_length = 100000\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "log_interval = 200\n",
        "num_eval_episodes = 10\n",
        "eval_interval = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYEz-S9gEv2-",
        "colab": {}
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict QValues (expected returns) for all actions, given an observation from the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgkdEPg_muzV",
        "colab": {}
      },
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vX2zGUWJGWAl",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer.\n",
        "collect_step adds/remembers time_step,action,trajectory,next_time_step of each step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wr1KSAEGG4h9",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://github.com/tensorflow/agents/blob/master/tf_agents/docs/python/tf_agents/drivers.md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7psMsFL0th9k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "*   batch_size=64\n",
        "*   possible actions=2\n",
        "*   possible states=4\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ba7bilizt_qW",
        "colab": {}
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "print( dataset)\n",
        "iterator = iter(dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0pTbJ3PeyF-u",
        "colab": {}
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "  ########  InvalidArgumentError: assertion failed: [TFUniformReplayBuffer is empty.\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "  iterator = iter(dataset)\n",
        "  ########\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7-bzG-CQi3",
        "colab_type": "text"
      },
      "source": [
        "## Subnetworks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAc5tuSeCScS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# QNet Structure\n",
        "for v in q_net.variables:\n",
        "    print(v.name, v.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4RtRaULDLhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Potential Implementation for Preference Extraction\n",
        "'''\n",
        "    Take RL agent Q network A\n",
        "    \n",
        "    For each neuron n of output layer   --- 2 neuron action output\n",
        "    Get subnetwork A’ as a copy of A but with single neuron n as output layer   --- single action output?\n",
        "    Assign each weight in A’ a score (initialise scores with Kaming uniform)     --- Kaming uniform?\n",
        "    Modify single neuron output layer in the following way\n",
        "    q = value of Q-head n\n",
        "    q = sigmoid(q - n_mean / n_std)\n",
        "    loss = binary_cross_entropy(q, preference)\n",
        "    In forward run batch of (observation, preference) pairs in A’ using top k% scores\n",
        "    In backward pass update scores by subtracting: learning rate * derivative of loss with respect to weight head neuron * weight * activation of weight tail neuron\n",
        "    Run for 100 epochs\n",
        "    Obtain best subnetwork A’’ from A’\n",
        "    Predict preferences using ensemble of subnetworks (one for each Q-head)\n",
        "    Ensemble voting can be done by majority or by most-confident vote\n",
        "    Ensemble voting can be used to do active reward modelling\n",
        "    Start with a training pairs (obs, prefs)\n",
        "    Predict prefs for b >> a observations\n",
        "    Query labels for |a| observations in b for which we have max std of predictions\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0bycyOZE1ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run A for t observations and store mean and std of each Q-head in the output layer\n",
        "q_heads_values = []\n",
        "for _ in range(num_iterations):\n",
        "    experience, unused_info = next(iterator)\n",
        "    logits, _ = q_net.call(observation=experience.observation)\n",
        "    q_heads_values.append(np.array(logits).reshape((64*2 ,2)))\n",
        "\n",
        "q_heads_values = np.concatenate(q_heads_values)\n",
        "q_heads_means = q_heads_values.mean(axis=0)\n",
        "q_heads_stds = q_heads_values.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KF3DlrqKisa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUguZvq8Jv45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each neuron n of output layer\n",
        "\n",
        "for qHix in [0, 1]:\n",
        "\n",
        "    # Get subnetwork A’ as a copy of A but with single neuron n as output layer\n",
        "    q_head_net = deepcopy(q_net)\n",
        "    q_head_net_W = q_head_net.get_weights()\n",
        "    q_head_net_W_qHix = q_head_net_W[-1][qHix]\n",
        "    q_head_net_W[-1] = np.zeros(shape=q_head_net_W[-1].shape)\n",
        "    q_head_net_W[-1][qHix] = q_head_net_W_qHix\n",
        "    q_head_net.set_weights(q_head_net_W)\n",
        "\n",
        "    # Assign each weight in A’ a score (initialise scores with Kaming uniform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_LXdxCKltc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}