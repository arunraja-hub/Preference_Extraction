{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract_preferences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7avR2MCrpdldFTj4nrkkU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/master/extract_preferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGhMeaXwica2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import io\n",
        "import collections\n",
        "\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-GaSDSZzSYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Hacks to make pickle work.\n",
        "\n",
        "class Trajectory(\n",
        "    collections.namedtuple('Trajectory', [\n",
        "        'step_type',\n",
        "        'observation',\n",
        "        'action',\n",
        "        'policy_info',\n",
        "        'next_step_type',\n",
        "        'reward',\n",
        "        'discount',\n",
        "    ])):\n",
        "  __slots__ = ()\n",
        "\n",
        "class ListWrapper(object):\n",
        "  def __init__(self, list_to_wrap):\n",
        "    self._list = list_to_wrap\n",
        "\n",
        "  def as_list(self):\n",
        "    return self._list\n",
        "\n",
        "class RenameUnpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "      if name == \"Trajectory\":\n",
        "        return Trajectory\n",
        "      if name == \"ListWrapper\":\n",
        "        return ListWrapper\n",
        "\n",
        "      return super(RenameUnpickler, self).find_class(module, name)\n",
        "\n",
        "def rename_load(s):\n",
        "    \"\"\"Helper function analogous to pickle.loads().\"\"\"\n",
        "    return RenameUnpickler(s, encoding='latin1').load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_pxqpKjOda",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Load all of the data\n",
        "# If this takes more than a minute, stop and restart it.\n",
        "\n",
        "def load_file(full_path):\n",
        "  try:\n",
        "    with urllib.request.urlopen(full_path) as f:\n",
        "      data = rename_load(f)\n",
        "    return data\n",
        "  except HTTPError:\n",
        "    pass\n",
        "\n",
        "def all_load_data(base_path):\n",
        "  executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
        "  \n",
        "  futures = []\n",
        "  for i in range(5000):\n",
        "    full_path = os.path.join(base_path, \"ts\"+str(i)+\".pickle?raw=true\")\n",
        "    future = executor.submit(load_file, full_path)\n",
        "    futures.append(future)\n",
        "\n",
        "  raw_data = []\n",
        "  for future in concurrent.futures.as_completed(futures):\n",
        "    result = future.result()\n",
        "    if result:\n",
        "      raw_data.append(result)\n",
        "\n",
        "  return raw_data\n",
        "\n",
        "# Need this useless load or else the all_load_data will hang forever the first time it's called.\n",
        "load_file(\"https://github.com/arunraja-hub/Preference_Extraction/blob/master/data/simple_env_1/ts10.pickle?raw=true\")\n",
        "\n",
        "all_raw_data = all_load_data(\"https://github.com/arunraja-hub/Preference_Extraction/blob/master/data/simple_env_1/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLw7aBy67gnH",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "b14fc2af-b72f-4bfd-9887-777e6ee6449e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# @title Preprocess the data into x,y training pairs\n",
        "# @markdown The use_activations control which data is used.\n",
        "# @markdown All of the cells below use the data specified here.\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "# Rerun this cell after setting these to different values to train on a different dataset.\n",
        "use_activations = True # @param\n",
        "\n",
        "for data in all_raw_data:\n",
        "  for i in range(data.observation.shape[0]):\n",
        "\n",
        "    if use_activations:\n",
        "      x = np.copy(data.policy_info[\"activations\"][i])\n",
        "    else:\n",
        "      x = np.copy(data.observation[i])\n",
        "\n",
        "    y = data.policy_info['satisfaction'].as_list()[i] > -6\n",
        "\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n",
        "\n",
        "xs = np.array(xs)\n",
        "ys = np.array(ys).astype(int)\n",
        "\n",
        "xs, ys = shuffle(xs, ys)\n",
        "\n",
        "print(\"xs\", xs.shape, \"ys\", ys.shape)\n",
        "print(\"ys 1\", np.sum(ys))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xs (23750, 64) ys (23750,)\n",
            "ys 1 9569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpNLoX8t8vC7",
        "colab_type": "code",
        "outputId": "20af1e48-b7be-4099-e804-b57e46912c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# @title Visualize and example\n",
        "rand_index = random.randint(0,1000)\n",
        "\n",
        "if use_activations == False:\n",
        "  print(\"Color channels:\")\n",
        "  plt.imshow(xs[rand_index,:,:,:3], interpolation=\"none\")\n",
        "  plt.show()\n",
        "  print(\"Remaining time channel:\")\n",
        "  plt.imshow(xs[rand_index,:,:,3], interpolation=\"none\")\n",
        "  plt.show()\n",
        "  print(\"A different value for each coordinate to help with convolution:\")\n",
        "  plt.imshow(xs[rand_index,:,:,4], interpolation=\"none\")\n",
        "  plt.show()\n",
        "else:\n",
        "  print(\"x\", xs[rand_index])\n",
        "print(\"y\", ys[rand_index])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x [  0.         0.         0.         0.        41.159344   0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "  52.636536   0.       101.553314   0.         0.         0.\n",
            "   0.         0.       123.541855   0.         0.         0.\n",
            "  18.645014   0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.        29.579803   0.         0.      ]\n",
            "y 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_yWm1n9Fp1",
        "colab_type": "code",
        "outputId": "17ca7e54-426f-4df9-8740-6338da98145b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_val_auc(logs):\n",
        "      for key in logs:\n",
        "        if key.startswith('val_auc'):\n",
        "          return logs[key]\n",
        "\n",
        "class BestStats(tf.keras.callbacks.Callback):\n",
        "  \"\"\"A callback to keep track of the best val accuracy and auc seen so far.\"\"\"\n",
        "  def on_train_begin(self, logs):\n",
        "      self.bestMetric = -float('inf')\n",
        "      self.bestLogs = None\n",
        "      self.bestTrain = -float('inf')\n",
        "      self.num_epochs = 0\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    self.num_epochs += 1\n",
        "    self.bestTrain = max(self.bestTrain, logs.get('accuracy'))\n",
        "\n",
        "    val_accuracy = logs.get('val_accuracy')\n",
        "    if val_accuracy == None:\n",
        "      return \n",
        "\n",
        "    val_auc = get_val_auc(logs)\n",
        "    \n",
        "    metric = (val_accuracy + val_auc) / 2.0\n",
        "\n",
        "    if metric > self.bestMetric:\n",
        "      self.bestMetric = metric\n",
        "      self.bestLogs = logs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfVZf7rX9MZe",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Image model.\n",
        "# @markdown Run this cell iff use_activations=False\n",
        "# @markdown If you're trying to improve the accuracy of the model trained on activations, you won't care about this cell.\n",
        "\n",
        "def get_model(reg_amount, drop_rate):\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # This layer gets one of the color channels. It works better than using all of them.\n",
        "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,tf.random.uniform((), 0,4,tf.int32)], 3), input_shape=xs.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(32, 2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.Conv2D(16, 1, activation='relu', strides=1, kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(drop_rate),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy',\n",
        "                        tf.keras.metrics.AUC()\n",
        "                        ],\n",
        "                )\n",
        "  return model\n",
        "all_hparam_possibilities = [{\"reg_amount\": [0.0], \"drop_rate\": [0.0],}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Xcc_Zx_Any",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Model for training on the network activations\n",
        "# @markdown Run this cell iff use_activations=True\n",
        "\n",
        "def get_model(reg_amount, drop_rate, layer_sizes):\n",
        "  layers = []\n",
        "  for layer_size in layer_sizes:\n",
        "    layers.append(tf.keras.layers.Dense(layer_size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)))\n",
        "    layers.append(tf.keras.layers.Dropout(drop_rate))\n",
        "\n",
        "  model = tf.keras.models.Sequential(layers + [\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount))\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', tf.keras.metrics.AUC()],\n",
        "                )\n",
        "  return model\n",
        "\n",
        "all_hparam_possibilities = [{\"drop_rate\": [.2], \"layer_sizes\": [(16,), (32,)], \"reg_amount\": [.5]}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2EQMqS_ayd",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "5ed106c4-6138-4c43-8d81-64a38c08c66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# @title Train the model\n",
        "# @markdown This tries all the combinations of hparams and picks the best one.\n",
        "# @markdown For each combination of hparams, it averages over 5 different train val splits.\n",
        "# @markdown It re runs the best hyperparameters at the end.\n",
        "\n",
        "num_train = 50\n",
        "num_val = 1000\n",
        "epochs = 200\n",
        "print(\"use_activations:\", use_activations, \"num_train:\", num_train, \"epochs\", epochs)\n",
        "if num_train > 50:\n",
        "  print(\"More than 50 train data!!!!!!!!\")\n",
        "\n",
        "# each item in all_hparam_possibilities specifies valid hyper params to try. Put parameters that don't make sense together in separate lists.\n",
        "\n",
        "hparam_combinations = []\n",
        "for hparam_possibilities in all_hparam_possibilities:\n",
        "  hparam_keys, hparam_values = zip(*hparam_possibilities.items())\n",
        "  hparam_combinations.extend([dict(zip(hparam_keys, v)) for v in itertools.product(*hparam_values)])\n",
        "random.shuffle(hparam_combinations)\n",
        "print(\"len(hparam_combinations)\", len(hparam_combinations), \"hparam_combinations\", hparam_combinations)\n",
        "\n",
        "def train_best_logs(xs, ys, do_summary, hparams):\n",
        "  \"\"\"Trains the model and retruns the logs of the best epoch. randomly splits the train and val data before training.\"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  model = get_model(**hparams)\n",
        "  xs, ys = shuffle(xs, ys)\n",
        "\n",
        "  xs_val = xs[num_train:num_train+num_val]\n",
        "  ys_val = ys[num_train:num_train+num_val]\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=0)\n",
        "  best_stats = BestStats()\n",
        "  model.fit(xs[:num_train], ys[:num_train], epochs=epochs, batch_size=256, validation_freq=1, callbacks=[best_stats, early_stopping], validation_data=(xs_val, ys_val), verbose=0)\n",
        "  if do_summary:\n",
        "    model.summary()\n",
        "    print(\"best train accuracy:\", best_stats.bestTrain)\n",
        "    print(\"Number of epochs:\", best_stats.num_epochs)\n",
        "  return best_stats.bestLogs\n",
        "\n",
        "def multiple_train_ave(hparams):\n",
        "  \"\"\"Trains the model multiple times with the same parameters and returns the average metrics\"\"\"\n",
        "  start = time.time()\n",
        "  all_val_auc = []\n",
        "  all_val_accuracy = []\n",
        "\n",
        "  do_summary = True\n",
        "  for i in range(5):\n",
        "    logs = train_best_logs(xs, ys, do_summary, hparams)\n",
        "    all_val_auc.append(get_val_auc(logs))\n",
        "    all_val_accuracy.append(logs.get('val_accuracy'))\n",
        "    do_summary = False \n",
        "\n",
        "  mean_val_auc = np.mean(all_val_auc)\n",
        "  mean_val_accuracy = np.mean(all_val_accuracy)\n",
        "  metric = (mean_val_auc + mean_val_accuracy) / 2.0\n",
        "  print_data = (\"mean_val_auc\", mean_val_auc, \"mean_val_accuracy\", mean_val_accuracy, \"metric\", metric, \"val_auc_std\", np.std(all_val_auc), \"val_accuracy_std\", np.std(all_val_accuracy))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Seconds per hyperparam config\", end - start)\n",
        "  # GPU: ('Seconds per hyperparam config', 16.970870971679688)\n",
        "\n",
        "  return metric, print_data\n",
        "\n",
        "best_metric = -float('inf')\n",
        "\n",
        "run_num = 0\n",
        "for hparams in hparam_combinations:\n",
        "  print(\"hparams\", hparams)\n",
        "\n",
        "  metric, print_data = multiple_train_ave(hparams)\n",
        "\n",
        "  print(print_data)\n",
        "  if metric > best_metric:\n",
        "    best_metric = metric\n",
        "    best_print_data = print_data\n",
        "    best_hparams = hparams\n",
        "\n",
        "  run_num += 1\n",
        "  print(\"fract done\", run_num/float(len(hparam_combinations)))\n",
        "  print\n",
        "  print(\"==============================================================================================\")\n",
        "  print\n",
        "  sys.stdout.flush()\n",
        "\n",
        "print(\"best_hparams\", best_hparams)\n",
        "print(\"best results\", best_print_data)\n",
        "print(\"Retraining on the best_hparams to make sure we didn't just get good results by random chance.\")\n",
        "\n",
        "_, print_data = multiple_train_ave(best_hparams)\n",
        "print(\"Result of retrain on the best hyperparameters\", print_data)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use_activations: True num_train: 50 epochs 200\n",
            "len(hparam_combinations) 2 hparam_combinations [{'drop_rate': 0.2, 'layer_sizes': (16,), 'reg_amount': 0.5}, {'drop_rate': 0.2, 'layer_sizes': (32,), 'reg_amount': 0.5}]\n",
            "hparams {'drop_rate': 0.2, 'layer_sizes': (16,), 'reg_amount': 0.5}\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  1040      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  17        \n",
            "=================================================================\n",
            "Total params: 1,057\n",
            "Trainable params: 1,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 1.0\n",
            "Number of epochs: 200\n",
            "Seconds per hyperparam config 33.334991693496704\n",
            "('mean_val_auc', 0.8885616, 'mean_val_accuracy', 0.8304, 'metric', 0.8594807982444763, 'val_auc_std', 0.028811943, 'val_accuracy_std', 0.030342042)\n",
            "fract done 0.5\n",
            "==============================================================================================\n",
            "hparams {'drop_rate': 0.2, 'layer_sizes': (32,), 'reg_amount': 0.5}\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  2080      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  33        \n",
            "=================================================================\n",
            "Total params: 2,113\n",
            "Trainable params: 2,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 1.0\n",
            "Number of epochs: 200\n",
            "Seconds per hyperparam config 23.52359914779663\n",
            "('mean_val_auc', 0.904315, 'mean_val_accuracy', 0.84559995, 'metric', 0.8749574422836304, 'val_auc_std', 0.0047429316, 'val_accuracy_std', 0.008212183)\n",
            "fract done 1.0\n",
            "==============================================================================================\n",
            "best_hparams {'drop_rate': 0.2, 'layer_sizes': (32,), 'reg_amount': 0.5}\n",
            "best results ('mean_val_auc', 0.904315, 'mean_val_accuracy', 0.84559995, 'metric', 0.8749574422836304, 'val_auc_std', 0.0047429316, 'val_accuracy_std', 0.008212183)\n",
            "Retraining on the best_hparams to make sure we didn't just get good results by random chance.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  2080      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  33        \n",
            "=================================================================\n",
            "Total params: 2,113\n",
            "Trainable params: 2,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.98\n",
            "Number of epochs: 200\n",
            "Seconds per hyperparam config 23.974591970443726\n",
            "Result of retrain on the best hyperparameters ('mean_val_auc', 0.9001616, 'mean_val_accuracy', 0.8498, 'metric', 0.8749808073043823, 'val_auc_std', 0.023722557, 'val_accuracy_std', 0.019853469)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}