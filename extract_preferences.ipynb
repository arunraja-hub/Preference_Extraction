{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract_preferences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3jrcNpKg/t0lSYoLiQW+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/adding_unsupervised/extract_preferences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_IZz0TlnzhQ",
        "colab_type": "text"
      },
      "source": [
        "Click \"open in colab\" above to run. No need to download.\n",
        "Change the runtime type to GPU or TPU to make it faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKmidrQSH8bZ",
        "colab_type": "text"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9uu_Ht_oXGx",
        "colab_type": "text"
      },
      "source": [
        "# Data and import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGhMeaXwica2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import io\n",
        "import collections\n",
        "\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-GaSDSZzSYt",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Hacks to make pickle work.\n",
        "\n",
        "class Trajectory(\n",
        "    collections.namedtuple('Trajectory', [\n",
        "        'step_type',\n",
        "        'observation',\n",
        "        'action',\n",
        "        'policy_info',\n",
        "        'next_step_type',\n",
        "        'reward',\n",
        "        'discount',\n",
        "    ])):\n",
        "  \"\"\"Stores the observation the agent saw and the action it took.\n",
        "      The rest of the attributes aren't used in this code.\"\"\"\n",
        "  __slots__ = ()\n",
        "\n",
        "class ListWrapper(object):\n",
        "  def __init__(self, list_to_wrap):\n",
        "    self._list = list_to_wrap\n",
        "\n",
        "  def as_list(self):\n",
        "    return self._list\n",
        "\n",
        "class RenameUnpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "      if name == \"Trajectory\":\n",
        "        return Trajectory\n",
        "      if name == \"ListWrapper\":\n",
        "        return ListWrapper\n",
        "\n",
        "      return super(RenameUnpickler, self).find_class(module, name)\n",
        "\n",
        "def rename_load(s):\n",
        "    \"\"\"Helper function analogous to pickle.loads().\"\"\"\n",
        "    return RenameUnpickler(s, encoding='latin1').load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_pxqpKjOda",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Load all of the data\n",
        "# If this takes more than a minute, stop and restart it.\n",
        "# TODO: figure out why this cell always hangs the first time it's run.\n",
        "\n",
        "def load_file(full_path):\n",
        "  try:\n",
        "    with urllib.request.urlopen(full_path) as f:\n",
        "      data = rename_load(f)\n",
        "    return data\n",
        "  except HTTPError:\n",
        "    pass\n",
        "\n",
        "def all_load_data(base_path):\n",
        "  executor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
        "  \n",
        "  futures = []\n",
        "  for i in range(5000):\n",
        "    full_path = os.path.join(base_path, \"ts\"+str(i)+\".pickle?raw=true\")\n",
        "    future = executor.submit(load_file, full_path)\n",
        "    futures.append(future)\n",
        "\n",
        "  raw_data = []\n",
        "  for future in concurrent.futures.as_completed(futures):\n",
        "    result = future.result()\n",
        "    if result:\n",
        "      raw_data.append(result)\n",
        "\n",
        "  return raw_data\n",
        "\n",
        "# Need this useless load or else the all_load_data will hang forever the first time it's called.\n",
        "load_file(\"https://github.com/arunraja-hub/Preference_Extraction/blob/master/data/simple_env_1/ts10.pickle?raw=true\")\n",
        "\n",
        "all_raw_data = all_load_data(\"https://github.com/arunraja-hub/Preference_Extraction/blob/master/data/simple_env_1/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLw7aBy67gnH",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "648990b7-b7d5-4dcd-f20d-4ec8c6743ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# @title Preprocess the data into x,y training pairs\n",
        "# @markdown The use_activations control which data is used.\n",
        "# @markdown All of the cells below use the data specified here.\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "# Rerun this cell after setting these to different values to train on a different dataset.\n",
        "use_activations = True # @param\n",
        "\n",
        "for data in all_raw_data:\n",
        "  for i in range(data.observation.shape[0]):\n",
        "\n",
        "    if use_activations:\n",
        "      x = np.copy(data.policy_info[\"activations\"][i])\n",
        "    else:\n",
        "      x = np.copy(data.observation[i])\n",
        "\n",
        "    y = data.policy_info['satisfaction'].as_list()[i] > -6\n",
        "\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n",
        "\n",
        "xs = np.array(xs)\n",
        "ys = np.array(ys).astype(int)\n",
        "\n",
        "xs, ys = shuffle(xs, ys)\n",
        "\n",
        "print(\"xs\", xs.shape, \"ys\", ys.shape)\n",
        "print(\"ys 1\", np.sum(ys))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xs (3350, 64) ys (3350,)\n",
            "ys 1 1320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpNLoX8t8vC7",
        "colab_type": "code",
        "outputId": "87b4125c-1c02-48b2-c768-bc1f65476de6",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# @title Visualize and example\n",
        "rand_index = random.randint(0,1000)\n",
        "\n",
        "if use_activations == False:\n",
        "  print(\"Color channels:\")\n",
        "  plt.imshow(xs[rand_index,:,:,:3], interpolation=\"none\")\n",
        "  plt.show()\n",
        "  print(\"Remaining time channel:\")\n",
        "  plt.imshow(xs[rand_index,:,:,3], interpolation=\"none\")\n",
        "  plt.show()\n",
        "  print(\"A different value for each coordinate to help with convolution:\")\n",
        "  plt.imshow(xs[rand_index,:,:,4], interpolation=\"none\")\n",
        "  plt.show()\n",
        "else:\n",
        "  print(\"x\", xs[rand_index])\n",
        "print(\"y\", ys[rand_index])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x [  0.         0.         0.         0.        13.35232    0.\n",
            "   0.        18.336212   0.         0.         0.        17.459444\n",
            "   0.        60.982452   0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "  16.84235    0.         0.         0.         0.         0.\n",
            "   0.         0.       109.10112    0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.        56.430775   0.         0.      ]\n",
            "y 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPKA64IvojNR",
        "colab_type": "text"
      },
      "source": [
        "# ML models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSIP8sjjGdiC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fad92848-65f5-49e4-f597-78a2acef795f"
      },
      "source": [
        "# @title ChannelReducer from lucid\n",
        "# Copied from https://github.com/tensorflow/lucid/blob/master/lucid/misc/channel_reducer.py\n",
        "\n",
        "# Copyright 2018 The Lucid Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Helper for using sklearn.decomposition on high-dimensional tensors.\n",
        "\n",
        "Provides ChannelReducer, a wrapper around sklearn.decomposition to help them\n",
        "apply to arbitrary rank tensors. It saves lots of annoying reshaping.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "\n",
        "try:\n",
        "    from sklearn.decomposition.base import BaseEstimator\n",
        "except AttributeError:\n",
        "    from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class ChannelReducer(object):\n",
        "  \"\"\"Helper for dimensionality reduction to the innermost dimension of a tensor.\n",
        "\n",
        "  This class wraps sklearn.decomposition classes to help them apply to arbitrary\n",
        "  rank tensors. It saves lots of annoying reshaping.\n",
        "\n",
        "  See the original sklearn.decomposition documentation:\n",
        "  http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_components=3, reduction_alg=\"NMF\", **kwargs):\n",
        "    \"\"\"Constructor for ChannelReducer.\n",
        "\n",
        "    Inputs:\n",
        "      n_components: Numer of dimensions to reduce inner most dimension to.\n",
        "      reduction_alg: A string or sklearn.decomposition class. Defaults to\n",
        "        \"NMF\" (non-negative matrix facotrization). Other options include:\n",
        "        \"PCA\", \"FastICA\", and \"MiniBatchDictionaryLearning\". The name of any of\n",
        "        the sklearn.decomposition classes will work, though.\n",
        "      kwargs: Additional kwargs to be passed on to the reducer.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(n_components, int):\n",
        "      raise ValueError(\"n_components must be an int, not '%s'.\" % n_components)\n",
        "\n",
        "    # Defensively look up reduction_alg if it is a string and give useful errors.\n",
        "    algorithm_map = {}\n",
        "    for name in dir(sklearn.decomposition):\n",
        "      obj = sklearn.decomposition.__getattribute__(name)\n",
        "      if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
        "        algorithm_map[name] = obj\n",
        "    if isinstance(reduction_alg, str):\n",
        "      if reduction_alg in algorithm_map:\n",
        "        reduction_alg = algorithm_map[reduction_alg]\n",
        "      else:\n",
        "        raise ValueError(\"Unknown dimensionality reduction method '%s'.\" % reduction_alg)\n",
        "\n",
        "\n",
        "    self.n_components = n_components\n",
        "    self._reducer = reduction_alg(n_components=n_components, **kwargs)\n",
        "    self._is_fit = False\n",
        "\n",
        "  @classmethod\n",
        "  def _apply_flat(cls, f, acts):\n",
        "    \"\"\"Utility for applying f to inner dimension of acts.\n",
        "\n",
        "    Flattens acts into a 2D tensor, applies f, then unflattens so that all\n",
        "    dimesnions except innermost are unchanged.\n",
        "    \"\"\"\n",
        "    orig_shape = acts.shape\n",
        "    acts_flat = acts.reshape([-1, acts.shape[-1]])\n",
        "    new_flat = f(acts_flat)\n",
        "    if not isinstance(new_flat, np.ndarray):\n",
        "      return new_flat\n",
        "    shape = list(orig_shape[:-1]) + [-1]\n",
        "    return new_flat.reshape(shape)\n",
        "\n",
        "  def fit(self, acts):\n",
        "    self._is_fit = True\n",
        "    return ChannelReducer._apply_flat(self._reducer.fit, acts)\n",
        "\n",
        "  def fit_transform(self, acts):\n",
        "    self._is_fit = True\n",
        "    return ChannelReducer._apply_flat(self._reducer.fit_transform, acts)\n",
        "\n",
        "  def transform(self, acts):\n",
        "    return ChannelReducer._apply_flat(self._reducer.transform, acts)\n",
        "\n",
        "  def __call__(self, acts):\n",
        "    if self._is_fit:\n",
        "      return self.transform(acts)\n",
        "    else:\n",
        "      return self.fit_transform(acts)\n",
        "\n",
        "  def __getattr__(self, name):\n",
        "    if name in self.__dict__:\n",
        "      return self.__dict__[name]\n",
        "    elif name + \"_\" in self._reducer.__dict__:\n",
        "      return self._reducer.__dict__[name+\"_\"]\n",
        "\n",
        "  def __dir__(self):\n",
        "    dynamic_attrs = [name[:-1]\n",
        "                     for name in dir(self._reducer)\n",
        "                     if name[-1] == \"_\" and name[0] != \"_\"\n",
        "                    ]\n",
        "\n",
        "    return list(ChannelReducer.__dict__.keys()) + list(self.__dict__.keys()) + dynamic_attrs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.decomposition.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_yWm1n9Fp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "330628cf-3309-4201-97fd-ada3e630e7fa"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_val_auc(logs):\n",
        "      for key in logs:\n",
        "        if key.startswith('val_auc'):\n",
        "          return logs[key]\n",
        "\n",
        "class BestStats(tf.keras.callbacks.Callback):\n",
        "  \"\"\"A callback to keep track of the best val accuracy and auc seen so far.\"\"\"\n",
        "  def on_train_begin(self, logs):\n",
        "      self.bestMetric = -float('inf')\n",
        "      self.bestLogs = None\n",
        "      self.bestTrain = -float('inf')\n",
        "      self.num_epochs = 0\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    self.num_epochs += 1\n",
        "    self.bestTrain = max(self.bestTrain, logs.get('accuracy'))\n",
        "\n",
        "    val_accuracy = logs.get('val_accuracy')\n",
        "    if val_accuracy == None:\n",
        "      return \n",
        "\n",
        "    val_auc = get_val_auc(logs)\n",
        "    \n",
        "    metric = (val_accuracy + val_auc) / 2.0\n",
        "\n",
        "    if metric > self.bestMetric:\n",
        "      self.bestMetric = metric\n",
        "      self.bestLogs = logs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOHNseSRi-0L",
        "colab_type": "text"
      },
      "source": [
        "The cells below set the model and hyperparameters to search through. Only run the ones that set the options you want.\n",
        "\n",
        "The all_hparam_possibilities in the code below are the best ones found so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfVZf7rX9MZe",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Image model.\n",
        "# @markdown Run this cell iff use_activations=False\n",
        "# @markdown If you're trying to improve the accuracy of the model trained on activations, you won't care about this cell.\n",
        "\n",
        "def get_model(reg_amount, drop_rate, reduction_alg, n_components):\n",
        "  del reduction_alg, n_components\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # This layer gets one of the color channels. It works better than using all of them.\n",
        "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,tf.random.uniform((), 0,4,tf.int32)], 3), input_shape=xs.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(32, 2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.Conv2D(16, 1, activation='relu', strides=1, kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(drop_rate),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy',\n",
        "                        tf.keras.metrics.AUC()\n",
        "                        ],\n",
        "                )\n",
        "  return model\n",
        "all_hparam_possibilities = [{\"reg_amount\": [0.0], \"drop_rate\": [0.0], 'reduction_alg': [None], 'n_components': [None]}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Xcc_Zx_Any",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Model for training on the network activations\n",
        "# @markdown Run this cell iff use_activations=True\n",
        "\n",
        "def get_model(reg_amount, drop_rate, layer_sizes, reduction_alg, n_components):\n",
        "  del reduction_alg, n_components\n",
        "\n",
        "  layers = []\n",
        "  for layer_size in layer_sizes:\n",
        "    layers.append(tf.keras.layers.Dense(layer_size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)))\n",
        "    layers.append(tf.keras.layers.Dropout(drop_rate))\n",
        "\n",
        "  model = tf.keras.models.Sequential(layers + [\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount))\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', tf.keras.metrics.AUC()],\n",
        "                )\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_8xEUbPd1l",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Run this cell for hparams without unsupervised feature extraction.\n",
        "# @markdown Run this cell iff use_activations=True and you don't want unspervised feature exraction.\n",
        "all_hparam_possibilities = [{\"drop_rate\": [.2,], \"layer_sizes\": [(32,)], \"reg_amount\": [.2], 'reduction_alg': [None], 'n_components': [None]},\n",
        "                            {\"drop_rate\": [.5,], \"layer_sizes\": [(32,)], \"reg_amount\": [.5], 'reduction_alg': [None], 'n_components': [None]}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igu9yl2BPoBP",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# @title Run this cell for hparams with unsupervised feature extraction.\n",
        "# @markdown Run this cell iff use_activations=True and you want unspervised feature exraction.\n",
        "all_hparam_possibilities = [\n",
        "  {'drop_rate': [0], 'reduction_alg': ['PCA'], 'layer_sizes': [()], 'reg_amount': [0.2], 'n_components': [2]},\n",
        "  {'drop_rate': [0], 'reduction_alg': ['FastICA'], 'layer_sizes': [(16, 16)], 'reg_amount': [0], 'n_components': [8]},\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJW0yanonbZ",
        "colab_type": "text"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-2EQMqS_ayd",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "a8a283cc-ab16-43f7-dad7-e81e3e491e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# @title Train the model\n",
        "# @markdown This tries all the combinations of hparams and picks the best one.\n",
        "# @markdown For each combination of hparams, it averages over 5 different train val splits.\n",
        "# @markdown It re runs the best hyperparameters at the end.\n",
        "\n",
        "num_train = 50\n",
        "num_val = 1000\n",
        "epochs = 400\n",
        "print(\"use_activations:\", use_activations, \"num_train:\", num_train, \"epochs\", epochs)\n",
        "if num_train > 50:\n",
        "  print(\"More than 50 train data!!!!!!!!\")\n",
        "\n",
        "# each item in all_hparam_possibilities specifies valid hyper params to try. Put parameters that don't make sense together in separate lists.\n",
        "\n",
        "hparam_combinations = []\n",
        "for hparam_possibilities in all_hparam_possibilities:\n",
        "  hparam_keys, hparam_values = zip(*hparam_possibilities.items())\n",
        "  hparam_combinations.extend([dict(zip(hparam_keys, v)) for v in itertools.product(*hparam_values)])\n",
        "random.shuffle(hparam_combinations)\n",
        "print(\"len(hparam_combinations)\", len(hparam_combinations), \"hparam_combinations\", hparam_combinations)\n",
        "\n",
        "def modify_x_for_reduce(xs):\n",
        "  reshaped_x = np.reshape(xs, [xs.shape[0], -1])\n",
        "  # Make everything positive because some reductions don't work with negatives.\n",
        "  reshaped_x -= np.min(reshaped_x)\n",
        "  return reshaped_x\n",
        "\n",
        "def unsup_exstract(xs, reg_amount, drop_rate, layer_sizes, reduction_alg, n_components):\n",
        "  del reg_amount, drop_rate, layer_sizes\n",
        "\n",
        "  print(\"Using unsupervised feature extraction.\")\n",
        "\n",
        "  dim_reduct_model = ChannelReducer(reduction_alg=reduction_alg, n_components=n_components)\n",
        "  xs = dim_reduct_model.fit_transform(modify_x_for_reduce(xs))\n",
        "  return xs\n",
        "\n",
        "def train_best_logs(xs, ys, num_val, do_summary, hparams, get_model):\n",
        "  \"\"\"Trains the model and retruns the logs of the best epoch. randomly splits the train and val data before training.\"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  model = get_model(**hparams)\n",
        "  xs, ys = shuffle(xs, ys)\n",
        "\n",
        "  xs_val = xs[num_train:num_train+num_val]\n",
        "  ys_val = ys[num_train:num_train+num_val]\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=0)\n",
        "  best_stats = BestStats()\n",
        "  model.fit(xs[:num_train], ys[:num_train], epochs=epochs, batch_size=256, validation_freq=1, callbacks=[best_stats, early_stopping], validation_data=(xs_val, ys_val), verbose=0)\n",
        "  if do_summary:\n",
        "    model.summary()\n",
        "    print(\"best train accuracy:\", best_stats.bestTrain)\n",
        "    print(\"Number of epochs:\", best_stats.num_epochs)\n",
        "  return best_stats.bestLogs\n",
        "\n",
        "def multiple_train_ave(hparams):\n",
        "  \"\"\"Trains the model multiple times with the same parameters and returns the average metrics\"\"\"\n",
        "  start = time.time()\n",
        "  all_val_auc = []\n",
        "  all_val_accuracy = []\n",
        "\n",
        "  if hparams['reduction_alg'] != None:\n",
        "    xs_for_train = unsup_exstract(xs, **hparams)\n",
        "  else:\n",
        "    xs_for_train = xs\n",
        "\n",
        "  do_summary = True\n",
        "  for i in range(5):\n",
        "    logs = train_best_logs(xs_for_train, ys, num_val, do_summary, hparams, get_model)\n",
        "    all_val_auc.append(get_val_auc(logs))\n",
        "    all_val_accuracy.append(logs.get('val_accuracy'))\n",
        "    do_summary = False \n",
        "\n",
        "  mean_val_auc = np.mean(all_val_auc)\n",
        "  mean_val_accuracy = np.mean(all_val_accuracy)\n",
        "  metric = (mean_val_auc + mean_val_accuracy) / 2.0\n",
        "  print_data = (\"mean_val_auc\", mean_val_auc, \"mean_val_accuracy\", mean_val_accuracy, \"metric\", metric, \"val_auc_std\", np.std(all_val_auc), \"val_accuracy_std\", np.std(all_val_accuracy))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Seconds per hyperparam config\", end - start)\n",
        "  # GPU: ('Seconds per hyperparam config', 16.970870971679688)\n",
        "\n",
        "  return metric, print_data\n",
        "\n",
        "best_metric = -float('inf')\n",
        "\n",
        "run_num = 0\n",
        "for hparams in hparam_combinations:\n",
        "  print(\"hparams\", hparams)\n",
        "\n",
        "  metric, print_data = multiple_train_ave(hparams)\n",
        "\n",
        "  print(print_data)\n",
        "  if metric > best_metric:\n",
        "    best_metric = metric\n",
        "    best_print_data = print_data\n",
        "    best_hparams = hparams\n",
        "\n",
        "  run_num += 1\n",
        "  print(\"fract done\", run_num/float(len(hparam_combinations)))\n",
        "  print\n",
        "  print(\"==============================================================================================\")\n",
        "  print\n",
        "  sys.stdout.flush()\n",
        "\n",
        "print(\"best_hparams\", best_hparams)\n",
        "print(\"best results\", best_print_data)\n",
        "print(\"Retraining on the best_hparams to make sure we didn't just get good results by random chance.\")\n",
        "\n",
        "_, print_data = multiple_train_ave(best_hparams)\n",
        "print(\"Result of retrain on the best hyperparameters\", print_data)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use_activations: True num_train: 50 epochs 400\n",
            "len(hparam_combinations) 2 hparam_combinations [{'drop_rate': 0, 'reduction_alg': 'FastICA', 'layer_sizes': (16, 16), 'reg_amount': 0, 'n_components': 8}, {'drop_rate': 0, 'reduction_alg': 'PCA', 'layer_sizes': (), 'reg_amount': 0.2, 'n_components': 2}]\n",
            "hparams {'drop_rate': 0, 'reduction_alg': 'FastICA', 'layer_sizes': (16, 16), 'reg_amount': 0, 'n_components': 8}\n",
            "Using unsupervised feature extraction.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  144       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  272       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  17        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.92\n",
            "Number of epochs: 101\n",
            "Seconds per hyperparam config 14.261501550674438\n",
            "('mean_val_auc', 0.8392193, 'mean_val_accuracy', 0.788, 'metric', 0.8136096000671387, 'val_auc_std', 0.064938806, 'val_accuracy_std', 0.049967993)\n",
            "fract done 0.5\n",
            "==============================================================================================\n",
            "hparams {'drop_rate': 0, 'reduction_alg': 'PCA', 'layer_sizes': (), 'reg_amount': 0.2, 'n_components': 2}\n",
            "Using unsupervised feature extraction.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.82\n",
            "Number of epochs: 171\n",
            "Seconds per hyperparam config 15.314512491226196\n",
            "('mean_val_auc', 0.82666767, 'mean_val_accuracy', 0.7514, 'metric', 0.789033830165863, 'val_auc_std', 0.02461598, 'val_accuracy_std', 0.020460699)\n",
            "fract done 1.0\n",
            "==============================================================================================\n",
            "best_hparams {'drop_rate': 0, 'reduction_alg': 'FastICA', 'layer_sizes': (16, 16), 'reg_amount': 0, 'n_components': 8}\n",
            "best results ('mean_val_auc', 0.8392193, 'mean_val_accuracy', 0.788, 'metric', 0.8136096000671387, 'val_auc_std', 0.064938806, 'val_accuracy_std', 0.049967993)\n",
            "Retraining on the best_hparams to make sure we didn't just get good results by random chance.\n",
            "Using unsupervised feature extraction.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  144       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  272       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  17        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.86\n",
            "Number of epochs: 166\n",
            "Seconds per hyperparam config 14.693581104278564\n",
            "Result of retrain on the best hyperparameters ('mean_val_auc', 0.82565737, 'mean_val_accuracy', 0.76420003, 'metric', 0.7949286699295044, 'val_auc_std', 0.03488778, 'val_accuracy_std', 0.034884956)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}