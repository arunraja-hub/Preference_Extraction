{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "extract_preferences.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-2-2-gpu.2-2.m49",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKmidrQSH8bZ"
      },
      "source": [
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9uu_Ht_oXGx"
      },
      "source": [
        "## Data and import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nC3ZAJFA6Jz",
        "outputId": "bea535da-5482-47c7-c360-999721d7e6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.2.1\n",
        "!git clone https://github.com/arunraja-hub/Preference_Extraction.git\n",
        "\n",
        "!pip install tensorflow==2.2.1\n",
        "!pip install tf-agents==0.5.0\n",
        "!pip install tensorflow-probability==0.10"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.2.1`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "fatal: destination path 'Preference_Extraction' already exists and is not an empty directory.\n",
            "Collecting tf-agents==0.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/ef/b0/88c9aab39050cfb544ec73ee48b8d0e67b4b16ed5470c82235255b119952/tf_agents-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents==0.5.0) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents==0.5.0) (0.10.0)\n",
            "Collecting tensorflow-probability>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/99/07d4220b021a5d6f6b3de8872f2a634389ad224f8508258452e57aaaf328/tensorflow_probability-0.11.1-py2.py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config==0.1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents==0.5.0) (0.1.3)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents==0.5.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents==0.5.0) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents==0.5.0) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents==0.5.0) (0.1.5)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.9.0->tf-agents==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents==0.5.0) (50.3.0)\n",
            "Installing collected packages: tensorflow-probability, tf-agents\n",
            "  Found existing installation: tensorflow-probability 0.7.0\n",
            "    Uninstalling tensorflow-probability-0.7.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.7.0\n",
            "  Found existing installation: tf-agents 0.3.0\n",
            "    Uninstalling tf-agents-0.3.0:\n",
            "      Successfully uninstalled tf-agents-0.3.0\n",
            "Successfully installed tensorflow-probability-0.11.1 tf-agents-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_probability",
                  "tf_agents"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/c5/783644c55074f42070acfa1662145f4a0c59ff425495194aa2dc4052f22a/tensorflow_probability-0.10.0-py2.py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.10) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.10) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.10) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.10) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.10) (4.4.2)\n",
            "Installing collected packages: tensorflow-probability\n",
            "  Found existing installation: tensorflow-probability 0.11.1\n",
            "    Uninstalling tensorflow-probability-0.11.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.1\n",
            "Successfully installed tensorflow-probability-0.10.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_probability"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/e3/663eac537202dee730ad6e61769fc3ebce92a6085dbfd13ca902df5f1477/tensorflow-2.2.1-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (0.2.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.18.5)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.1) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.1) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (2020.6.20)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (3.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.1) (0.4.8)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.1 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGhMeaXwica2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import concurrent.futures\n",
        "import itertools\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import io\n",
        "import collections\n",
        "\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "from tf_agents.trajectories.time_step import TimeStep\n",
        "from tf_agents.specs.tensor_spec import TensorSpec\n",
        "from tf_agents.specs.tensor_spec import TensorSpec\n",
        "from tf_agents.specs.tensor_spec import BoundedTensorSpec\n",
        "from tf_agents.networks import q_network\n",
        "\n",
        "sys.path.append('Preference_Extraction')\n",
        "from imports_data import all_load_data\n",
        "import joblib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHm3f_W6BX0X"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYbfWqE8BWyz"
      },
      "source": [
        "use_agent = True\n",
        "env = 'grid' # valid options 'doom', 'grid'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp0xu_3RHDJK"
      },
      "source": [
        "if env == 'doom':\n",
        "    all_raw_data = joblib.load('/home/jupyter/train_data/agentV2.9/AgentExperienceData.pkl')\n",
        "else:\n",
        "    all_raw_data = all_load_data(\"/content/Preference_Extraction/data/simple_env_1/\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "HLw7aBy67gnH",
        "outputId": "8ff96930-1259-418f-e609-a96cf9dac1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# @title Preprocess the data into x,y training pairs\n",
        "# @markdown The use_agent control which model is trained.\n",
        "# @markdown All of the cells below use the data specified here.\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "for data in all_raw_data:\n",
        "    for i in range(data.observation.shape[0]):\n",
        "        x = np.copy(data.observation[i])\n",
        "        \n",
        "        if env == 'doom':\n",
        "            # Doom label object is a dictionary with object_angle and distance_from_wall\n",
        "            label_object = data.policy_info['satisfaction'][i]\n",
        "            if len(label_object) == 0: # When label is empty, i.e. human is dead, skip frame\n",
        "                continue\n",
        "            else:\n",
        "                y = label_object['object_angle'] < 90 or label_object['object_angle'] > 270\n",
        "        else:\n",
        "            y = data.policy_info['satisfaction'].as_list()[i] > -6\n",
        "\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "xs = np.array(xs)\n",
        "ys = np.array(ys).astype(int)\n",
        "xs, ys = shuffle(xs, ys)\n",
        "print(\"xs\", xs.shape, \"ys\", ys.shape)\n",
        "print(\"ys 1\", np.sum(ys))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xs (23750, 14, 16, 5) ys (23750,)\n",
            "ys 1 9569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMB6Z24hHDJa",
        "outputId": "45697bf1-97dd-4225-f5d5-da41bb31bb6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Rebalancing data to minority class\n",
        "points = xs\n",
        "labels = ys\n",
        "\n",
        "# indexes of 1s and 0s\n",
        "indexes1 = [i for i in range(len(points)) if labels[i] == 1]\n",
        "indexes0 = [i for i in range(len(points)) if labels[i] == 0]\n",
        "\n",
        "# separate 0s and 1s\n",
        "x0, x1, y0, y1 = points[indexes0], points[indexes1], labels[indexes0], labels[indexes1]\n",
        "\n",
        "minority_points, minority_labels = x1, y1  # points and labels for the minority class\n",
        "majority_points, majority_labels = x0, y0  # points and labels for the majority class\n",
        "\n",
        "# get a random permutation of indexes of the majority that includes a number of indexes equal to the minority\n",
        "sample_ind = np.random.permutation(len(majority_labels))[:len(minority_labels)]\n",
        "\n",
        "# subsample the majority\n",
        "majority_points, majority_labels = majority_points[sample_ind], majority_labels[sample_ind]\n",
        "\n",
        "# concat the minority and the sub-sampled majority\n",
        "xs = np.concatenate((majority_points, minority_points))\n",
        "ys = np.concatenate((majority_labels, minority_labels))\n",
        "\n",
        "print(\"xs\", xs.shape, \"ys\", ys.shape)\n",
        "print(\"ys 1\", np.sum(ys))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xs (19138, 14, 16, 5) ys (19138,)\n",
            "ys 1 9569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "jpNLoX8t8vC7",
        "outputId": "2cd3eef9-be22-4603-97f8-e33da96107d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# @title Visualize and example\n",
        "rand_index = random.randint(0,1000)\n",
        "plt.imshow(xs[rand_index,:,:,:3])\n",
        "plt.show()\n",
        "print(\"x\", xs[rand_index])\n",
        "print(\"y\", ys[rand_index])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "x [[[0.02903926 0.01320187 0.01321343 0.12704638 0.        ]\n",
            "  [0.11983195 0.54157025 0.4002355  0.12704638 0.00446429]\n",
            "  [0.02903926 0.01320187 0.01321343 0.12704638 0.00892857]\n",
            "  ...\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.05803571]\n",
            "  [0.02903926 0.01320187 0.01321343 0.9181332  0.0625    ]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.06696428]]\n",
            "\n",
            " [[0.11983195 0.54157025 0.4002355  0.12704638 0.07142857]\n",
            "  [0.02903926 0.01320187 0.01321343 0.12704638 0.07589286]\n",
            "  [0.11983195 0.54157025 0.4002355  0.12704638 0.08035714]\n",
            "  ...\n",
            "  [0.02903926 0.01320187 0.01321343 0.9181332  0.12946428]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.13392857]\n",
            "  [0.02903926 0.01320187 0.01321343 0.9181332  0.13839285]]\n",
            "\n",
            " [[0.02903926 0.01320187 0.01321343 0.12704638 0.14285715]\n",
            "  [0.11983195 0.54157025 0.4002355  0.12704638 0.14732143]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.15178572]\n",
            "  ...\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.20089285]\n",
            "  [0.02903926 0.01320187 0.01321343 0.9181332  0.20535715]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.20982143]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.11983195 0.54157025 0.4002355  0.12704638 0.78571427]\n",
            "  [0.02903926 0.01320187 0.01321343 0.12704638 0.7901786 ]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.79464287]\n",
            "  ...\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.84375   ]\n",
            "  [0.11983195 0.54157025 0.4002355  0.9181332  0.84821427]\n",
            "  [0.02903926 0.01320187 0.01321343 0.9181332  0.8526786 ]]\n",
            "\n",
            " [[0.         0.         0.         0.12704638 0.85714287]\n",
            "  [0.         0.         0.         0.12704638 0.86160713]\n",
            "  [0.         0.         0.         0.9181332  0.8660714 ]\n",
            "  ...\n",
            "  [0.69803923 0.09803922 0.09803922 0.9181332  0.9151786 ]\n",
            "  [0.         0.         0.         0.9181332  0.91964287]\n",
            "  [0.         0.         0.         0.9181332  0.92410713]]\n",
            "\n",
            " [[0.         0.         0.         0.12704638 0.9285714 ]\n",
            "  [0.         0.         0.         0.12704638 0.93303573]\n",
            "  [0.         0.         0.         0.9181332  0.9375    ]\n",
            "  ...\n",
            "  [0.69803923 0.09803922 0.09803922 0.9181332  0.98660713]\n",
            "  [0.         0.         0.         0.9181332  0.9910714 ]\n",
            "  [0.         0.         0.         0.9181332  0.99553573]]]\n",
            "y 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPKA64IvojNR"
      },
      "source": [
        "## Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "aSIP8sjjGdiC",
        "outputId": "fcf20d77-44d6-4b88-a473-a7e95311f38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# @title ChannelReducer from lucid\n",
        "# Copied from https://github.com/tensorflow/lucid/blob/master/lucid/misc/channel_reducer.py\n",
        "\n",
        "# Copyright 2018 The Lucid Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Helper for using sklearn.decomposition on high-dimensional tensors.\n",
        "\n",
        "Provides ChannelReducer, a wrapper around sklearn.decomposition to help them\n",
        "apply to arbitrary rank tensors. It saves lots of annoying reshaping.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "\n",
        "try:\n",
        "    from sklearn.decomposition.base import BaseEstimator\n",
        "except AttributeError:\n",
        "    from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class ChannelReducer(object):\n",
        "  \"\"\"Helper for dimensionality reduction to the innermost dimension of a tensor.\n",
        "\n",
        "  This class wraps sklearn.decomposition classes to help them apply to arbitrary\n",
        "  rank tensors. It saves lots of annoying reshaping.\n",
        "\n",
        "  See the original sklearn.decomposition documentation:\n",
        "  http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_components=3, reduction_alg=\"NMF\", **kwargs):\n",
        "    \"\"\"Constructor for ChannelReducer.\n",
        "\n",
        "    Inputs:\n",
        "      n_components: Numer of dimensions to reduce inner most dimension to.\n",
        "      reduction_alg: A string or sklearn.decomposition class. Defaults to\n",
        "        \"NMF\" (non-negative matrix facotrization). Other options include:\n",
        "        \"PCA\", \"FastICA\", and \"MiniBatchDictionaryLearning\". The name of any of\n",
        "        the sklearn.decomposition classes will work, though.\n",
        "      kwargs: Additional kwargs to be passed on to the reducer.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(n_components, int):\n",
        "      raise ValueError(\"n_components must be an int, not '%s'.\" % n_components)\n",
        "\n",
        "    # Defensively look up reduction_alg if it is a string and give useful errors.\n",
        "    algorithm_map = {}\n",
        "    for name in dir(sklearn.decomposition):\n",
        "      obj = sklearn.decomposition.__getattribute__(name)\n",
        "      if isinstance(obj, type) and issubclass(obj, BaseEstimator):\n",
        "        algorithm_map[name] = obj\n",
        "    if isinstance(reduction_alg, str):\n",
        "      if reduction_alg in algorithm_map:\n",
        "        reduction_alg = algorithm_map[reduction_alg]\n",
        "      else:\n",
        "        raise ValueError(\"Unknown dimensionality reduction method '%s'.\" % reduction_alg)\n",
        "\n",
        "\n",
        "    self.n_components = n_components\n",
        "    self._reducer = reduction_alg(n_components=n_components, **kwargs)\n",
        "    self._is_fit = False\n",
        "\n",
        "  @classmethod\n",
        "  def _apply_flat(cls, f, acts):\n",
        "    \"\"\"Utility for applying f to inner dimension of acts.\n",
        "\n",
        "    Flattens acts into a 2D tensor, applies f, then unflattens so that all\n",
        "    dimesnions except innermost are unchanged.\n",
        "    \"\"\"\n",
        "    orig_shape = acts.shape\n",
        "    acts_flat = acts.reshape([-1, acts.shape[-1]])\n",
        "    new_flat = f(acts_flat)\n",
        "    if not isinstance(new_flat, np.ndarray):\n",
        "      return new_flat\n",
        "    shape = list(orig_shape[:-1]) + [-1]\n",
        "    return new_flat.reshape(shape)\n",
        "\n",
        "  def fit(self, acts):\n",
        "    self._is_fit = True\n",
        "    return ChannelReducer._apply_flat(self._reducer.fit, acts)\n",
        "\n",
        "  def fit_transform(self, acts):\n",
        "    self._is_fit = True\n",
        "    return ChannelReducer._apply_flat(self._reducer.fit_transform, acts)\n",
        "\n",
        "  def transform(self, acts):\n",
        "    return ChannelReducer._apply_flat(self._reducer.transform, acts)\n",
        "\n",
        "  def __call__(self, acts):\n",
        "    if self._is_fit:\n",
        "      return self.transform(acts)\n",
        "    else:\n",
        "      return self.fit_transform(acts)\n",
        "\n",
        "  def __getattr__(self, name):\n",
        "    if name in self.__dict__:\n",
        "      return self.__dict__[name]\n",
        "    elif name + \"_\" in self._reducer.__dict__:\n",
        "      return self._reducer.__dict__[name+\"_\"]\n",
        "\n",
        "  def __dir__(self):\n",
        "    dynamic_attrs = [name[:-1]\n",
        "                     for name in dir(self._reducer)\n",
        "                     if name[-1] == \"_\" and name[0] != \"_\"\n",
        "                    ]\n",
        "\n",
        "    return list(ChannelReducer.__dict__.keys()) + list(self.__dict__.keys()) + dynamic_attrs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.decomposition.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_yWm1n9Fp1"
      },
      "source": [
        "def get_val_auc(logs):\n",
        "      for key in logs:\n",
        "        if key.startswith('val_auc'):\n",
        "          return logs[key]\n",
        "\n",
        "class BestStats(tf.keras.callbacks.Callback):\n",
        "  \"\"\"A callback to keep track of the best val accuracy and auc seen so far.\"\"\"\n",
        "  def on_train_begin(self, logs):\n",
        "      self.bestMetric = -float('inf')\n",
        "      self.bestLogs = None\n",
        "      self.bestTrain = -float('inf')\n",
        "      self.num_epochs = 0\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    self.num_epochs += 1\n",
        "    self.bestTrain = max(self.bestTrain, logs.get('accuracy'))\n",
        "\n",
        "    val_accuracy = logs.get('val_accuracy')\n",
        "    if val_accuracy == None:\n",
        "      return \n",
        "\n",
        "    val_auc = get_val_auc(logs)\n",
        "    \n",
        "    metric = (val_accuracy + val_auc) / 2.0\n",
        "\n",
        "    if metric > self.bestMetric:\n",
        "      self.bestMetric = metric\n",
        "      self.bestLogs = logs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tFrFen8l7Q4"
      },
      "source": [
        "### CNN from observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "bfVZf7rX9MZe"
      },
      "source": [
        "# @title Image model.\n",
        "# @markdown Run this cell iff use_agent=False\n",
        "# @markdown If you're trying to improve the accuracy of the model trained on activations, you won't care about this cell.\n",
        "\n",
        "def get_model(reg_amount, drop_rate, reduction_alg, n_components):\n",
        "  del reduction_alg, n_components\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # This layer gets one of the color channels. It works better than using all of them.\n",
        "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,tf.random.uniform((), 0,4, tf.int32)], 3), input_shape=xs.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(64, 2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.Conv2D(32, 1, activation='relu', strides=1, kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(drop_rate),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy',\n",
        "                        tf.keras.metrics.AUC()\n",
        "                        ],\n",
        "                )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ9rHssEmQPf"
      },
      "source": [
        "# Hyperparameters for CNN from observations\n",
        "all_hparam_possibilities = [{\"reg_amount\": [0], \"drop_rate\": [0], 'reduction_alg': [None], 'n_components': [None]}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjrGt3aEB-BJ"
      },
      "source": [
        "### QNet Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqcO8ZrWB5_V"
      },
      "source": [
        "cpt_name = \"/content/Preference_Extraction/model_ckpt\"\n",
        "\n",
        "input_shape = [14, 16, 5]\n",
        "q_net = q_network.QNetwork(input_tensor_spec=TensorSpec(shape=input_shape), action_spec=BoundedTensorSpec((), tf.int32, 0, 2), conv_layer_params = [[16, 3, 1], [32, 3, 2]], fc_layer_params = [64])\n",
        "q_net.layers[0].layers[1]._name = \"EncodingNetwork/conv2d_1\"\n",
        "\n",
        "latest_cpt =  tf.train.latest_checkpoint(cpt_name)\n",
        "reader = tf.compat.v1.train.NewCheckpointReader(latest_cpt)\n",
        "model_input = tf.keras.Input(shape=input_shape)\n",
        "q_model_nested = tf.keras.models.Model(inputs=model_input, outputs=[q_net(model_input)])\n",
        "q_model_nested.build(input_shape=input_shape)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfrEjW3LCCm6",
        "outputId": "53f4f81a-11d3-460c-8e15-c5dd05459cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "def flatten_model(model_nested):\n",
        "    def get_layers(layers):\n",
        "        layers_flat = []\n",
        "        for layer in layers:\n",
        "            try:\n",
        "                layers_flat.extend(get_layers(layer.layers))\n",
        "            except AttributeError:\n",
        "                layers_flat.append(layer)\n",
        "        return layers_flat\n",
        "\n",
        "    model_flat = tf.keras.models.Sequential(\n",
        "        get_layers(model_nested.layers)\n",
        "    )\n",
        "    return model_flat\n",
        "\n",
        "def load_weigths(model, last_layer, random=False):\n",
        "\n",
        "    layer_map = {\n",
        "        model.layers[0]: \"agent/_q_network/_encoder/_postprocessing_layers/0\",\n",
        "        model.layers[1]: \"agent/_q_network/_encoder/_postprocessing_layers/1\",\n",
        "    }\n",
        "    if last_layer > 3:\n",
        "        layer_map[model.layers[3]] = \"agent/_q_network/_encoder/_postprocessing_layers/3\"\n",
        "    if last_layer > 4:\n",
        "        layer_map[model.layers[4]] = \"agent/_q_network/_q_value_layer\"\n",
        "\n",
        "    last_name_part = \"/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "    for keras_layer, weights_bias_name in layer_map.items():\n",
        "        weights = reader.get_tensor(weights_bias_name+\"/kernel\"+last_name_part)\n",
        "        biases = reader.get_tensor(weights_bias_name+\"/bias\"+last_name_part)\n",
        "        if random:\n",
        "            initializer = tf.keras.initializers.VarianceScaling(\n",
        "                scale=1.0, mode=\"fan_in\", distribution=\"truncated_normal\", seed=None)\n",
        "            weights = initializer(shape=weights.shape)\n",
        "            biases = initializer(shape=biases.shape)\n",
        "        keras_layer.set_weights([weights, biases])\n",
        "    \n",
        "    return model\n",
        "\n",
        "q_model = flatten_model(q_model_nested)\n",
        "q_model = load_weigths(q_model, last_layer=5, random=False)\n",
        "q_model.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 67,075\n",
            "Trainable params: 67,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjbFIA5KCKRx"
      },
      "source": [
        "# Verify agent it performs inference correctly\n",
        "def verify_model(model, output_index):\n",
        "  activation_model = tf.keras.models.Model(inputs=model.input, outputs=model.layers[output_index].output)\n",
        "  for i in range(len(all_raw_data[0].observation)):\n",
        "    single_observation = np.array([all_raw_data[0].observation[i]])\n",
        "\n",
        "    restored_activations = activation_model(single_observation)[0]\n",
        "    old_activations = all_raw_data[0].policy_info[\"activations\"][i]\n",
        "    np.testing.assert_allclose(restored_activations, old_activations, rtol=.1)\n",
        "\n",
        "verify_model(q_model, output_index=-2)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpr6wvvzHDKY"
      },
      "source": [
        "### PPO Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkNhdl5iHDKZ",
        "outputId": "22088db1-b13e-4701-9aad-45a475fc5471"
      },
      "source": [
        "PPO_AGENT_PATH = '/home/jupyter/train_data/agentV2.9/actorNet.keras'\n",
        "ppo_model = tf.keras.models.load_model(PPO_AGENT_PATH)\n",
        "ppo_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  7200200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  20100     \n",
            "_________________________________________________________________\n",
            "logits (Dense)               multiple                  404       \n",
            "=================================================================\n",
            "Total params: 7,220,704\n",
            "Trainable params: 7,220,704\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP_kL8W3CcPK"
      },
      "source": [
        "### Method to get activations DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "v30FiGi2Cf94"
      },
      "source": [
        "# @title Model for training on top of the agent\n",
        "# @markdown Run this cell iff use_agent=True\n",
        "\n",
        "def get_model(agent_model, agent_type, reg_amount, drop_rate, layer_sizes, net_last_cut, net_freeze, reduction_alg, n_components):\n",
        "  del reduction_alg, n_components\n",
        "\n",
        "  if agent_type == 'ppo':\n",
        "        # Re-load on every train to ensure weigths degeneration\n",
        "        agent_model = tf.keras.models.load_model(PPO_AGENT_PATH)\n",
        "\n",
        "  layers = []\n",
        "  for ix, layer_size in enumerate(layer_sizes):\n",
        "    layers.append(tf.keras.layers.Dense(layer_size, activation='relu', \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(reg_amount), name='post_agent_{}'.format(ix)))\n",
        "    layers.append(tf.keras.layers.Dropout(drop_rate))\n",
        "\n",
        "  for ix, _ in enumerate(agent_model.layers[:net_last_cut]):\n",
        "      if ix in net_freeze:\n",
        "          agent_model.layers[ix].trainable = False\n",
        "      else:\n",
        "          agent_model.layers[ix].trainable = True\n",
        "        \n",
        "  if agent_type == 'qnet':\n",
        "    agent_layers = [agent_model.input] + agent_model.layers[:net_last_cut] \n",
        "  elif agent_type == 'ppo':\n",
        "    agent_layers = agent_model.layers[:net_last_cut]\n",
        "\n",
        "  model = tf.keras.models.Sequential(agent_layers + layers + [\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount), name='output')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "  if agent_type == 'qnet':\n",
        "    # Set random=True for a randomly weigthed baseline\n",
        "    return load_weigths(model, last_layer=net_last_cut, random=False)\n",
        "  else:\n",
        "    return model"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw6YTEfclZYF"
      },
      "source": [
        "# @title Run this cell for hparams with unsupervised feature extraction.\n",
        "# @markdown Run this cell iff use_agent=True and you want unspervised feature exraction.\n",
        "\n",
        "all_hparam_possibilities = [\n",
        "  {'drop_rate': [0], 'reduction_alg': ['PCA'], 'layer_sizes': [()], 'reg_amount': [0.2], 'n_components': [2]},\n",
        "  {'drop_rate': [0], 'reduction_alg': ['FastICA'], 'layer_sizes': [(16, 16)], 'reg_amount': [0], 'n_components': [8]},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "bgCRDam2DZWB"
      },
      "source": [
        "# @title Run this cell for hparams without unsupervised feature extraction.\n",
        "# @markdown Run this cell iff use_agent=True and you don't want unspervised feature exraction.\n",
        "all_hparam_possibilities = [\n",
        "   {\n",
        "    \"drop_rate\": [.2],  \n",
        "    \"reg_amount\": [.2],  \n",
        "    \"layer_sizes\": [(32,)],\n",
        "    \"net_last_cut\": [4],\n",
        "    \"net_freeze\": [(None,)],\n",
        "    \"reduction_alg\": [None], \n",
        "    \"n_components\": [None]\n",
        "   }\n",
        "]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJW0yanonbZ"
      },
      "source": [
        "# Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wKrJqVEHDKv"
      },
      "source": [
        "agent_type = 'qnet'\n",
        "agent_model = q_model"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Rg76Q4ckGr"
      },
      "source": [
        "# Run this to train on 10k data instead.\n",
        "num_train = 50\n",
        "num_val = 1000\n",
        "epochs = 400\n",
        "num_repeat = 5"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG8eFVRagQ_J"
      },
      "source": [
        "# Run this to reproduce the original results.\n",
        "num_train = 500\n",
        "num_val = 1000\n",
        "epochs = 500\n",
        "num_repeat = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "d-2EQMqS_ayd",
        "outputId": "b0bf9b7e-1c5c-4924-d4be-7948dd745da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# @title Train the model\n",
        "# @markdown This tries all the combinations of hparams and picks the best one.\n",
        "# @markdown For each combination of hparams, it averages over num_repeat different train val splits.\n",
        "# @markdown It re runs the best hyperparameters at the end.\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "print(\"use_agent:\", use_agent, \"num_train:\", num_train, \"epochs\", epochs)\n",
        "if num_train > 50:\n",
        "  print(\"More than 50 train data!!!!!!!!\")\n",
        "\n",
        "# each item in all_hparam_possibilities specifies valid hyper params to try. Put parameters that don't make sense together in separate lists.\n",
        "\n",
        "hparam_combinations = []\n",
        "for hparam_possibilities in all_hparam_possibilities:\n",
        "  hparam_keys, hparam_values = zip(*hparam_possibilities.items())\n",
        "  hparam_combinations.extend([dict(zip(hparam_keys, v)) for v in itertools.product(*hparam_values)])\n",
        "random.shuffle(hparam_combinations)\n",
        "print(\"len(hparam_combinations)\", len(hparam_combinations), \"hparam_combinations\", hparam_combinations)\n",
        "\n",
        "def modify_x_for_reduce(xs):\n",
        "  reshaped_x = np.reshape(xs, [xs.shape[0], -1])\n",
        "  # Make everything positive because some reductions don't work with negatives.\n",
        "  reshaped_x -= np.min(reshaped_x)\n",
        "  return reshaped_x\n",
        "\n",
        "def unsup_exstract(xs, reg_amount, drop_rate, layer_sizes, reduction_alg, n_components):\n",
        "  del reg_amount, drop_rate, layer_sizes\n",
        "\n",
        "  print(\"Using unsupervised feature extraction.\")\n",
        "\n",
        "  dim_reduct_model = ChannelReducer(reduction_alg=reduction_alg, n_components=n_components)\n",
        "  xs = dim_reduct_model.fit_transform(modify_x_for_reduce(xs))\n",
        "  return xs\n",
        "\n",
        "def train_best_logs(xs, ys, num_val, do_summary, hparams, get_model):\n",
        "  \"\"\"Trains the model and retruns the logs of the best epoch. randomly splits the train and val data before training.\"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  if use_agent:\n",
        "    model = get_model(agent_model, agent_type, **hparams)\n",
        "  else:\n",
        "    model = get_model(**hparams)\n",
        "  xs, ys = shuffle(xs, ys)\n",
        "\n",
        "  xs_val = xs[num_train:num_train+num_val]\n",
        "  ys_val = ys[num_train:num_train+num_val]\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=0)\n",
        "  best_stats = BestStats()\n",
        "  model.fit(xs[:num_train], ys[:num_train], epochs=epochs, batch_size=128, validation_freq=10, \n",
        "            callbacks=[best_stats, early_stopping], validation_data=(xs_val, ys_val), verbose=0)\n",
        "    \n",
        "  if do_summary:\n",
        "    model.summary()\n",
        "    print(\"best train accuracy:\", best_stats.bestTrain)\n",
        "    print(\"Number of epochs:\", best_stats.num_epochs)\n",
        "  return best_stats.bestLogs\n",
        "\n",
        "def multiple_train_ave(hparams):\n",
        "  \"\"\"Trains the model multiple times with the same parameters and returns the average metrics\"\"\"\n",
        "  start = time.time()\n",
        "  all_val_auc = []\n",
        "  all_val_accuracy = []\n",
        "\n",
        "  if hparams['reduction_alg'] != None:\n",
        "    xs_for_train = unsup_exstract(xs, **hparams)\n",
        "  else:\n",
        "    xs_for_train = xs\n",
        "\n",
        "  do_summary = True\n",
        "  for i in range(num_repeat):\n",
        "    logs = train_best_logs(xs_for_train, ys, num_val, do_summary, hparams, get_model)\n",
        "    all_val_auc.append(get_val_auc(logs))\n",
        "    all_val_accuracy.append(logs.get('val_accuracy'))\n",
        "    do_summary = False \n",
        "\n",
        "  mean_val_auc = np.mean(all_val_auc)\n",
        "  mean_val_accuracy = np.mean(all_val_accuracy)\n",
        "  metric = (mean_val_auc + mean_val_accuracy) / 2.0\n",
        "  print_data = (\"mean_val_auc\", mean_val_auc, \"mean_val_accuracy\", mean_val_accuracy, \n",
        "                \"metric\", metric, \"val_auc_std\", np.std(all_val_auc), \"val_accuracy_std\", np.std(all_val_accuracy))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Seconds per hyperparam config\", end - start)\n",
        "  # GPU: ('Seconds per hyperparam config', 16.970870971679688)\n",
        "\n",
        "  return metric, print_data\n",
        "\n",
        "best_metric = -float('inf')\n",
        "\n",
        "run_num = 0\n",
        "for hparams in hparam_combinations:\n",
        "  print(\"hparams\", hparams)\n",
        "\n",
        "  metric, print_data = multiple_train_ave(hparams)\n",
        "\n",
        "  print(print_data)\n",
        "  if metric > best_metric:\n",
        "    best_metric = metric\n",
        "    best_print_data = print_data\n",
        "    best_hparams = hparams\n",
        "\n",
        "  run_num += 1\n",
        "  print(\"fract done\", run_num/float(len(hparam_combinations)))\n",
        "  print\n",
        "  print(\"==============================================================================================\")\n",
        "  print\n",
        "  sys.stdout.flush()\n",
        "\n",
        "print(\"best_hparams\", best_hparams)\n",
        "print(\"best results\", best_print_data)\n",
        "print(\"Retraining on the best_hparams to make sure we didn't just get good results by random chance.\")\n",
        "\n",
        "_, print_data = multiple_train_ave(best_hparams)\n",
        "print(\"Result of retrain on the best hyperparameters\", print_data)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use_agent: True num_train: 50 epochs 400\n",
            "len(hparam_combinations) 1 hparam_combinations [{'drop_rate': 0.2, 'reg_amount': 0.2, 'layer_sizes': (32,), 'net_last_cut': 4, 'net_freeze': (None,), 'reduction_alg': None, 'n_components': None}]\n",
            "hparams {'drop_rate': 0.2, 'reg_amount': 0.2, 'layer_sizes': (32,), 'net_last_cut': 4, 'net_freeze': (None,), 'reduction_alg': None, 'n_components': None}\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "post_agent_0 (Dense)         (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 68,993\n",
            "Trainable params: 68,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 1.0\n",
            "Number of epochs: 400\n",
            "Seconds per hyperparam config 60.680795192718506\n",
            "('mean_val_auc', 0.5267997324466706, 'mean_val_accuracy', 0.5843999922275543, 'metric', 0.5555998623371124, 'val_auc_std', 0.028395185945563343, 'val_accuracy_std', 0.027990001486605392)\n",
            "fract done 1.0\n",
            "==============================================================================================\n",
            "best_hparams {'drop_rate': 0.2, 'reg_amount': 0.2, 'layer_sizes': (32,), 'net_last_cut': 4, 'net_freeze': (None,), 'reduction_alg': None, 'n_components': None}\n",
            "best results ('mean_val_auc', 0.5267997324466706, 'mean_val_accuracy', 0.5843999922275543, 'metric', 0.5555998623371124, 'val_auc_std', 0.028395185945563343, 'val_accuracy_std', 0.027990001486605392)\n",
            "Retraining on the best_hparams to make sure we didn't just get good results by random chance.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 12, 14, 16)        736       \n",
            "_________________________________________________________________\n",
            "EncodingNetwork/conv2d_1 (Co (None, 5, 6, 32)          4640      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 960)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                61504     \n",
            "_________________________________________________________________\n",
            "post_agent_0 (Dense)         (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 68,993\n",
            "Trainable params: 68,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 1.0\n",
            "Number of epochs: 400\n",
            "Seconds per hyperparam config 59.964754581451416\n",
            "Result of retrain on the best hyperparameters ('mean_val_auc', 0.5207420885562897, 'mean_val_accuracy', 0.590200001001358, 'metric', 0.5554710447788238, 'val_auc_std', 0.02488421978323561, 'val_accuracy_std', 0.02154901721289344)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}