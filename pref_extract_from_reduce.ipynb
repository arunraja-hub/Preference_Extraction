{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pref_extract_from_reduce.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtJ8EBoodDY8LAoSDptYWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunraja-hub/Preference_Extraction/blob/rl_util/pref_extract_from_reduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30Y6Tjy9j89",
        "colab_type": "text"
      },
      "source": [
        "Uses the reduction from the rl util to infer human preferences. **Run the use_rl_util.ipynb notebook before this one** to get the reduction saved in your drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYEIvaw755Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import joblib\n",
        "import os\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9FqWdiv6drn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0a0a23eb-076e-4edc-cc25-b0d15cbc4d46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8Xdv5Sv6rXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive/pref_extract_files/'\n",
        "ys = joblib.load(os.path.join(file_path, 'ys.pkl'))\n",
        "observations = joblib.load(os.path.join(file_path, 'observations.pkl'))\n",
        "xs = joblib.load(os.path.join(file_path, 'reduced.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUuEUk_G65pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_val_auc(logs):\n",
        "      for key in logs:\n",
        "        if key.startswith('val_auc'):\n",
        "          return logs[key]\n",
        "\n",
        "class BestStats(tf.keras.callbacks.Callback):\n",
        "  \"\"\"A callback to keep track of the best val accuracy and auc seen so far.\"\"\"\n",
        "  def on_train_begin(self, logs):\n",
        "      self.bestMetric = -float('inf')\n",
        "      self.bestLogs = None\n",
        "      self.bestTrain = -float('inf')\n",
        "      self.num_epochs = 0\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    self.num_epochs += 1\n",
        "    self.bestTrain = max(self.bestTrain, logs.get('accuracy'))\n",
        "\n",
        "    val_accuracy = logs.get('val_accuracy')\n",
        "    if val_accuracy == None:\n",
        "      return \n",
        "\n",
        "    val_auc = get_val_auc(logs)\n",
        "    \n",
        "    metric = (val_accuracy + val_auc) / 2.0\n",
        "\n",
        "    if metric > self.bestMetric:\n",
        "      self.bestMetric = metric\n",
        "      self.bestLogs = logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cuwyUg7Sss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(reg_amount, drop_rate, reduction_alg, n_components):\n",
        "  del reduction_alg, n_components\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_amount), input_shape=xs.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(16, 3, activation='relu', strides=1, kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(reg_amount)),\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(.01),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy',\n",
        "                        tf.keras.metrics.AUC()\n",
        "                        ],\n",
        "                )\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWKlUG7u7dcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_activations = False\n",
        "\n",
        "all_hparam_possibilities = [\n",
        "    {\"drop_rate\": [.0,], \"reg_amount\": [.0], 'reduction_alg': [None], 'n_components': [None]},\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bCgQoS2B1nD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76cc9ee7-4d6f-45d2-8041-4b20e8021c0d"
      },
      "source": [
        "xs.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 12, 14, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpJ1xtu-7V9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "616b3253-b7b2-47fb-fb9c-f68e96da687e"
      },
      "source": [
        "num_train = 50\n",
        "num_val = 1000\n",
        "epochs = 400\n",
        "print(\"use_activations:\", use_activations, \"num_train:\", num_train, \"epochs\", epochs)\n",
        "if num_train > 50:\n",
        "  print(\"More than 50 train data!!!!!!!!\")\n",
        "\n",
        "# each item in all_hparam_possibilities specifies valid hyper params to try. Put parameters that don't make sense together in separate lists.\n",
        "\n",
        "hparam_combinations = []\n",
        "for hparam_possibilities in all_hparam_possibilities:\n",
        "  hparam_keys, hparam_values = zip(*hparam_possibilities.items())\n",
        "  hparam_combinations.extend([dict(zip(hparam_keys, v)) for v in itertools.product(*hparam_values)])\n",
        "random.shuffle(hparam_combinations)\n",
        "print(\"len(hparam_combinations)\", len(hparam_combinations), \"hparam_combinations\", hparam_combinations)\n",
        "\n",
        "def modify_x_for_reduce(xs):\n",
        "  reshaped_x = np.reshape(xs, [xs.shape[0], -1])\n",
        "  # Make everything positive because some reductions don't work with negatives.\n",
        "  reshaped_x -= np.min(reshaped_x)\n",
        "  return reshaped_x\n",
        "\n",
        "def unsup_exstract(xs, reg_amount, drop_rate, layer_sizes, reduction_alg, n_components):\n",
        "  del reg_amount, drop_rate, layer_sizes\n",
        "\n",
        "  print(\"Using unsupervised feature extraction.\")\n",
        "\n",
        "  dim_reduct_model = ChannelReducer(reduction_alg=reduction_alg, n_components=n_components)\n",
        "  xs = dim_reduct_model.fit_transform(modify_x_for_reduce(xs))\n",
        "  return xs\n",
        "\n",
        "def train_best_logs(xs, ys, num_val, do_summary, hparams, get_model):\n",
        "  \"\"\"Trains the model and retruns the logs of the best epoch. randomly splits the train and val data before training.\"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  model = get_model(**hparams)\n",
        "  xs, ys = shuffle(xs, ys)\n",
        "\n",
        "  xs_val = xs[num_train:num_train+num_val]\n",
        "  ys_val = ys[num_train:num_train+num_val]\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=0)\n",
        "  best_stats = BestStats()\n",
        "  model.fit(xs[:num_train], ys[:num_train], epochs=epochs, batch_size=256, validation_freq=1, callbacks=[best_stats, early_stopping], validation_data=(xs_val, ys_val), verbose=0)\n",
        "  if do_summary:\n",
        "    model.summary()\n",
        "    print(\"best train accuracy:\", best_stats.bestTrain)\n",
        "    print(\"Number of epochs:\", best_stats.num_epochs)\n",
        "  return best_stats.bestLogs\n",
        "\n",
        "def multiple_train_ave(hparams):\n",
        "  \"\"\"Trains the model multiple times with the same parameters and returns the average metrics\"\"\"\n",
        "  start = time.time()\n",
        "  all_val_auc = []\n",
        "  all_val_accuracy = []\n",
        "\n",
        "  if hparams['reduction_alg'] != None:\n",
        "    xs_for_train = unsup_exstract(xs, **hparams)\n",
        "  else:\n",
        "    xs_for_train = xs\n",
        "\n",
        "  do_summary = True\n",
        "  for i in range(5):\n",
        "    logs = train_best_logs(xs_for_train, ys, num_val, do_summary, hparams, get_model)\n",
        "    all_val_auc.append(get_val_auc(logs))\n",
        "    all_val_accuracy.append(logs.get('val_accuracy'))\n",
        "    do_summary = False \n",
        "\n",
        "  mean_val_auc = np.mean(all_val_auc)\n",
        "  mean_val_accuracy = np.mean(all_val_accuracy)\n",
        "  metric = (mean_val_auc + mean_val_accuracy) / 2.0\n",
        "  print_data = (\"mean_val_auc\", mean_val_auc, \"mean_val_accuracy\", mean_val_accuracy, \"metric\", metric, \"val_auc_std\", np.std(all_val_auc), \"val_accuracy_std\", np.std(all_val_accuracy))\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Seconds per hyperparam config\", end - start)\n",
        "  # GPU: ('Seconds per hyperparam config', 16.970870971679688)\n",
        "\n",
        "  return metric, print_data\n",
        "\n",
        "best_metric = -float('inf')\n",
        "\n",
        "run_num = 0\n",
        "for hparams in hparam_combinations:\n",
        "  print(\"hparams\", hparams)\n",
        "\n",
        "  metric, print_data = multiple_train_ave(hparams)\n",
        "\n",
        "  print(print_data)\n",
        "  if metric > best_metric:\n",
        "    best_metric = metric\n",
        "    best_print_data = print_data\n",
        "    best_hparams = hparams\n",
        "\n",
        "  run_num += 1\n",
        "  print(\"fract done\", run_num/float(len(hparam_combinations)))\n",
        "  print\n",
        "  print(\"==============================================================================================\")\n",
        "  print\n",
        "  sys.stdout.flush()\n",
        "\n",
        "print(\"best_hparams\", best_hparams)\n",
        "print(\"best results\", best_print_data)\n",
        "print(\"Retraining on the best_hparams to make sure we didn't just get good results by random chance.\")\n",
        "\n",
        "_, print_data = multiple_train_ave(best_hparams)\n",
        "print(\"Result of retrain on the best hyperparameters\", print_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use_activations: False num_train: 50 epochs 400\n",
            "len(hparam_combinations) 1 hparam_combinations [{'drop_rate': 0.0, 'reg_amount': 0.0, 'reduction_alg': None, 'n_components': None}]\n",
            "hparams {'drop_rate': 0.0, 'reg_amount': 0.0, 'reduction_alg': None, 'n_components': None}\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 10, 12, 16)        880       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 10, 16)         2320      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 3,217\n",
            "Trainable params: 3,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.9599999785423279\n",
            "Number of epochs: 50\n",
            "Seconds per hyperparam config 18.639793634414673\n",
            "('mean_val_auc', 0.8181363582611084, 'mean_val_accuracy', 0.7572000026702881, 'metric', 0.7876681804656982, 'val_auc_std', 0.06625665201476676, 'val_accuracy_std', 0.0688081246868354)\n",
            "fract done 1.0\n",
            "==============================================================================================\n",
            "best_hparams {'drop_rate': 0.0, 'reg_amount': 0.0, 'reduction_alg': None, 'n_components': None}\n",
            "best results ('mean_val_auc', 0.8181363582611084, 'mean_val_accuracy', 0.7572000026702881, 'metric', 0.7876681804656982, 'val_auc_std', 0.06625665201476676, 'val_accuracy_std', 0.0688081246868354)\n",
            "Retraining on the best_hparams to make sure we didn't just get good results by random chance.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 10, 12, 16)        880       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 10, 16)         2320      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 3,217\n",
            "Trainable params: 3,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "best train accuracy: 0.9200000166893005\n",
            "Number of epochs: 38\n",
            "Seconds per hyperparam config 19.631194829940796\n",
            "Result of retrain on the best hyperparameters ('mean_val_auc', 0.8540621876716614, 'mean_val_accuracy', 0.7904000163078309, 'metric', 0.8222311019897461, 'val_auc_std', 0.038214657451181674, 'val_accuracy_std', 0.049918325857738206)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}